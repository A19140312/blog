<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[设计模式-原型模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Prototype%2F</url>
    <content type="text"><![CDATA[什么是原型模式原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。 相信大家都都听过Java中的克隆（clone()），所谓的原型模式其实就是克隆，以某个对象为原型，复制出新的对象。 代码实现原型模式其实就是Java中的克隆，在Java中实现克隆可以通过实现 Cloneable接口，并重写clone()方法来实现。可以发现Cloneable接口中并没有定义任何方法，clone()方法定义在Object中，其实Cloneable和Serializable一样都是标记型接口，内部没有方法和属性，实现Cloneable接口表示该对象能被克隆，能使用Object.clone()方法。如果没有实现Cloneable的类调用Object.clone()方法就会抛出CloneNotSupportedException。 Prototype实现Cloneable，并重写clone()方法，Prototype有两个属性，一个是基本类型的，一个是对象引用，之后来看clone的结果是怎么样的。 1234567891011121314151617181920212223242526272829303132333435363738public class Prototype implements Cloneable &#123; //基本类型的属性 private String attribute; //对象属性，引用 private Attribute attributeObject; public Prototype(String attribute, Attribute attributeObject) &#123; this.attribute = attribute; this.attributeObject = attributeObject; &#125; public String getAttribute() &#123; return attribute; &#125; public void setAttribute(String attribute) &#123; this.attribute = attribute; &#125; public Attribute getAttributeObject() &#123; return attributeObject; &#125; public void setAttributeObject(Attribute attributeObject) &#123; this.attributeObject = attributeObject; &#125; /** * 重写clone方法，这里实现的是浅拷贝，如果要进行深拷贝需要自己实现。 * @return * @throws CloneNotSupportedException */ @Override public Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125; Attribute 123456789101112131415public class Attribute &#123; public String name; public Attribute(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 客户端调用和输出 123456789101112public class Client &#123; public static void main(String[] args) throws CloneNotSupportedException &#123; Attribute attributeObject = new Attribute("BrightLoong"); Prototype prototype = new Prototype("属性", attributeObject); Prototype copy = (Prototype) prototype.clone(); System.out.println(copy.getAttribute() + "======" + copy.getAttributeObject().getName()); //克隆后，原型中的对象引用的还是同一个，所以输出true System.out.println(attributeObject == copy.getAttributeObject()); &#125;&#125; 12属性======BrightLoongtrue 可以看到结果符合预期，进行了clone，但是发现Attribute属性试用==比较返回的是true，说明引用的是同一个Attribute，两个Prototype对象引用了同一个Attribute对象，这就是所谓的浅拷贝。 浅拷贝和深拷贝Object的clone()方法，如果属性是基本类型，对该属性的值进行复制，如果属性是引用类型，则复制引用而不是复制引用的对象。 浅拷贝：浅拷贝是指拷贝对象时，拷贝的对象的所有基本类型属性的值都与原来的对象的值相同，而引用属性仍然指向原来对象中的引用属性。 深拷贝：深拷贝不仅拷贝对象本身，而且拷贝对象包含的引用指向的所有对象。 深拷贝代码实现如何实现深拷贝，当然最简单粗暴的方法就是对引用的对象实现克隆，如果引用的对象中还有对象，那么对引用的对象中的对象的实现克隆，依次类推。 这里使用另外一种方法，通过序列化(Serialization) 类实现深克隆。通过将对象写到流中，写到流中的对象是原有对象的一个拷贝，而原对象仍然存在于内存中，再从流里将其读出来，可以实现深克隆。 对象序列化需要实现Serializable 接口。 Prototype同时实现Cloneable, Serializable ，并重写clone()方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class Prototype implements Cloneable, Serializable &#123; //基本类型的属性 private String attribute; //对象属性，引用 private Attribute attributeObject; public Prototype(String attribute, Attribute attributeObject) &#123; this.attribute = attribute; this.attributeObject = attributeObject; &#125; public String getAttribute() &#123; return attribute; &#125; public void setAttribute(String attribute) &#123; this.attribute = attribute; &#125; public Attribute getAttributeObject() &#123; return attributeObject; &#125; public void setAttributeObject(Attribute attributeObject) &#123; this.attributeObject = attributeObject; &#125; /** * 重写clone方法，这里实现的是浅拷贝，如果要进行深拷贝需要自己实现。 * @return * @throws CloneNotSupportedException */ @Override public Object clone() throws CloneNotSupportedException &#123; //将对象写入流中 ByteArrayOutputStream bao=new ByteArrayOutputStream(); ObjectOutputStream oos = null; ObjectInputStream ois = null; Object copy = null; try &#123; //将对象写入流中 oos = new ObjectOutputStream(bao); oos.writeObject(this); //将对象从流中取出 ByteArrayInputStream bis=new ByteArrayInputStream(bao.toByteArray()); ois=new ObjectInputStream(bis); copy = ois.readObject(); &#125; catch (Exception e) &#123; e.printStackTrace(); if (oos != null) &#123; try &#123; oos.close(); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; &#125; if (ois != null) &#123; try &#123; ois.close(); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; return copy; &#125;&#125; Attribute 123456789101112131415public class Attribute implements Serializable&#123; public String name; public Attribute(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 输出还是使用原来的Client，输出如下，可以看到Attribute属性不再是同一个了，使用==比较返回了false。 12属性======BrightLoongfalse 总结使用场景 * 如果某个对象new的过程中很耗时（类初始化需要消化非常多的资源，这个资源包括数据、硬件资源等 ），则可以考虑使用原型模式 。 * 如果系统要保存对象的状态，而对象的状态变化很小，或者对象本身占用内存较少时。 * 一个对象需要提供给其他对象访问，而且各个调用者可能都需要修改其值时，可以考虑使用原型模式拷贝多个对象供调用者使用。 优点 * 提高了效率，逃避了类的构造方法（对象拷贝时，类的构造函数是不会被执行的）。 * 当创建新的对象实例较为复杂时，使用原型模式可以简化对象的创建过程 。 缺点 * 在实现深克隆的时候，使用的对象可能是原来已经存在的，并且没有实现Serializable，这个时候只能自己去一层一层的克隆，编写较为复杂的代码。 其他: 在很多工具类中已经实现了属性拷贝，并不用我们自己去实现比如apache.commons.beanutils 中的BeanUtils.copyProperties(obj1,obj2) 和PropertyUtils .copyProperties(obj1,obj2)。spring中也有类似的实现。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-享元模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Flyweight%2F</url>
    <content type="text"><![CDATA[什么是享元模式享元模式（Flyweight Pattern） ,运用共享技术有效的支持大量的细粒度对象。换句话说，使用享元模式对相同或者类似的对象进行复用，以此来减少少相同或者类似的对象的创建数量，从而减少内存占用，提高性能。 在学习享元模式之前需要先了解一下 细粒度 和享元对象中的 内部状态、外部状态 这三个概念： 内部状态：不随环境改变而改变的状态，内部状态可以共享，例如人的性别，不管任何环境下都不会改变 外部状态：随着环境改变而改变的状态，不可以共享的状态，享元对象的外部状态通常由客户端保存，并在享元对象创建后，需要的时候传入享元对象内部，不同的外部状态是相互独立的。例如衣服和鞋子，人在不同的环境下会穿不同的衣服和鞋子，但是衣服和鞋子又是相互独立不受彼此影响的 细粒度：较小的对象，所包含的内部状态较小 单纯享元模式单纯享元模式，也就是说所有的具体享元类都是可以被共享的，不存在不可以共享的具体享元类。 类图 FlyWeight:享元类的接口，接口中operation方法中的extrinsicState为外部状态。 ConcreteFlyWeight:具体的享元类，实现接口FlyWeight,存储内部状态intrinsicState。 FlyWeightFactory:享元工厂，用来创建和管理享元对象，其主要思想就是用一个Map来保存已经创建的对象实例。它主要是用来确保合理的共享FlyWeight对象，当用户请求一个FlyWeight对象时（getFlyWeight()方法），工厂提供一个已经存在的FlyWeight实例，如果不存在则创建一个返回。 代码实现享元类接口——FlyWeight 1234567891011/** * FlyWeight class * 定义享元类接口 */public interface FlyWeight &#123; /** * 操作方法. * @param extrinsicState 外部状态 */ public void operation(String extrinsicState);&#125; 具体享元实现——ConcreteFlyWeight 12345678910111213141516171819202122/** * ConcreteFlyWeight class */public class ConcreteFlyWeight implements FlyWeight &#123; /**内部状态*/ private String intrinsicState; /** * 构造方法，并设置内部状态. * @param intrinsicState */ public ConcreteFlyWeight(String intrinsicState) &#123; this.intrinsicState = intrinsicState; &#125; @Override public void operation(String extrinsicState) &#123; System.out.println("内部状态：" + intrinsicState); System.out.println("外部状态：" + extrinsicState); &#125;&#125; 享元工厂——FlyWeightFactory 123456789101112131415161718192021222324252627282930313233343536373839/** * FlyWeightFactory class * 用于创建和管理享元对象，本身是单例的。 */public class FlyWeightFactory &#123; /**唯一的factory实例*/ private static FlyWeightFactory factory = new FlyWeightFactory(); /**用于保存享元对象的map*/ private Map&lt;String, FlyWeight&gt; flyWeights = new HashMap&lt;&gt;(); /** * 私有的构造函数 */ private FlyWeightFactory() &#123;&#125; /** * 返回FlyWeightFactory唯一实例. * @return */ public static FlyWeightFactory getInstance() &#123; return factory; &#125; /** * 单纯享元模式工厂方法. * @param state key * @return */ public FlyWeight getFlyWeight(String state) &#123; FlyWeight flyWeight = flyWeights.get(state); if (flyWeight == null) &#123; flyWeight = new ConcreteFlyWeight(state); flyWeights.put(state, flyWeight); &#125; return flyWeight; &#125;&#125; 客户端调用和输出 12345678910111213141516public class Client &#123; public static void main(String[] args) &#123; FlyWeightFactory factory = FlyWeightFactory.getInstance(); FlyWeight flyWeight1 = factory.getFlyWeight("a"); FlyWeight flyWeight2 = factory.getFlyWeight("b"); FlyWeight flyWeight3 = factory.getFlyWeight("a"); FlyWeight flyWeight4 = factory.getFlyWeight("c"); //flyWeight1和flyWeight3具有相同的内部状态a,使用同一个实例，下面结果输出true System.out.println(flyWeight1 == flyWeight3); //不同的外部状态从方法传入 flyWeight2.operation("hello"); flyWeight2.operation("hi"); &#125;&#125; 12345true内部状态：b外部状态：hello内部状态：b外部状态：hi 复合享元模式复合享元模式，将一些单纯享元使用组合模式加以组合，可以形成复合享元对象，这样的复合享元对象本身不能共享，但是它们可以分解成单纯享元对象，而后者则可以共享。 复合享元模式中，组成复合享元对象的每个单纯享元对象拥有自己的内部状态，而每个单纯享元对象的外部状态都和复合享元对象的外部状态相同。所以复合享元模式可以对多个单纯享元对象设置相同的外部状态， 这也是复合享元模式的应用场景。 类图FlyWeight:享元类的接口，接口中operation方法中的extrinsicState为外部状态,是单纯享元类和复合享元类的共同接口。ConcreteFlyWeight:具体的享元类，实现接口FlyWeight,存储内部状态intrinsicState。CompositeConcreteFlyweight：复合享元类，实现FlyWeight接口，同时使用Map来保存单纯享元对象，并提供add()和remove()方法来增加或者删除单纯享元对象。FlyWeightFactory:享元工厂，用来创建和管理享元对象，其主要思想就是用一个Map来保存已经创建的对象实例。它主要是用来确保合理的共享FlyWeight对象。 * getFlyWeight(state:String):单纯享元工厂方法，当用户请求一个FlyWeight对象时，工厂提供一个已经存在的FlyWeight实例，如果不存在则创建一个返回(直接返回单纯享元对象)。 * getFlyWeight(states:List):复合享元工厂方法，根据提供的states:List，使用add()方法，将每个state对应的单纯享元对象添加到复合享元对象（CompositeConcreteFlyweight）的flyWeights中，并且这些单纯享元对象是从FlyWeightFactory的flyWeights中获取的（如果不存在则添加并返回），因此这些单纯享元对象是可以共享的。 代码实现复合享元类——CompositeConcreteFlyweight 123456789101112131415161718192021222324252627282930313233343536/** * CompositeConcreteFlyweight class * */public class CompositeConcreteFlyweight implements FlyWeight &#123; Map&lt;String, FlyWeight&gt; flyWeights = new HashMap&lt;&gt;(); /** * flyWeights是单纯享元对象的集合，它们具有相同的外部状态extrinsicState， * 调用的时候使用循环调用单纯享元对象的operation方法 * @param extrinsicState 外部状态 */ @Override public void operation(String extrinsicState) &#123; for (Map.Entry&lt;String, FlyWeight&gt; entry : flyWeights.entrySet()) &#123; entry.getValue().operation(extrinsicState); &#125; &#125; /** * 添加单纯享元对象. * @param state * @param flyWeight */ public void add(String state, FlyWeight flyWeight) &#123; flyWeights.put(state, flyWeight); &#125; /** * 移除单纯享元对象. * @param state */ private void remove(String state) &#123; flyWeights.remove(state); &#125;&#125; 修改后的享元工厂——FlyWeightFactory 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * FlyWeightFactory class * 用于创建和管理享元对象，本身是单例的。 */public class FlyWeightFactory &#123; /**唯一的factory实例*/ private static FlyWeightFactory factory = new FlyWeightFactory(); /**用于保存享元对象的map*/ private Map&lt;String, FlyWeight&gt; flyWeights = new HashMap&lt;&gt;(); /** * 私有的构造函数 */ private FlyWeightFactory() &#123;&#125; /** * 返回FlyWeightFactory唯一实例. * @return */ public static FlyWeightFactory getInstance() &#123; return factory; &#125; /** * 单纯享元模式工厂方法. * @param state key * @return */ public FlyWeight getFlyWeight(String state) &#123; FlyWeight flyWeight = flyWeights.get(state); if (flyWeight == null) &#123; flyWeight = new ConcreteFlyWeight(state); flyWeights.put(state, flyWeight); &#125; return flyWeight; &#125; /** * 复合享元模式工厂方法. * @param states * @return */ public FlyWeight getFlyWeight(List&lt;String&gt; states) &#123; CompositeConcreteFlyweight flyWeight = new CompositeConcreteFlyweight(); for (String state : states) &#123; //调用了单纯享元模式工厂方法,所以使用flyWeight.add加入的单纯享元对象是可以共享的 flyWeight.add(state, this.getFlyWeight(state)); &#125; return flyWeight; &#125;&#125; 客户端调用和输出 12345678910111213141516public class Client2 &#123; public static void main(String[] args) &#123; FlyWeightFactory factory = FlyWeightFactory.getInstance(); List&lt;String&gt; states = new ArrayList&lt;&gt;(); states.add("a"); states.add("b"); states.add("c"); FlyWeight flyWeight = factory.getFlyWeight(states); FlyWeight flyWeight2 = factory.getFlyWeight(states); //并不相等，输出false，复合享元对象不可共享 System.out.println(flyWeight == flyWeight2); //多个单纯享元对象具有相同的外部状态hello flyWeight.operation("hello"); &#125;&#125; 1234567false内部状态：a外部状态：hello内部状态：b外部状态：hello内部状态：c外部状态：hello 四. 总结使用场景 * 系统中有大量相同或者相似的对象，因此而造成了系统存储开销。 * 对象的大多数状态是外部状态，如果删除对象的外部状态，那么可以用相对较少的共享对象取代很多组对象，此时可以考虑使用享元模式。也就是说可以将外部状态使用传入对象的方式来处理。 优点 * 实现了对象的共享 * 使用享元模式减少对象的创建，减少了对内存的使用。 缺点 * 使用享元模式，需要分离出内部状态和外部状态，使得系统更加复杂。 应用：Java中不可变类String，Integer等。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-代理模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Proxy%2F</url>
    <content type="text"><![CDATA[什么是代理给某一个对象提供一个代理，并由代理对象控制对原对象的引用简单来说就是增强了一个对象的功能，比如：买火车票，app就是一个代理，他代理了火车站 #java实现的代理的办法 代理的名词 代理对象：增强后的对象 目标对象：被增强的对象 静态代理继承代理对象继承目标对象，重写需要增强的方法缺点：会代理类过多，非常复杂 以租房为例，我们一般用租房软件、找中介或者找房东。这里的中介就是代理者。 123456789101112131415161718192021222324// 目标对象public class RentHose &#123; public void rentHose() &#123; System.out.println("租了一间房子。。。"); &#125;&#125;//代理对象public class IntermediaryProxy extends RentHose&#123; @Override public void rentHose() &#123; System.out.println("交中介费.."); super.rentHose(); System.out.println("中介负责维修管理.."); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; RentHose rentHose = new IntermediaryProxy(); rentHose.rentHose(); &#125;&#125; 聚合目标对象和代理对象实现同一个接口，代理对象当中要包含目标对象。缺点：也会产生类爆炸，只不过比继承少一点点 12345678910111213141516171819202122232425262728293031323334353637public interface IRentHose &#123; public void rentHose();&#125;// 目标对象public class RentHose implements IRentHose&#123; @Override public void rentHose() &#123; System.out.println("租了一间房子。。。"); &#125;&#125;// 代理对象public class IntermediaryProxy implements IRentHose&#123; private IRentHose iRentHose; public IntermediaryProxy(IRentHose iRentHose) &#123; this.iRentHose = iRentHose; &#125; @Override public void rentHose() &#123; System.out.println("交中介费.."); iRentHose.rentHose(); System.out.println("中介负责维修管理.."); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; RentHose rentHose = new RentHose(); IRentHose iRentHose = new IntermediaryProxy(rentHose); iRentHose.rentHose(); &#125;&#125; 总结如果在不确定的情况下，尽量不要去使用静态代理。因为一旦你写代码，就会产生类，一旦产生类就爆炸。 动态代理我们知道现在的中介不仅仅是有租房业务，同时还有卖房、家政、维修等得业务，只是我们就不能对每一个业务都增加一个代理，就要提供通用的代理方法，这就要通过动态代理来实现了。 JDK动态代理通过接口-&gt;反射得到字节码(.class)，然后把字节码转成Class对象（利用native方法）。 1234567891011121314151617181920212223242526272829303132333435363738public class IntermediaryProxy implements InvocationHandler &#123; private Object obj; public IntermediaryProxy(Object obj) &#123; this.obj = obj; &#125; /** * 调用被代理的方法 * @param proxy：代理对象 * @param method：方法 * @param args：参数 * @return * @throws Throwable */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("交中介费"); //执行目标对象的方法 Object result = method.invoke(this.obj, args); System.out.println("中介负责维修管理"); return result; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; RentHose rentHose = new RentHose(); //定义一个handler InvocationHandler handler = new IntermediaryProxy(rentHose); // 获取对象的classLoader ClassLoader classLoader = rentHose.getClass().getClassLoader(); // JDK 动态代理产生代理类 IRentHose proxy = (IRentHose) Proxy.newProxyInstance(classLoader,new Class[]&#123;IRentHose.class&#125;,handler); proxy.rentHose(); &#125;&#125; 自己模拟的动态代理步骤： 1. 通过目标对象反射和字符串拼接生成一个类文件 2. 然后调用第三方的编译技术,动态编译这个产生的类文件成class文件 3. 利用UrlclassLoader,把这个动态编译的类加载到jvm当中 4. 最后通过反射把这个类实例化。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-Spring-AOP]]></title>
    <url>%2FJAVA%2FJava-SpringAOP%2F</url>
    <content type="text"><![CDATA[AOP与OOP对比，面向切面，传统的OOP开发中的代码逻辑是自上而下的，而这些过程会产生一些横切性问题，这些横切性的问题和我们的主业务逻辑关系不大，这些横切性问题不会影响到主逻辑实现的，但是会散落到代码的各个部分，难以维护。AOP是处理一些横切性问题，AOP的编程思想就是把这些问题和主业务逻辑分开，达到与主业务逻辑解耦的目的。使代码的重用性和开发效率更高。 AOP的应用场景日志记录、权限验证、效率检查、事务管理、exception springAop的概念 切面(Aspect)：切面是通知和切点的结合，通知和切点共同定义了切面的全部内容 连接点(Join point)：目标对象中的方法。 通知(Advice)：定义了切面是做什么以及何时使用。 切点(Pointcut)：表示连接点的集合。（PointCut是JoinPoint的谓语，这是一个动作，主要是告诉通知连接点在哪里，切点表达式决定 JoinPoint 的数量） 目标对象(Target object)：目标对象 原始对象 aop代理(AOP proxy)：代理对象 包含了原始对象的代码和增加后的代码的那个对象 织入(Weaving)：把代理逻辑加入到目标对象上的过程 advice通知类型 Before 连接点执行之前，但是无法阻止连接点的正常执行，除非该段执行抛出异常 After 连接点正常执行之后，执行过程中正常执行返回退出，非异常退出 After throwing 执行抛出异常的时候 After (finally) 无论连接点是正常退出还是异常退出，都会执行 Around advice 围绕连接点执行，例如方法调用。这是最有用的切面方式。around通知可以在方法调用之前和之后执行自定义行为。它还负责选择是继续加入点还是通过返回自己的返回值或抛出异常来快速建议的方法执行。 springAop支持AspectJ启用@AspectJ支持 使用Java @Configuration启用@AspectJ支持，添加@EnableAspectJAutoProxy注释 12345@Configuration@EnableAspectJAutoProxypublic class AppConfig &#123;&#125; 使用XML配置启用@AspectJ支持 1&lt;aop:aspectj-autoproxy/&gt; 声明一个Aspect1234@Component@Aspectpublic class UserAspect &#123;&#125; 声明明一个pointCut切入点表达式由@Pointcut注释表示。切入点声明由两部分组成:一个签名包含名称和任何参数，以及一个切入点表达式，该表达式确定我们对哪个方法执行感兴趣。 12@Pointcut("execution(* transfer(..))")// 切入点表达式private void anyOldTransfer() &#123;&#125;// 切入点签名 声明一个Advice通知1234567891011121314151617181920212223242526272829/** * 申明Aspect，并且交给spring容器管理 */@Component@Aspectpublic class UserAspect &#123; /** * 申明切入点，匹配UserDao所有方法调用 * execution匹配方法执行连接点 * within:将匹配限制为特定类型中的连接点 * args：参数 * target：目标对象 * this：代理对象 */ @Pointcut("execution(* com.yao.dao.UserDao.*(..))") public void pintCut()&#123; System.out.println("point cut"); &#125; /** * 申明before通知,在pintCut切入点前执行 * 通知与切入点表达式相关联， * 并在切入点匹配的方法执行之前、之后或前后运行。 * 切入点表达式可以是对指定切入点的简单引用，也可以是在适当位置声明的切入点表达式。 */ @Before("com.yao.aop.UserAspect.pintCut()") public void beforeAdvice()&#123; System.out.println("before"); &#125;&#125; 连接点joinPoint的意义 execution：用于匹配方法执行 join points连接点，最小粒度方法，在aop中主要使用。 execution(modifiers-pattern? ret-type-pattern declaring-type-pattern?name-pattern(param-pattern) throws-pattern?)这里问号表示当前项可以有也可以没有，其中各项的语义如下modifiers-pattern：方法的可见性，如public，protected；ret-type-pattern：方法的返回值类型，如int，void等；declaring-type-pattern：方法所在类的全路径名，如com.spring.Aspect；name-pattern：方法名类型，如buisinessService()；param-pattern：方法的参数类型，如java.lang.String；throws-pattern：方法抛出的异常类型，如java.lang.Exception； example: 12345678910@Pointcut("execution(* com.chenss.dao.*.*(..))")//匹配com.chenss.dao包下的任意接口和类的任意方法@Pointcut("execution(public * com.chenss.dao.*.*(..))")//匹配com.chenss.dao包下的任意接口和类的public方法@Pointcut("execution(public * com.chenss.dao.*.*())")//匹配com.chenss.dao包下的任意接口和类的public 无方法参数的方法@Pointcut("execution(* com.chenss.dao.*.*(java.lang.String, ..))")//匹配com.chenss.dao包下的任意接口和类的第一个参数为String类型的方法@Pointcut("execution(* com.chenss.dao.*.*(java.lang.String))")//匹配com.chenss.dao包下的任意接口和类的只有一个参数，且参数为String类型的方法@Pointcut("execution(* com.chenss.dao.*.*(java.lang.String))")//匹配com.chenss.dao包下的任意接口和类的只有一个参数，且参数为String类型的方法@Pointcut("execution(public * *(..))")//匹配任意的public方法@Pointcut("execution(* te*(..))")//匹配任意的以te开头的方法@Pointcut("execution(* com.chenss.dao.IndexDao.*(..))")//匹配com.chenss.dao.IndexDao接口中任意的方法@Pointcut("execution(* com.chenss.dao..*.*(..))")//匹配com.chenss.dao包及其子包中任意的方法 由于Spring切面粒度最小是达到方法级别，而execution表达式可以用于明确指定方法返回类型，类名，方法名和参数名等与方法相关的信息，并且在Spring中，大部分需要使用AOP的业务场景也只需要达到方法级别即可，因而execution表达式的使用是最为广泛的 within:表达式的最小粒度为类 12@Pointcut("within(com.chenss.dao.*)")//匹配com.chenss.dao包中的任意方法@Pointcut("within(com.chenss.dao..*)")//匹配com.chenss.dao包及其子包中的任意方法 within与execution相比，粒度更大，仅能实现到包和接口、类级别。而execution可以精确到方法的返回值，参数个数、修饰符、参数类型等 args:表达式的作用是匹配指定参数类型和指定参数数量的方法,与包名和类名无关 12@Pointcut("args(java.io.Serializable)")//匹配运行时传递的参数类型为指定类型的、且参数个数和顺序匹配@Pointcut("@args(com.chenss.anno.Chenss)")//接受一个参数，并且传递的参数的运行时类型具有@Classified args同execution不同的地方在于,args匹配的是运行时传递给方法的参数类型,execution(* *(java.io.Serializable))匹配的是方法在声明时指定的方法参数类型。 this:JDK代理时，指向接口和代理类proxy，cglib代理时 指向接口和子类(不使用proxy) target:指向接口和子类 1234567891011/** * 此处需要注意的是，如果配置设置proxyTargetClass=false，或默认为false，则是用JDK代理，否则使用的是CGLIB代理 * JDK代理的实现方式是基于接口实现，代理类继承Proxy，实现接口。 * 而CGLIB继承被代理的类来实现。 * 所以使用target会保证目标不变，关联对象不会受到这个设置的影响。 * 但是使用this对象时，会根据该选项的设置，判断是否能找到对象。 */@Pointcut("target(com.chenss.dao.IndexDaoImpl)")//目标对象，也就是被代理的对象。限制目标对象为com.chenss.dao.IndexDaoImpl类@Pointcut("this(com.chenss.dao.IndexDaoImpl)")//当前对象，也就是代理对象，代理对象时通过代理目标对象的方式获取新的对象，与原值并非一个@Pointcut("@target(com.chenss.anno.Chenss)")//具有@Chenss的目标对象中的任意方法@Pointcut("@within(com.chenss.anno.Chenss)")//等同于@targ]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-Spring]]></title>
    <url>%2FJAVA%2FJava-Spring%2F</url>
    <content type="text"><![CDATA[springIOCwhat is IOC 控制反转（Inversion of Control，缩写为IoC），是面向对象编程中的一种设计原则，将你设计好的对象交给容器控制，可以用来减低计算机代码之间的耦合度。其中最常见的方式叫做依赖注入（Dependency Injection，简称DI），还有一种方式叫“依赖查找”（Dependency Lookup） 依赖注入（Dependency Injection）依赖：比如A.class中有一个B.class的属性，那么我们可以理解为A依赖了B依赖注入：由容器动态的将某个依赖关系注入到组件之中。依赖注入是实现IOC的一种方式。 为什么要使用spring IOC？ 在日常程序开发过程当中，我们推荐面向抽象编程，面向抽象编程会产生类的依赖当然如果你够强大可以自己写一个管理的容器但是既然spring以及实现了，并且spring如此优秀，我们仅仅需要学习spring框架便可。当我们有了一个管理对象的容器之后，类的产生过程也交给了容器，至于我们自己的程序则可以不需要去关系这些对象的产生了。 spring实现IOC的思路和方法 应用程序中提供类，提供依赖关系（属性或者构造方法） 把需要交给容器管理的对象通过配置信息告诉容器（xml、annotation，javaconfig） 把各个类之间的依赖关系通过配置信息告诉容器 配置这些信息的方法有三种分别是xml，annotation和javaconfig维护的过程称为自动注入，自动注入的方法有两种构造方法和setter自动注入的值可以是对象，数组，map，list和常量比如字符串整形等 spring编程的风格 schemal-based——-xml annotation-based—–annotation java-based—-java Configuration 注入的两种方法官网文档已经很详细了：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-factory-properties-detailed 构造方法注入（Constructor-based Dependency Injection）构造方法参考文档：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-constructor-injection setter方法注入（Setter-based Dependency Injection）setter参考文档：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-setter-injection 自动装配上面说过，IOC的注入有两个地方需要提供依赖关系，一是类的定义中，二是在spring的配置中需要去描述。自动装配则把第二个取消了，即我们仅仅需要在类中提供依赖，继而把对象交给容器管理即可完成注入。 自动装配的优点参考文档：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-factory-autowire缺点参考文档：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-autowired-exceptions 自动装配的方法 no：默认值，表示没有自动装配，应使用显式 bean 引用进行装配。 byName：它根据 bean 的名称注入对象依赖项。 byType：它根据类型注入对象依赖项。 constructor：通过构造函数来注入依赖项，需要设置大量的参数。 @Component，@Service，@Controller，@Repository @Component是一个通用的Spring容器管理的单例bean组件 @Repository 通常用于持久层 @Service 通常用于业务逻辑层 @Controller 通常用于表现层（spring-mvc的注解） 这几种注解当前的作用没有任何区别。官网有这样一段话，意思是这几种注解在未来可能会有其他语意。因此推荐按照通用使用方式使用注解。 @Repository, @Service, and @Controller can also carry additional semantics in future releases of the Spring Framework spring懒加载官方文档：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-factory-lazy-init springbean的作用域文档参考：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-factory-scopes Singleton 中引用了一个Prototype的bean的时候引发的问题官网引导我们参考：https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-factory-method-injection 由于Singleton状态的bean A在初始化的时候只new一次，也只会注入一次依赖的对象B因此B对象设置为Prototype 就没有任何意义。解决方式参考官网。 自己模拟springbean定义几个类 1234567891011121314151617181920212223242526272829303132public interface UserDao &#123; public String query();&#125;public class UserDaoImpl implements UserDao &#123; @Override public String query() &#123; return "我要减肥～～～"; &#125;&#125;public interface UserService &#123; public void find();&#125;public class UserServiceImpl implements UserService &#123; UserDao userDao; @Override public void find() &#123; System.out.println(userDao.query() + "hahaha"); &#125; public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125;// public UserServiceImpl(UserDao userDao) &#123;// this.userDao = userDao;// &#125;&#125; 新建BeanFactory解析xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120public class BeanFactory &#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); public BeanFactory(String xml) &#123; try &#123; parseXml(xml); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; private void parseXml(String xml) throws Exception &#123; File file = new File(this.getClass().getResource("/").getPath()+"/"+xml); SAXReader reader = new SAXReader(); //解析XML形式的文本,得到document对象. Document document = reader.read(file); // 获取文档的根节点. Element elementRoot = document.getRootElement(); // &lt;beans default="byType"&gt; Attribute attribute = elementRoot.attribute("default-autowire"); boolean flag = false; if (attribute != null)&#123; flag = true; &#125; for(Iterator it = elementRoot.elementIterator();it.hasNext();) &#123; /** * 实例化对象 */ Element elementFirstChil = (Element) it.next(); // &lt;bean id="userDao" class="com.qing.dao.UserDaoImpl"/&gt; // 获取id属性 Attribute attributeId = elementFirstChil.attribute("id"); // 得到bean名称 String beanName = attributeId.getValue(); // 获取class属性 Attribute attributeClazz = elementFirstChil.attribute("class"); // 得到className String className = attributeClazz.getValue(); Class clazz = Class.forName(className); /** * 维护依赖关系 * 看这个对象有没有依赖（判断是否有property。或者判断类是否有属性） * 如果有则注入 */ Object object = null; for (Iterator itSon = elementFirstChil.elementIterator(); itSon.hasNext(); ) &#123; Element elementSon = (Element) itSon.next(); if (elementSon.getName().equals("property")) &#123; // setter 方法注入 // &lt;property name="userDao" ref="userDao"/&gt; object = clazz.newInstance(); // 获取ref属性 Attribute attributeRef = elementSon.attribute("ref"); Object objectSon = map.get(attributeRef.getValue()); // 获取name属性 Attribute attributeName = elementSon.attribute("name"); String name = attributeName.getValue(); // 获取类本身的属性成员 Field field = clazz.getDeclaredField(name); // 利用反射set属性，要设置取消 Java 语言访问检查 field.setAccessible(true); // set属性 field.set(object, objectSon); &#125; else if(elementSon.getName().equals("constructor-arg"))&#123; //构造方法注入 // 获取ref属性 Attribute attributeRef = elementSon.attribute("ref"); Object objectSon = map.get(attributeRef.getValue()); // 获取objectSon所实现的接口 Class sonInterface = objectSon.getClass().getInterfaces()[0]; Constructor constructor = clazz.getConstructor(sonInterface); object = constructor.newInstance(objectSon); &#125; &#125; if(object == null)&#123; if (flag) &#123;// 自动装配 if (attribute.getValue().equals("byType")) &#123; //判斷是否有依賴 Field fields[] = clazz.getDeclaredFields(); for (Field field : fields) &#123; //得到屬性的類型，比如String aa那麽這裏的field.getType()=String.class Class injectObjectClazz = field.getType(); /** * 由於是bytype 所以需要遍历map当中的所有对象 * 判断对象的类型是不是和这个injectObjectClazz相同 */ Object objectSon = null; int count = 0; for(Map.Entry&lt;String,Object&gt; entry : map.entrySet())&#123; Class temp = entry.getValue().getClass().getInterfaces()[0]; if(temp.getName().equals(injectObjectClazz.getName()))&#123; objectSon = entry.getValue(); count ++; &#125; &#125; if(count &gt; 1)&#123; throw new MySpringException("需要一个对象，但是找到了两个对象"); &#125;else &#123; object = clazz.newInstance(); field.setAccessible(true); field.set(object, objectSon); &#125; &#125; &#125; &#125; &#125; if (object == null) &#123;//沒有子标签 object = clazz.newInstance(); &#125; map.put(beanName, object); &#125; System.out.println(map); &#125; public Object getBean(String name)&#123; return map.get(name); &#125; 测试类 12345678public class Test &#123; public static void main(String[] args) &#123; BeanFactory beanFactory = new BeanFactory("spring.xml"); UserService userService = (UserService) beanFactory.getBean("userService"); userService.find(); &#125;&#125; xml 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans default-autowire="byType"&gt;&lt;!-- setter注入--&gt; &lt;bean id="userDao" class="com.qing.dao.UserDaoImpl"/&gt; &lt;bean id="userService" class="com.qing.service.UserServiceImpl"&gt; &lt;property name="userDao" ref="userDao"/&gt; &lt;/bean&gt;&lt;!-- 构造方法注入--&gt; &lt;bean id="userDao" class="com.qing.dao.UserDaoImpl"/&gt; &lt;bean id="userService" class="com.qing.service.UserServiceImpl"&gt; &lt;constructor-arg ref="userDao"/&gt; &lt;/bean&gt;&lt;!-- default-autowire="byType"--&gt; &lt;bean id="userDao" class="com.qing.dao.UserDaoImpl"/&gt; &lt;bean id="userService" class="com.qing.service.UserServiceImpl"/&gt;&lt;/beans&gt;]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试必备]]></title>
    <url>%2FJAVA%2FInterview%2F</url>
    <content type="text"><![CDATA[Java基础 ------------------------ 基础 ------------------------ 基础JDK 和 JRE 有什么区别？ JDK：Java 开发工具包，提供了 Java 的开发环境和运行环境（包括JRE）。 JRE：Java 运行环境 == 和 equals 的区别是什么？ ==： 基本类型：比较的是值是否相同；引用类型：比较的是引用的地址是否相同； equals：equals 本质上就是 ==，只不过 String 和 Integer 等重写了 equals 方法，把它变成了值比较 总结 ：== 对于基本类型来说是值比较，对于引用类型来说是比较的是引用；而 equals 默认情况下是引用比较，只是很多类重新了 equals 方法，比如 String、Integer 等把它变成了值比较，所以一般情况下 equals 比较的是值是否相等。 为什么重写了equals方法，就要重写hashCode方法？不是必须，只是建议。虽然可以不重写hashCode，但是会对集合造成影响。比如hashMap，如果equals判断相同的key，但是hashCode不同，也就是内存中的地址不同，那么进行如下操作，就不会被覆盖，而是被put两次hashMap.put(“k”,”v1”)，hashMap.put(“k”:”v2”) final 在 Java 中有什么作用？ final 修饰的类叫最终类，该类不能被继承。 final 修饰的方法不能被重写。 final 修饰的变量叫常量，常量必须初始化，初始化之后值就不能被修改。 Java基础的数据类型有哪些？基础类型有 8 种：byte、boolean、char、short、int、float、long、double Java 中操作字符串都有哪些类？它们之间有什么区别？操作字符串的类有：String、StringBuffer、StringBuilder。String： * 声明的是不可变的对象，它的底层是一个用final修饰的字符数组，每次操作都会生成新的 String 对象，然后将指针指向新的 String 对象 * String 对象赋值之后就会在字符串常量池中缓存，如果下次创建会判定常量池是否已经有缓存对象，如果有的话直接返回该引用给创建者。StringBuffer： 可以在原有对象的基础上进行操作，线程安全的，性能高于StringBuilderStringBuilder： 可以在原有对象的基础上进行操作，非线程安全的 String和StringBuilder 的区别？jdk1.5：string 对象时恒定不变的,stringBuider对象表示的字符串是可变的。所以在字符串频繁修改的情况下stringBuider效率jdk1.8: 编译器利用String的可变配套类(StringBuilder)帮我们做了优化，编译器自动调用StringBuilder.apend()方法添加。因此在拼接字符串小于500左右时，两个对象效率相同。 接口和抽象类有什么区别？设计目的： * 接口的设计目的，是对类的行为进行约束，也就是提供一种机制，可以强制要求不同的类具有相同的行为。 * 抽象类的设计目的，是代码复用。相同： * 都不能被实例化 * 都能包含抽象的方法不同： * 在抽象类中可以写非抽象的方法，从而避免在子类中重复书写他们，这样可以提高代码的复用性，这是抽象类的优势；接口中只能有抽象的方法。 * 抽象类中的成员变量可以是各种类型的；而接口中的成员变量只能是public static final类型的； * 抽象类可以有静态代码块和静态方法；接口中不能含有静态代码块以及静态方法 * 一个类只能继承一个抽象类，而一个类却可以实现多个接口。 请列举你所知道的Object类的方法并简要说明。 getClass():用于返回当前运行时对象的Class对象 equals():用于比较两个对象的地址是否相同，即两个引用是否指向同一个对象； clone():用于创建并返回当前对象的一份拷贝 toString():返回类的名字@实例的哈希码的16进制字符串； notify():唤醒等待队列中的其中一个线程 notifyAll():唤醒线程等待队列中的所有线程； wait(long timeout):让一个线程等待一段时间。 finalize()：用于释放资源，可以覆盖此方法实现资源清理工作。GC在回收对象之前调用该方法，但是无法确定该方法具体什么时候被调用 类的加载顺序 父类静态代码块(包括静态初始化块，静态属性，但不包括静态方法) 子类静态代码块(包括静态初始化块，静态属性，但不包括静态方法 ) 父类非静态代码块( 包括非静态初始化块，非静态属性 ) 父类构造函数 子类非静态代码块 ( 包括非静态初始化块，非静态属性 ) 子类构造函数 对象在堆上要分配多大内存？ java对象的属性—不固定 对象头—固定12 byte 数据对齐（64位虚拟机要求对象大小是8的整数倍，不够就补齐） 64位jvm中，一个仅包含一个boolean属性的对象，大小是16 byte，12byte对象头，1byte数据，3byte填充数据 对象有几种状态？ 无状态 刚new出来的时候 biased_lock:0 | lock: 10 偏向锁 biased_lock:1 | lock: 10 轻量级锁 重量级锁 无引用，被gc标记 什么是对象头？所有对象通用的一部分结构。由两部分组成： mark word 32 bits jvm 占 32 bits hashcode：25｜ age：4｜ biased_lock(偏向锁)：1 | lock(同步状态) : 2 64 bits jvm 占 64 bits 无状态下，在未调用过hashcode函数时，对象头hashcode位置都为0，调用过hashcode函数计算之后才会保存。 klass pointer 指向类.class的指针 64 bits jvm 开启指针压缩占 32 bits 不开启占 64 bits 解释一下对象创建过程？假设有一个对象T,有一个属性m=8，那么在new 这个对象时先在堆中申请空间，然后属性半初始化也就是m=0，然后再调用构造方法将m修改为8，然后将引用变量t指向堆中的地址。 DCL与volatile的问题？volatile 的作用是线程的可见性和禁止了指令重排序。在单例模式中，未了保证多线程的安全性，采用了DCL（Double-Check-Locking）方式，那么在线程1创建对象X的时候，对象先半初始化，再调用构造方法，线程2再去判断X对象等不等于空，如果此时发生指令重排序，对象先半初始化，线程2再去判断X对象等不等于空，再调用构造方法，会导致线程2拿到了半初始化的对象。所以需要volatile来禁止指令重排。 对象在内存中的存储布局？对象头、实例数据和对齐填充 对象怎么定位？引用-&gt;堆内存的对象-&gt;方法区常量。 对象怎么分配？开始new一个对象 *尝试在栈上分配？ * 可以在栈上分配，在pop时结束生命周期 * 若一个对象的引用逃出了方法或者线程，在方法调用过程中传递对象的引用到另一个方法，或栈空间不够大。则不能在栈上分配。 * 对象是否够大？ * 足够大，老年代分配，full gc时结束生命周期 * 不够大，尝试 tlab 分配？ * tlab空间是否足够？ * 足够，tlab（线程本地空间）分配到eden， * 不足，共享空间分配到eden * young gc-&gt; 幸存者区-&gt; 老年代-&gt; full gc 结束生命周期 一般的对象分配内存，都是在新生代进行空间申请的。在多个线程都在申请空间时，每次对象分配都必须进行同步。竞争激烈的场合分配的效率又会进一步下降。TLAB是一个存在于eden区的线程独享内存区域，主要用于降低在新生代分配对象时的内存竞争，提升对象分配的效率。 Object o = new Object() 在内存中占多少字节？对象的引用o 4 byteObject 的 mark word 占 8 byteklass pointer 若开启指针压缩占 4 byte，不开启压缩占 8 byte对象的属性大小不确定。 ------------------------ 集合 ------------------------ 集合HashSet 的底层实现是什么?HashSet 的实现是依赖于 HashMap 的，HashSet 的值都是存储 在 HashMap 中的。在 HashSet 的构造法中会初始化一个 HashMap 对象HashSet 不允许值重复，因此，HashSet 的值是作为 HashMap 的 key 存储在 HashMap 中的，当存储的值已经存在时返回 false。 Iterator 和 ListIterator 的区别是什么? Iterator 可用来遍历 Set 和 List 集合，但是 ListIterator 只能用来遍历 List。 Iterator 对集合只能是前向遍历，ListIterator 既可以前向也可以后向。ListIterator 实现了 Iterator 接口，并包含其他的功能，比如:增加元素，替换元 素，获取前一个和后一个元素的索引，等等。 数组 (Array) 和列表 (ArrayList) 有什么区别? Array 可以包含基本类型和对象类型，ArrayList 只能包含对象类型。 Array 大小是固定的，ArrayList 的大小是动态变化的。 ArrayList 处理固定大小的基本数据类型的时候，这种方式相对比较慢。 Comparable 和 Comparator 接口是干什么的?Comparable：只包含一个 compareTo() 方法，这个方法可以个 给两个对象排序。具体来说，它返回负数，0，正数来表明输入对象小于，等于，大于 已经存在的对象。Comparator：包含 compare() 和 equals() 两个方法。 * compare() 方法用来给两个输入参数排序，返回负数，0，正数表明第一个参数是小 于，等于，大于第二个参数。 * equals() 方法需要一个对象作为参数，它用来决定输入 参数是否和 comparator 相等。 Collection 和 Collections 的区别？Collection：是集合类的上级接口, 继承与它的接口主要是 set 和 list。Collections： 类是针对集合类的一个帮助类. 它提供一系列的静态方法对各种集合的搜 索, 排序, 线程安全化等操作。 ArrayList 和 Vector 的区别？相同： * 都实现了 List 接口(List 接口继承了 Collection 接口) * 都是有序集合，即存储在这两个集合中的元素的位置都是有顺序的，相当于一种动态的数组 * 允许重复 区别： * Vector 是线程安全的，ArrayList 是线程序不安全的。数据增长: * 相同：ArrayList 与 Vector 都可以设置初始的空间大小 * 不同：Vector 还可以设置增长的空间大小，而 ArrayList 没有提供设置增长空间的方法。 Vector 默认增加原来的 1 倍，ArrayList 增加原来的 0.5 倍。 快速失败 (fail-fast) 和安全失败 (fail-safe) 的区别是什么?Iterator 的安全失败是基于对底层集合做拷贝，因此，它不受源集合上修改的影响。 java.util 包下面的所有的集合类都是快速失败的，迭代器会抛出 ConcurrentModificationException 异常java.util.concurrent 包下面的所有的类都是安全失败的。安全失败的迭代器永远不会抛出这样的异常。 List、Map、Set 三个接口，存取元素时，各有什么特点?这样的题属于随意发挥题:这样的题比较考水平，两个方面的水平:一是要真正明白 这些内容，二是要有较强的总结和表述能力。如果你明白，但表述不清楚，在别人那 里则等同于不明白。 首先List与Set相同： * 都是单列元素的集合，有一个相同的父类 Collection不同： * List 元素可以重复，Set不可以 * List 可以按index取元素，Set只能逐一遍历 * List 是有序集合。 Map 是双列集合，要存储一对 key/value，不能存储重复的 key。 HashMap 的工作原理是什么?HashMap 1.8是 数组 + 链表 + 红黑树 实现的。 负载因子（loadFactor）：0.75f 容量（capacity）：16 扩容阈值（threshold）：loadFactor * capacity 转化成树的链表阈值（TREEIFY_THRESHOLD）：8 转化成树的最小容量（MIN_TREEIFY_CAPACITY）：64Java 中的 HashMap 是以键值对 (key-value) 的形式存储元素的，我们把一对(key-value)称为Node。HashMap 需要 一个 hash 函数，当调用 put() 方法的时候，HashMap 会计算 key 的 hash 值，然后把键值对存储在集合中合适的索引上。如果索引上已经存在了Node，发生哈希冲突。 如果索引上的结构是链表，则在链表中遍历，如果有相同的key，value 会被更新成新值，否则遍历到链表尾部，插入新的(key-value) ，size+1 如果链表的长度大于转化成树的链表阈值（TREEIFY_THRESHOLD） 并且，hashMap的容量大于转化成树的最小容量（MIN_TREEIFY_CAPACITY） 则转换成红黑树。 如果索引上的结构是红黑树，则在红黑树中遍历，如果有相同的key，value 会被更新成新值，否则插入红黑树，size+1。 如果 size &gt; threshold 则进行扩容。 * 当原来的容量已经达到最大容量的时候，将阈值设置为Integer.MAX_VALUE，这样就不会再发生重构的情况 * 将新的阈值设置为旧的阈值的两倍, 新的容量设置为旧容量的2倍。 * 根据新容量新建一个Node数组，将旧数组中的元素全部取出，重新映射到新数组中 hashMap 容量为什么是 2 的幂次？为了加快哈希计算以及减少哈希冲突。为什么可以加快计算？我们都知道为了找到 KEY 的位置在哈希表的哪个槽里面，需要计算 hash(KEY) % 数组长度但是 % 计算比 &amp; 慢很多，所以用 &amp; 代替 %，为了保证 &amp; 的计算结果等于 % 的结果需要把 length 减 1，也就是 hash(KEY) &amp; (length - 1)证明：当length 为2的幂次时，m % length = m &amp; (length-1) m 可以分成两部分 x（length的正数倍） 和 y（剩余部分） 因此 y的范围为 0 &lt;= y &lt;length m % length = (x + y)% length = x % length + y % length = 0 + y % length = y 假设y 是 5 二进制 0101，length 为 8 二进制 1000 ，length -1 二进制 0111 0101&amp;0111 = 0101 刚好将 5取出，因此成立 为什么可以减少冲突？假设现在数组的长度 length 可能是偶数也可能是奇数。length 为偶数时，length-1 为奇数，奇数的二进制最后一位是 1，这样便保证了 hash &amp;(length-1) 的最后一位可能为 0，也可能为 1（这取决于 h 的值），即 &amp; 运算后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性。length 为奇数的话，很明显 length-1 为偶数，它的最后一位是 0，这样 hash &amp; (length-1) 的最后一位肯定为 0，即只能为偶数，这样任何 hash 值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间 ConcurrentHashMap 的工作原理是什么什么？ JDK1.7: ConcurrentHashMap采用了数组+Segment+分段锁的方式实现。 Segment：类似于HashMap的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表,同时又是一个ReentrantLock（Segment继承了ReentrantLock）。 从上面的结构我们可以了解到，ConcurrentHashMap定位一个元素的过程需要进行两次Hash操作。 第一次Hash定位到Segment，第二次Hash定位到元素所在的链表的头部。 JDK1.8: 采用了数组+链表+红黑树实现.彻底放弃了Segment转而采用的是Node，其设计思想也不再是JDK1.7中的分段锁思想。 Java8 ConcurrentHashMap结构基本上和Java8的HashMap一样，原来是对需要进行数据操作的Segment加锁，现调整为对每个数组元素加锁（Node）。 HashTable 、ConcurrentHashMap 和 hashMap 的区别？hashTable 默认初始容量是11，hashMap 和 ConcurrentHashMap 默认初始容量是16，HashTable 在put和get方法上用了 synchronized 。ConcurrentHashMap 对每个node节点使用 synchronizedHashTable 不允许 key 为null 会抛出异常：当你通过get(k)获取对应的value时，如果获取到的是null时，你无法判断，它是put（k,v）的时候value为null，还是这个key不存在。在于多线程情况下，即便此可你通过contains(key)知晓了是否包含null，下一步当你使用这个结果去做一些事情时可能其他线程已经改变了这种状态，这在单线程下是不可能发生的。ConcurrentHashMap 不允许 key 和 value 为 null 会抛出异常ConcurrentHashMap不能put null 是因为 无法分辨是key没找到的null还是有key值为null，这在多线程里面是模糊不清的，所以压根就不让put null。 HashTable 的默认大小为11，有什么特别的理由吗？HashTable 的容量增加逻辑是乘2+1，保证奇数。http://www.vvbin.com/?p=376 ArrayList 默认大小为10，有什么特别的理由吗？ArrayList 的容量增长逻辑是乘 1.5 + 1，逻辑比较随意，看不出有什么特别含义。 LinkedHashMap 工作原理？LinkedHashMap基于hashMap的基础上，对每个键值对(Node节点)多维护了一个before和after指针，每次插入时维护双向链表。LinkedHashMap有序，可分为插入顺序和访问顺序两种。如果是访问顺序，那put和get操作已存在的Entry时，都会把Entry移动到双向链表的表尾(其实是先删除再插入)。 TreeMap 工作原理？底层红黑树实现。TreeMap有序是通过Comparator来进行比较的，如果comparator为null，那么就使用自然顺序 Stack 的工作原理？继承vector，添加操作是添加到vector的尾部，pop操作是移除vector末尾元素。 ------------------------ 异常 ------------------------ 异常Java中的异常有哪几类？异常类有分为编译时异常和运行时异常常见的编译时异常： * IOException * SQLException * parseException 常见的运行时异常（RuntimeException）： * NullPointerException: 空指针异常,一般出现于数组,空对象的变量和方法 * ArrayIndexOutOfBoundsException: 数组越界异常 * NoClassDefFoundException: java运行时系统找不到所引用的类 * NumberFormatException: 数据格式异常 * OutOfMemoryException: 内存溢出异常 * ArithmeticException: 算数异常,一般在被除数是0中 * IllegalArgumentException: 非法参数异常 Error 和 Exception 区别是什么？Error 类型的错误通常为虚拟机相关错误，如系统崩溃，内存不足，堆栈溢出等，编译器不会对这类错误进行检测，JAVA 应用程序也不应对这类错误进行捕获，一旦这类错误发生，通常应用程序会被终止，仅靠应用程序本身无法恢复；Exception 类的错误是可以在应用程序中进行捕获并处理的，通常遇到这种错误，应对其进行处理，使应用程序可以继续正常运行。 运行时异常和编译时异常区别是什么？运行时异常：编译器不会对运行时异常进行检测，没有 try-catch，方法签名中也没有 throws 关键字声明，编译依然可以通过。如果出现了 RuntimeException, 那一定是程序员的错误编译时异常：如果没有 try-catch，且方法签名中也没有用 throws 关键字声明可能抛出的异常，则编译无法通过。这类异常通常为应用环境中的错误，即外部错误，非应用程序本身错误，如文件找不到等。 throw 和 throws 的区别是什么？throw 关键字用来抛出方法或代码块中的异常，受查异常和非受查异常都可以被抛出。throws 关键字用在方法签名处，用来标识该方法可能抛出的异常列表。一个方法用 throws 标识了可能抛出的异常列表，调用该方法的方法中必须包含可处理异常的代码，否则也要在方法签名中用 throws 关键字声明相应的异常。 Java内存溢出是什么？内存溢出分三种情况。 OutOfMemoryError： PermGen space 元空间 这个区域主要用来保存加来的Class的一些信息，在程序运行期间属于永久占用的，Java的GC不会对他进行释放，所以如果启动的程序加载的信息比较大，超出了这个空间的大小，就会发生溢出错误； 解决的办法无非就是增加空间分配了——增加java虚拟机中的XX:PermSize和XX:MaxPermSize参数的大小，其中XX:PermSize是初始永久保存区域大小，XX:MaxPermSize是最大永久保存区域大小。 OutOfMemoryError：Java heap space heap 是Java内存中的堆区，主要用来存放对象，当对象太多超出了空间大小，GC又来不及释放的时候，就会发生溢出错误。 一般来说，当已存在对象没有引用(即不可达)的时候，GC就会定时的来回收对象，释放空间。但是因为程序的设计问题，导致对象可达但是又没有用(即前文提到的内存泄露)，当这种情况越来越多的时候，问题就来了。 针对这个问题，我们需要做一下两点： 1、检查程序，减少大量重复创建对象的死循环，减少内存泄露。 2、增加Java虚拟机中Xms（初始堆大小）和Xmx（最大堆大小）参数的大小。 StackOverFlowError stack是Java内存中的栈空间，主要用来存放方法中的变量，参数等临时性的数据的，发生溢出一般是因为分配空间太小，或是执行的方法递归层数太多创建了占用了太多栈帧导致溢出。 针对这个问题，除了修改配置参数-Xss参数增加线程栈大小之外，优化程序是尤其重要。 ------------------------ 反射 ------------------------ 反射什么是反射？反射是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 Java 语言的反射机制。 什么是 Java 序列化？什么情况下需要序列化？Java 序列化是为了保存各种对象在内存中的状态，并且可以把保存的对象状态再读出来。以下情况需要使用 Java 序列化： 想把的内存中的对象状态保存到一个文件中或者数据库中时候； 想用套接字在网络上传送对象的时候； 想通过RMI（远程方法调用）传输对象的时候。 动态代理是什么？有哪些应用？动态代理是运行时动态生成代理类。 动态代理的应用有 spring aop、hibernate 数据查询、测试框架的后端 mock、rpc，Java注解对象获取等。 怎么实现动态代理？JDK 原生动态代理和 cglib 动态代理。JDK 原生动态代理是基于接口实现的，而 cglib 是基于继承当前类的子类实现的。 JDK 动态代理为什么只能基于接口？java的代理对象自动继承了Proxy，又因为JAVA是单继承的，所以目标对象只能实现接口不能继承。 Jdk 和 Cglib 的区别？jdk动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。cglib动态代理是利用asm开源包，通过“继承”可以继承父类所有的公开方法，然后可以重写这些方法，在重写时对这些方法增强。 JDK动态代理和CGLIB字节码生成的区别？（1）JDK动态代理只能对实现了接口的类生成代理，而不能针对类（2）CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法因为是继承，所以该类或方法最好不要声明成final ------------------------ JVM ------------------------ JVM什么是JVM？什么是hotspot？JVM ——- 规范/标准hotspot — 产品/实现 说一下 JVM 的主要组成部分？及其作用？ 类加载器（ClassLoader） 运行时数据区（Runtime Data Area） 执行引擎（Execution Engine） 本地库接口（Native Interface） 组件的作用： 首先通过类加载器（ClassLoader）会把 Java 代码转换成字节码， 运行时数据区（Runtime Data Area）再把字节码加载到内存中，而字节码文件只是 JVM 的一套指令集规范，并不能直接交给底层操作系统去执行， 因此需要特定的命令解析器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行， 而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。 说一下类装载的执行过程？类装载分为以下 5 个步骤： 加载：将.class文件从磁盘读到内存 通过类的全限定名(com.xxx.xxx)+类加载器确定唯一的类，来获取定义此类的二进制字节流 将这个类字节流代表的静态存储结构转为方法区的运行时数据结构 在堆中生成一个代表此类的java.lang.Class对象，作为访问方法区这些数据结构的入口。 检查：检查加载的 class 文件的正确性； 文件格式验证:验证字节流是否符合 Class 文件的规范，如 主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型. 元数据验证:对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。 字节码验证:是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如:方法中的类型转换是否正确，跳转指令是否正确等。 符号引用验证:基于方法区的存储结构验证，发生在解析中，是否可以将符号引用成功解析为直接引用。 准备：给类中的静态变量分配内存空间； public static int value = 123; //此时在准备阶段过后的初始值为0而不是123，在初始化过程才会被赋值为123 public static final int value = 123;//value的值在准备阶段过后就是123。 解析：虚拟机将常量池中的符号引用替换成直接引用的过程。符号引用就理解为一个标示，而在直接引用直接指向内存中的地址； 初始化：对静态变量和静态代码块执行初始化工作。 类加载器的种类？ 启动类加载器(Bootstrap ClassLoader)：负责加载JRE的核心类库，如JRE目标下的rt.jar，charsets.jar等 扩展类加载器(Extension ClassLoader)：负责加载JRE扩展目录ext中jar类包 系统类加载器(Application ClassLoader)：负责加载ClassPath路径下的类包 用户自定义加载器(User ClassLoader)：负责加载用户自定义路径下的类包 什么是双亲委派模型？如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是把这个请求委派给父类加载器去完成，每一层的类加载器都是如此，这样所有的加载请求都会被传送到顶层的启动类加载器中，只有当父加载无法完成加载请求（它的搜索范围中没找到所需的类）时，子加载器才会尝试去加载类。 双亲委派模式的优势? 沙箱安全机制:比如自己写的String.class类不会被加载，这样可以防止核心库被随意篡改 避免类的重复加载:当父ClassLoader已经加载了该类的时候，就不需要子ClassLoader再加载一次 为什么要打破双亲委派模式?例如：tomcatTomcat是个web容器,可能需要部署两个应用程序，不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求同一个类库在同一个服务器只有一份，因此要保证每个应用程序的类库都是独立的，保证相互隔离。如果使用默认的类加载器机制，那么是无法加载两个相同类库的不同版本的，默认的类加载器是不管你是什么版本的，只在乎你的全限定类名，并且只有一份。 JVM 运行时数据区？ 程序计数器（Program Counter Register）: 前线程所执行的字节码的行号指示器，字节码解析器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成； Java 虚拟机栈（Java Virtual Machine Stacks）: 用于存储局部变量表、操作数栈、动态链接、方法出口等信息； 本地方法栈（Native Method Stack）: 与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 Java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的； Java 堆（Java Heap）: Java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存； 方法区（Methed Area）: 用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据。 Java 中都有哪些引用类型？ 强引用：通常我们使用new操作符创建一个对象时所返回的引用即为强引用 软引用：有用但不是必须的对象，在发生内存溢出之前会被回收。内存不够的时候垃圾回收器会回收。softReference适用于做缓存。 弱引用：有用但不是必须的对象，在下一次GC时会被回收。垃圾回收器遇到就会回收。WeekReference能解决某些地方的内存泄露问题。 虚引用（幽灵引用/幻影引用）：无法通过虚引用获得对象。垃圾回收器遇到就会回收，PhantomReference&lt;Object,queue&gt; 实现虚引用，虚引用的用途是在 gc 时返回一个通知放到queue中。java在申请一块堆外内存之后，会在堆内存分配一个对象保存这个堆外内存的引用，这个对象被垃圾收集器管理，一旦这个对象被回收，相应的用户线程会收到通知并对直接内存进行清理工作。 Java 堆的结构是什么样子的?堆空间一般分为新生代、老年代。 什么是新生代？新生代分为两部分:伊甸区(Eden space)和幸存者区(Survivor space)，所有的类都是在伊甸区被new出来的。幸存区(Survivor space):分为From和To区,TO区永远保持空。当Eden区的空间用完是，程序又需要创建对象，JVM的垃圾回收器将Eden区进行垃圾回收(Minor GC)，将Eden区中的不再被其它对象应用的对象进行销毁。然后将Eden区中剩余的对象移到From Survivor区。若From Survivor区也满了，再对该区进行垃圾回收，然后移动到To Survivor区，From区为空后，将To和From区转换，保证To区为空，并且对象年龄加一。当对象年龄默认加到15（因为对象头只有4个bits是存对象年龄，最大为15）时将剩下的对象移到老年代。 什么是老年代？新生代经过多次GC仍然存货的对象移动到老年区。若老年代也满了，这时候将发生Major GC(也可以叫Full GC)， 进行老年区的内存清理。若老年区执行了Full GC之后发现依然无法进行对象的保存，就会抛出 OOM(OutOfMemoryError)异常. Survivor区 到 老年代有什么条件？对象年龄到15，才会移动到老年代。因为对象头只有4个bits是存对象年龄，最大为15 JVM 有哪些垃圾回收算法？ 标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片。 标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。 复制算法：按照容量划分二个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的内存空间一次清理掉。缺点：内存使用率不高，只有原来的一半。 分代算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代基本采用复制算法，老年代采用标记整理算法。 JVM 有哪些垃圾回收器？ Serial：最早的单线程串行垃圾回收器。新生代采用复制算法，老年代采用标记-整理算法。 Serial Old：Serial 垃圾回收器的老年代版本，同样也是单线程的，可以作为 CMS 垃圾回收器的备选预案。采用标记-整理算法。 ParNew：是 Serial 的多线程版本。新生代采用复制算法，老年代采用标记-整理算法。 Parallel： 是 ParNew 收集器类似是多线程的，但 Parallel 是吞吐量优先的收集器，可以牺牲等待时间换取系统的吞吐量。新生代采用复制算法，老年代采用标记-整理算法。 Parallel Old： 是 Parallel 老年代版本，，Parallel Old 使用的是标记-整理的内存回收算法。 CMS：一种以获得最短停顿时间为目标的收集器，第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程(基本上)同时工作。是一种标记-清除算法实现 初始标记(CMS initial mark): 暂停所有的其他线程，并记录下直接与GCroot相连的对象，速度很快 并发标记(CMS concurrent mark): 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记(CMS remark): 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶 段时间短 并发清除(CMS concurrent sweep): 开启用户线程，同时GC线程开始对为标记的区域做清扫。 G1：一种兼顾吞吐量和停顿时间的 GC 实现，是 JDK 9 以后的默认 GC 选项。G1从整体来看是基于标记整理算法实现的收集器;从局部上来看是基于标记复制算法实现的 G1首先在内存结构上采用了region化的方法，将堆内存划分成2000块左右的小块，每块大小1-32M（2的幂次），每块region都可以作为E、S、O任意一种，分配灵活 小于一半region size的可以正常存入E区 一半到一个region size的直接存入O区一个region中 比一个region size还要大的对象，需要存入连续的多个region中 每一块region又有两个概念 RememberSets：又叫Rsets是每个region中都有的一份存储空间，用于存储本region的对象被其他region对象的引用记录。 CollectionSets：又叫Csets是一次GC中需要被清理的regions集合，注意G1每次GC不是全部region都参与的，可能只清理少数几个，这几个就被叫做Csets。 YGC 暂停所有的其他线程，复制算法，将E和S(from)区复制到S(to)，注意S(to)一开始是没有标识的，就是个free region MixGC：本质上不是只针对老年代，也有部分年轻代，所以又叫MixGC。 初次标记：标记GCroot直接引的对象和所在Region，与CMS不同的是，这里不止标记O区。注意初次标记一般和YGC同时发生，利用YGC的停顿时间，顺带把这事给干了。 RootRegion扫描：扫描old区中所有region，看Rset中是否有RootRegion，表示被RootRegion引用。与应用程序并发执行。 并发标记：类似CMS，在整个堆中查找可访问的（存活的）对象，这期间如果发现某个region所有对象都是’垃圾’则标记为X。 重新标记：类似CMS，但也是整个堆，并且上一步中的X区被删除。另外采用了初始标记阶段的SATB，重新标记的速度变快。 复制清除：选择所有Y区reigons和’垃圾较多’的O区regions组成Csets，进行复制清理。RootRegionScan这个阶段是干嘛的？标记出RootRegion指向O区的region，标记这些region是为了降低并发标记的扫描范围，因为并发标记需要扫描GCROOT引用或间接的所有对象，而这些对象一定是在RootRegion出发指向的Region中的。MIXGC中Y区本来就要全扫，所以这里再按照O区过滤下，这样就缩小了扫描范围。 Rset作用有哪些？ 遍历O区region查询Rset是否有来自RootRegion的 YGC时，O区不GC因而认为O区全为‘GCroot’，需扫描全部O区。有了Rset只需要查看所有Y区region的Rset就知道被哪些O区region跨带引用了，避免了扫描整个O区。 G1提高效率的点有哪些？ 重新标记时X区域直接删除。 Rset降低了扫描的范围 重新标记阶段使用SATB速度比CMS快。 清理过程为选取部分存活率低的Region进行清理，不是全部，提高了清理的效率。 什么是STAB？为了解决在并发标记过程中，存活对象漏标的情况，GC HandBook把对象分成三种颜色：1、黑色：自身以及可达对象都已经被标记2、灰色：自身被标记，可达对象还未标记3、白色：还未被标记所以，漏标的情况只会发生在白色对象中，且满足以下任意一个条件：1、并发标记时，应用线程给一个黑色对象的引用类型字段赋值了该白色对象2、并发标记时，应用线程删除所有灰色对象到该白色对象的引用对于第一种情况，利用post-write barrier，记录所有新增的引用关系，然后根据这些引用关系为根重新扫描一遍对于第二种情况，利用pre-write barrier，将所有即将被删除的引用关系的旧引用记录下来，最后以这些旧引用为根重新扫描一遍 1、写前栅栏 Pre-Write Barrrier：即将执行一段赋值语句时，等式左侧对象将修改引用到另一个对象，那么JVM就需要在赋值语句生效之前，记录丧失引用的对象。2、写后栅栏 Post-Write Barrrier：当执行一段赋值语句后，等式右侧对象获取了左侧对象的引用，同样需要记录 堆栈的区别？ 功能方面：堆是用来存放对象的，栈是用来执行程序的。 共享性：堆是线程共享的，栈是线程私有的。 空间大小：堆大小远远大于栈。 怎么判断对象是否可以被回收？引用计数器：为每个对象创建一个引用计数，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。它有一个缺点不能解决循环引用的问题；可达性分析：这个算法的基本思想就是通过一系列的称为”GC Roots”的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连的话，则证明此对象时不可用的。 * GC Roots根节点:类加载器、Thread、虚拟机栈的局部变量表、static成员、常量引用、本地方法栈的变量等等. 怎么选择垃圾收集器?（尽量由JVM自己选择） 优先调整堆的大小让服务器自己来选择 如果内存小于100m，使用串行收集器 如果是单核，并且没有停顿时间的要求，串行或JVM自己选择 如果允许停顿时间超过1秒，选择并行或者JVM自己选 如果响应时间最重要，并且不能超过1秒，使用并发收集器官方推荐ZGC(java最新版本垃圾收器器，可预测的停顿最低2ms)，性能高。 JVM 调优的工具？JDK 自带了很多监控工具，都位于 JDK 的 bin 目录下，其中最常用的是 jconsole 和 jvisualvm 这两款视图监控工具。 jconsole：用于对 JVM 中的内存、线程和类等进行监控； jvisualvm：JDK 自带的全能分析工具，可以分析：内存快照、线程快照、程序死锁、监控内存的变化、gc 变化等。 jstat: 可以查看堆内存各部分的使用量，以及加载类的数量 jstack: 生成java虚拟机当前时刻的线程快照，可以用来检测死锁。 jmap: 用来查看内存信息,导出dump文件。 jinfo: 查看正在运行的Java程序的扩展参数 jps: 显示当前系统的java进程情况 JVM 调优的参数都有哪些？-Xms2g：初始化推大小为 2g；-Xmx2g：堆最大内存为 2g；-XX:NewRatio=4：设置年轻的和老年代的内存比例为 1:4；-XX:SurvivorRatio=8：设置新生代 Eden 和 Survivor 比例为 8:2；–XX:+UseParNewGC：指定使用 ParNew + Serial Old 垃圾回收器组合；-XX:+UseParallelOldGC：指定使用 ParNew + ParNew Old 垃圾回收器组合；-XX:+UseConcMarkSweepGC：指定使用 CMS + Serial Old 垃圾回收器组合；-XX:+PrintGC：开启打印 gc 信息；-XX:+PrintGCDetails：打印 gc 详细信息。 Java 中会存在内存泄漏吗？所谓内存泄露就是指一个不再被程序使用的对象或变量一直被占据在内存中。长生命周期的对象持有短生命周期对 象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需 要，但是因为长生命周期对象持有它的引用而导致不能被回收。 finalize() 方法什么时候被调用?垃圾回收器(garbage colector)决定回收某对象时，就会运行该 对象的 finalize() 方法 但是在 Java 中很不幸，如果内存总是充 足的，那么垃圾回收可能永远不会进行，也就是说 filalize() 可能 永远不被执行，显然指望它做收尾工作是靠不住的。 那么 finalize() 究竟是做什么的呢? 它最主要的用途是回收特殊渠道申请的内存。Java 程序有垃圾回收器，所以一般情况下内存问题 不用程序员操心。但有一种 JNI(Java Native Interface)调用 non-Java 程序(C 或 C++)， finalize() 的工作就是回收这部分的内存。 深拷贝和浅拷贝区别是什么？ 浅克隆：当对象被复制时只复制它本身和其中包含的值类型的成员变量，而引用类型的成员对象并没有复制，指向同一个地址。 深克隆：除了对象本身被复制外，对象所包含的所有成员变量也将复制。 如何实现对象克隆？ 实现 Cloneable 接口并重写 Object 类中的 clone() 方法。 实现 Serializable 接口，通过对象的序列化和反序列化实现克隆，可以实现真正的深度克隆。 ------------------------ 多线程 ------------------------ 多线程什么是线程安全？如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。 如何保证线程安全？ synchronized关键字 用Lock锁 volatile+CAS【单纯的volatile是轻量级的同步机制保证可见性但是不具备原子性所以要配合CAS来实现线程安全】 atomic原子类 哪些集合类是线程安全的？Vector、Hashtable、Stack 都是线程安全的，而像 HashMap 则是非线程安全的，不过在 JDK 1.5 之后随着 Java. util. concurrent 并发包的出现，它们也有了自己对应的线程安全类，比如 HashMap 对应的线程安全类就是 ConcurrentHashMap。 并行和并发有什么区别? 并行：多个处理器或多核处理器同时处理多个任务。( 两个队列和一台咖啡机 ) 并发：多个任务在同一个 CPU 核上，按细分的时间片轮流(交替)执行，从逻辑上来看那些任务是同时执行。( 两个队列和两台咖啡机 ) 线程和进程的区别?一个程序下至少有一个进程，一个进程下至少有一个线程，一个进程下也可以有多个线程来增加程序的执行速度。 守护线程是什么?守护线程是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。在 Java 中垃圾回收线程就是特殊的守护线程。 创建线程有哪几种方式? 继承 Thread 重写 run 方法； 实现 Runnable 接口； 实现 Callable 接口。(可以获取线程执行之后的返回值) 线程池 Runnable和Callable的区别? Callable规定（重写）的方法是call()，Runnable规定（重写）的方法是run()。 Callable的任务执行后可返回值，而Runnable的任务是不能返回值的。 call方法可以抛出异常，run方法不可以。 运行Callable任务可以拿到一个Future对象，表示异步计算的结果。它提供了检查计算是否完成的方法，以等待计算的完成，并检索计算的结果。通过Future对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果。 FutureTask 和 Future 的区别？FutureTask实现了RunnableFuture接口，而RunnableFuture继承了Runnable接口和Future接口。所以FutureTask既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值。 线程有哪些状态? NEW 尚未启动 RUNNABLE 就绪状态 RUNNING 执行状态 BLOCKED 阻塞的（被同步锁或者IO锁阻塞） WAITING 等待状态 TIMED_WAITING 超时等待状态 TERMINATED 执行完成 sleep() 和 wait() 有什么区别? 类的不同：sleep() 来自 Thread，wait() 来自 Object。 释放锁：sleep() 不释放锁；wait() 释放锁。 用法不同：sleep() 时间到会自动恢复；wait() 可以使用 notify()/notifyAll()直接唤醒。 sleep() 和 yield() 区别？ sleep() 方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield() 方法只会给相同优先级或更高优先级的线程以运行的机会； 线程执行 sleep() 方法后转入阻塞（blocked）状态，而执行 yield() 方法后转入就绪（Runnable）状态； sleep() 方法声明抛出InterruptedException，而 yield() 方法没有声明任何异常； sleep() 方法比 yield() 方法（跟操作系统相关）具有更好的可移植性。 notify()和 notifyAll()有什么区别？ notifyAll: 会唤醒所有的线程,会将全部线程由等待池移到锁池,然后参与锁的竞争，竞争成功则继续执行，如果不成功则留在锁池等待锁被释放后再次参与竞争 notify: 唤醒一个线程,具体唤醒哪一个线程由虚拟机控制。 线程的 run() 和 start() 有什么区别？start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。run() 可以重复调用，而 start() 只能调用一次。 park() 的作用LockSupport类中的 park(Object blocker) 表示阻塞指定线程，参数blocker当前线程对象，使线程进入等待状态，释放cpu,并不会抛出中断异常unpark(Thread thread) 唤醒指定线程，参数thread指定线程对象 创建线程池有哪几种方式？ public static ExecutorService newCachedThreadPool() 默认corePoolSize = 0，maximumPoolSize= 2147483647。 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程，但是在之前构造的线程可用时将重用它们。 public static ExecutorService newFixedThreadPool(int nThreads) 默认corePoolSize = nThreads，maximumPoolSize= nThreads。 创建一个定长线程池，可控制线程最大并发数，以共享的无界队列方式来运行线程，超出的线程会在队列中等待。 public static ExecutorService newSingleThreadExecutor() 默认corePoolSize = 1，maximumPoolSize= 1。 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，以无界队列方式来运行线程，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) 默认corePoolSize = corePoolSize，maximumPoolSize= 2147483647。 创建一个周期线程池，支持定时及周期性任务执行。 public static ExecutorService newWorkStealingPool() 创建持有足够线程的线程池来支持给定的并行级别，并通过使用多个队列，减少竞争，它需要穿一个并行级别的参数，如果不传，则被设定为默认的CPU数量，这个线程池实际上是ForkJoinPool的扩展，适合使用在很耗时的任务中，能够合理的使用CPU进行并行操作。 线程池流程？ 线程池中 submit() 和 execute() 方法有什么区别？ execute()：只能执行 Runnable 类型的任务。 submit()：可以执行 Runnable 和 Callable 类型的任务。 什么是CAS？全称Compare and swap，比较并交换。CAS机制当中使用了3个基本操作数：内存地址V，旧的预期值A，要修改的新值B。只有当变量的预期值A和内存地址V当中的实际值相同时，才会将内存值修改为 Bjava 的 CAS 利用的的是 unsafe（该类都是基于JVM对操作系统进行的操作，因此是unsafe）这个类提供的 CAS 操作unsafe 的CAS 依赖了的 JVM 针对不同的操作系统实现的 Atomic::cmpxchgAtomic::cmpxchg 的实现使用了汇编的 cas 操作，并使用 cpu 硬件提供的 lock信号保证其原子性 CAS有什么问题？ABA问题。举个例子：假设有一个遵循CAS原理的提款机，小灰有100元存款，要用这个提款机来提款50元。由于提款机硬件出了点小问题，小灰的提款操作被同时提交两次，开启了两个线程，两个线程都是获取当前值100元，要更新成50元。理想情况下，应该一个线程更新成功，另一个线程更新失败，小灰的存款只被扣一次。线程1首先执行成功，把余额从100改成50。线程2因为某种原因阻塞了。这时候，小灰的妈妈刚好给小灰汇款50元。线程2仍然是阻塞状态，线程3执行成功，把余额从50改成100。线程2恢复运行，由于阻塞之前已经获得了“当前值”100，并且经过compare检测，此时存款实际值也是100，所以成功把变量值100更新成了50。小灰凭空少了50元钱。 解决方案：使用携带版本的号的AtomicStampedReference 什么是AQS？AbstractQueuedSynchronizer，抽象队列同步器AQS就是一个并发包的基础组件，是一个抽象类，用来实现各种锁，各种同步组件的。它包含了state变量、加锁线程、等待队列等并发中的核心组件。常见的实现类有:ReentrantLock等。 AQS原理？ AQS对象内部有一个核心的变量叫做state，是int类型的，代表了加锁的状态，初始状态下，这个state的值是0。 AQS内部还有一个关键变量，用来记录当前加锁的是哪个线程，初始化状态下，这个变量是null。 AQS内部还有一个等待队列，专门放那些加锁失败的线程 当 线程1 尝试进行加锁，用CAS操作将state值从0变为1。如果CAS操作成功，将当前加锁线程设置成自己。线程1 可重入加锁时，每次判断当前加锁线程是否是自己，是的话state+1当 线程2 尝试获取锁，用CAS操作将state值从0变为1，失败！！！然后检查当前加锁线程是否是自己，那当然不是。线程2 会将自己放入AQS中的一个等待队列，等待 线程1 释放锁之后，自己就可以重新尝试加锁了。线程1 释放锁就是将state变量的值递减1，将当前加锁线程设置成null。接下来，会从等待队列的队头唤醒 线程2 重新尝试加锁。重复上述过程。 ReentrantLock 原理？ReentrantLock 可是分为公平锁和非公平锁，默认的构造函数是非公平锁公平锁：当判断到锁状态字段state == 0 时，不会立马将当前线程设置为该锁的占用线程，而是去判断是在此线程之前是否有其他线程在等待这个锁，然后再CAS操作获取锁非公平锁：没有判断是否有在此之前的排队线程，而是直接CAS操作进行获锁，此多个线程之间同时争用一把锁的时候，谁先获取到就变得随机了加锁过程：线程A，首先获取state的值判断是否为0，若state == 0。 * 判断是自己是否需要排队 * 若需要排队，也就是 队列的 head != tail 也就是队列已初始化了 并且 （head.next !=null 或 head.next.thread != 当前线程） * 将head.next 指向A，A.pre 指向head。 * 判断A是否是排队的第一个线程 * 若不需要排队，尝试进行加锁，用CAS操作将state值从0变为1。 * 如果CAS操作成功，将当前加锁线程设置成自己。 * 如果CAS操作失败，会检查当前加锁线程是否是自己。 * 若是当前线程，就是重入锁，将state++。 * 若不是当前线程，将线程放入等待队列。 * 如果队列为空，初始化一个thread=null的node作为队列的头head，将head.next 指向A，A.pre 指向head。 * 判断A是否是排队的第一个线程 * 如果A.pre == head ,表示A是排队的第一个线程，自旋两次尝试加锁，将A.pre 的ws值改为-1，若还失败则使用park()进行线程阻塞。 * 如果不是，则直接使用park()进行线程阻塞。 释放锁：将state变量的值递减1，如果state == 0，将当前加锁线程设置成null。如果队列不为空，将队列的head节点的ws值改为0，然后用unpark()唤醒 head.next也就是A线程，然后将队列的head设置为A，A节点的thread设置为null。 什么是锁？锁(lock)或互斥(mutex)是一种同步机制，用于在有许多执行线程的环境中强制对资源的访问限制。锁旨在强制实施互斥排他、并发控制策略。锁的作用就是保证多线程同步执行。 Java中有哪些锁？公平锁、非公平锁、读写锁、共享锁、互斥锁、自旋锁、偏向锁、轻量级锁、重量级锁等等。 什么是死锁？线程A持有独占锁资源a，并尝试去获取独占锁资源b同时，线程B持有独占锁资源b，并尝试去获取独占锁资源a这样线程A和线程B相互持有对方需要的锁，从而发生阻塞，最终变为死锁。 死锁发生的必要条件？ 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 怎么防止死锁？ 尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。 尽量使用 Java. util. concurrent 并发类代替自己手写锁。 尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。 尽量减少同步的代码块。 当一个线程进入一个对象的一个 synchronized 方法后，其它线程是否可进入此对象的其它方法? 其他方法前是否加了 synchronized 关键字，如果没加，则能。 如果这个方法内部调用了 wait，则可以进入其他 synchronized 方法。 synchronized(l){XX}是锁了代码块还是对象？怎么实现？锁了对象l，那么是对l做了什么来表示l被锁了呢？上锁就是改变了对象头的锁标记。 线程调度(优先级)与线程休眠类似，线程的优先级仍然无法保障线程的执行次序。只不过，优先级高的线 程获取 CPU 资源的概率较大，优先级低的并非没机会执行。线程的优先级用 1-10 之 间的整数表示，数值越大优先级越高，默认的优先级为 5。 在一个线程中开启另外一个新线程，则新开线程称为该线程的子线程，子线程初始优先级与父线程相同。 什么是线程饥饿？线程饥饿是另一种活跃性问题，也可以使程序无法执行下去。如果一个线程因为处理器时间全部被其他线程抢走而得不到处理器运行时间，这种状态被称之为饥饿一般是由高优先级线程吞噬所有的低优先级线程的处理器时间引起的。 什么是活锁？这两个线程虽然都没有停止运行，但是却无法向下执行，这种情况就是所谓的活锁。举个例子，两个人在走廊上碰见，大家都互相很有礼貌，互相礼让，A从左到右，B也从从左转向右，发现又挡住了地方，继续转换方向，但又碰到了，反反复复，一直没有机会运行下去。 当所有线程在序中执行 Object.wait(0)，参数为 0 的 wait 方法。程序将发生 活锁直到在相应的对象上有线程调用 Object.notify() 或者 Object.notifyAll()。 Volatile关键字的作用？让其他线程能够马上感知到某一线程多某个变量的修改 保证可见性:对共享变量的修改，其他的线程马上能感知到 保证有序性:禁止重排序（编译阶段、指令优化阶段）volatile之前的代码不能调整到他的后面，volatile之后的代码不能调整到他的前面 volatile 能使得一个非原子操作变成原子操作吗?在 Java 中除了 long 和 double 之外的所有基本类型的读和赋值，都是原子性操作。而 64 位的 long 和 double 变量由于会被 JVM 当作两个分离的 32 位来进行操 作，所以不具有原子性，会产生字撕裂问题。但是当你定义 long 或 double 变量时， 如果使用 volatile 关键字，就会获到(只有简单的赋值与返回操作的)原子性。不能保证其他情况的原子性。 synchronized 和 volatile 的区别是什么？ volatile 是变量修饰符；synchronized 是修饰类、方法、代码段。 volatile 仅能实现变量的修改可见性，不能保证原子性；而 synchronized 则可以保证变量的修改可见性和原子性。 volatile 不会造成线程的阻塞；synchronized 可能会造成线程的阻塞。 synchronized 和 Lock 有什么区别？ synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。 synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 synchronized 和 ReentrantLock 区别是什么？ ReentrantLock 使用起来比较灵活，但是必须有释放锁的配合动作； ReentrantLock 必须手动获取与释放锁，而 synchronized 不需要手动释放和开启锁； ReentrantLock 只适用于代码块锁，而 synchronized 可用于修饰方法、代码块等。 性能区别：由于线程执行是交替执行。jdk1.6 synchronized 是重量级锁，要频繁调用操作系统函数，交替执行，从用户态转到内核态效率低。jdk1.8 synchronized 增加了偏向锁和轻量级锁，也是在jdk层面解决，所以和 ReentrantLock 性能差不多。ReentrantLock 将线程交替执行放到jdk层面解决，所以比1.6的synchronized效率高如果线程是单线程或者线程是交替执行，那么ReentrantLock队列不会进行初始化，不会发送阻塞，都在jdk层面运行。而1.6 synchronized需要调用os操作更改锁对象状态，然后进入同步块。 synchronized 底层实现原理？synchronized 是由一对 monitorenter/monitorexit 指令实现的，monitor 对象是同步的基本实现单元。在 Java 6 之前，monitor 的实现完全是依靠操作系统内部的互斥锁，因为需要进行用户态到内核态的切换，所以同步操作是一个无差别的重量级操作，性能也很低。但在 Java 6 的时候，Java 虚拟机 对此进行了大刀阔斧地改进，提供了三种不同的 monitor 实现，也就是常说的三种不同的锁：偏向锁（Biased Locking）、轻量级锁和重量级锁，大大改进了其性能。 什么是Monitor？Monitor其实是一种同步工具，也可以说是一种同步机制，它通常被描述为一个对象,也常被翻译为“监视器\管程”,每个对象都会有一个 monitor 某一线程占有一个对象的时候，先看该对象的 monitor 的计数器是不是0，如果是0表示这个对象还没有线程占有，这个时候线程占有这个对象，并且对这个对象的monitor+1；如果不为0，表示这个对象已经被其他线程占有，这个线程等待。当线程释放占有权的时候，monitor-1； 同一线程可以对同一对象进行多次加锁，+1，+1，体现了重入性 多线程中 synchronized 锁升级的原理是什么？在锁对象的对象头里面有一个 threadid 字段，在第一次访问的时候 threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断 threadid 是否与其线程 id 一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。 什么是偏向锁？当线程请求到锁对象后，将锁对象的状态标志位改为01，即偏向模式。然后使用CAS操作将线程的threadid记录在锁对象的Mark Word中。以后该线程可以直接进入同步块，连CAS操作都不需要。但是，一旦有第二条线程需要竞争锁，那么偏向模式立即结束，进入轻量级锁的状态。 什么是轻量级锁？从偏向锁的状态发生竞争后，会锁膨胀变成轻量级锁。轻量级锁是在内存有一个锁记录，当某个线程获取到锁后，会将锁记录保存到线程内存中，同时将原本的锁记录指向它的线程内存中的备份锁记录位置。其他线程判断锁记录已经指向了其他线程，因此会进入自旋状态。达到一定的次数或时间后，会膨胀变成重量级锁。 悲观锁和乐观锁的概念？ 乐观锁：乐观地认为所有的并发操作都是线程安全的，不需要加锁来处理。但为了保证数据没有被修改，会通过CAS操作来进行判定和替换。 悲观锁：悲观地认为所有的并发操作都是线程不安全的，都需要加锁来保证线程安全 ThreadLocal 是什么？有哪些使用场景？ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。 ThreadLocal 的经典使用场景是数据库连接和 session 管理等。 ThreadLocal原理？ 每个Thread维护着一个ThreadLocalMap的引用 ThreadLocalMap是ThreadLocal的内部类，用Entry来进行存储 调用ThreadLocal的set()方法时，实际上就是往ThreadLocalMap设置值，key是ThreadLocal对象，值是传递进来的对象 调用ThreadLocal的get()方法时，实际上就是往ThreadLocalMap获取值，key是ThreadLocal对象 ThreadLocal本身并不存储值，它只是作为一个key来让线程从ThreadLocalMap获取value。 ThreadLocal缺点？ThreadLocal t1 = new ThreadLocal&lt;&gt;();ThreadLocal 中 ThreadLocalMap 中的key和value是保存在Entry中，但是Entry实现了WeekReference，因此key是一个虚引用指向ThreadLocal对象。假设entry是个强引用，那么t1 = null时，ThreadLocal对象还被key强引用，导致无法回收。生命周期跟Thread一样长，只能等待线程结束。Entry是个弱引用，那么在t1 = null时，ThreadLocal对象 会在内存不够的时候被回收。但是还有一个问题，当ThreadLocal被回收时，key=null，那么value再也无法被访问到，还是存在内存溢出的问题。所以当ThreadLocal使用完成之后要调用remove方法。 ThreadLocal由于真正存储数据的ThreadLocalMap只单纯地采取了数组的形式来存储数据，因此出现hash冲突时会为hash值就增加一个固定的大小0x61c88647进行线性寻找位置，会导致大量的hash冲突，造成很高的资源消耗。 什么是CyclicBarrier？栅栏(Barrier)类似于闭锁，他能阻塞一组线程直到某个事件发生后再全部同时执行。CyclicBarrier 字面意思是回环栅栏，回环的意思是它能够被重复利用，当然前提是在所有线程释放了以后。 什么是CountDownLatch？CountDownLatch也叫闭锁,使得一(多)个主线程必须等待其他线程完成操作后再执行.CountDownLatch内部维护一个计数器(父类的int state),主线程先执行await方法，如果此时计数器大于0，则阻塞等待。当一个线程完成任务后，计数器值减1。直到计数器为0时，表示所有的线程已经完成任务，等待的主线程被唤醒继续执行。 CyclicBarrier 和 CountDownLatch 的区别？ CyclicBarrier 是所有线程必须同时到达栅栏位置，才能继续执行。它用于等待其他线程，并且能够重置使用。 CountDownLatch 用于等待事件，是一次性对象，一旦进入终止状态，就不能被重置。 CountDownLatch 通常阻塞的是主线程，开锁以后主线程才继续执行。 CyclicBarrier 阻塞的是子线程，到达栅栏位置后，每个线程还可以继续做自己后续的事情。 10个线程执行，然后主线程必须等10个线程都执行完了，然后获取到10个线程的计算结果，然后才能计算出自己的结果，也就是说必须等待10个线程都执行完了，我才执行，如何做？使用CountDownLatch，或者Thread.join() 123456789101112final CountDownLatch latch = new CountDownLatch(10);for(int i = 0 ; i &lt; 10 ; i ++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); latch.countDown(); &#125; &#125;).start();&#125;latch.await();System.out.println("main"); 123456789101112131415List&lt;Thread&gt; list = new ArrayList&lt;&gt;();for(int i = 0 ; i &lt; 10 ; i ++)&#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125; &#125;); list.add(thread); thread.start();&#125;for(int i = 0 ; i &lt; 10 ; i ++)&#123; list.get(i).join();&#125;System.out.println("main"); IO字节流和字符流？字节流：以字节（8bit）为单位，能处理所有类型的数据字符流：以字符为单位，根据码表映射字符，一次可能读多个字节，只能处理字符类型的数据。InputStreamReader:字节到字符的桥梁OutputStreamWriter:字符到字节的桥梁 NIO是什么？NIO（Non-blocking I/O，在Java领域，也称为New I/O），是一种同步非阻塞的I/O模型。说到NIO就得先从传统BIO（Blocking I/O）说起。BIO：传统的网络通讯模型，就是BIO，同步阻塞IO它其实就是服务端创建一个ServerSocket， 然后就是客户端用一个Socket去连接服务端的那个ServerSocket， ServerSocket接收到了一个的连接请求就创建一个Socket和一个线程去跟那个Socket进行通讯。服务端会在等待连接、等待读取或者等待写入时发生阻塞。这种方式的缺点：每次一个客户端接入，都需要在服务端创建一个线程来服务这个客户端。这样大量客户端来的时候，就会造成服务端的线程数量可能达到了几千甚至几万，这样就可能会造成服务端过载过高，最后崩溃死掉。因此我们要在BIO的基础上进行改进。 假设一个单线程的Socket服务，那么它会在accept()也就是等待客户端连接时阻塞，当有客户端A连接时，服务端会等待read()服务端发来的的数据。首先，如果可以让read（）不阻塞，那么在客户端A连接之后，如果读到数据，就进行逻辑操作，如果没读到数据，我们就继续等待下一个客户端连接那么此时有客户端B连接，这时A发送数据，就接受不到数据了，所以我们再增加一个list保存socket连接，也就是在A连接之后，我们就用list保存这个socket连接，然后轮询这个list，看看有没有客户端发送数据，有就处理，然后继续等待下一个客户端连接。此时 accept（）操作还是阻塞的，在没有客户端连接时，还是接收不到数据。如果可以让accept（）不阻塞，在没有客户端连接的时候，也进行轮询这个list，看看有没有客户端发送数据。这样就实现了一个简单的nio。java的nio就是将上述操作中的轮询操作交给操作系统来进行了。 ------------------------ Spring ------------------------ Spring基础为什么要使用 spring？ spring 提供 ioc 技术，容器会帮你管理依赖的对象，从而不需要自己创建和管理依赖对象了，更轻松的实现了程序的解耦。 spring 提供了事务支持，使得事务操作变的更加方便。 spring 提供了面向切片编程，这样可以更方便的处理某一类的问题。 更方便的框架集成，spring 可以很方便的集成其他框架，比如 MyBatis 等。 spring 有哪些主要模块？ spring core：框架的最基础部分，提供 ioc 和依赖注入特性。 spring context：构建于 core 封装包基础上的 context 封装包，提供了一种框架式的对象访问方法。 spring dao：Data Access Object 提供了JDBC的抽象层。 spring aop：提供了面向切面的编程实现，让你可以自定义拦截器、切点等。 spring Web：提供了针对 Web 开发的集成特性，例如文件上传，利用 servlet listeners 进行 ioc 容器初始化和针对 Web 的 ApplicationContext。 spring Web mvc：spring 中的 mvc 封装包提供了 Web 应用的 Model-View-Controller（MVC）的实现。 spring mvc流程 AOP什么是 aop？aop 是面向切面编程，可以通过预编译方式和运行期动态代理实现在不修改源代码的情况下给程序动态统一添加功能的一种技术。aop的应用场景：日志记录、权限验证、效率检查、事务管理、exception AOP术语？ 切面(Aspect)：切面是通知和切点的结合，通知和切点共同定义了切面的全部内容 连接点(Join point)：目标对象中的方法。 通知(Advice)：定义了切面是做什么以及何时使用。 切点(Pointcut)：表示连接点的集合。（PointCut是JoinPoint的谓语，这是一个动作，主要是告诉通知连接点在哪里，切点表达式决定 JoinPoint 的数量） 目标对象(Target object)：目标对象 原始对象 aop代理(AOP proxy)：代理对象 包含了原始对象的代码和增加后的代码的那个对象 织入(Weaving)：把代理逻辑加入到目标对象上的过程 AOP实现原理？通过动态代理实现。动态代理又分为jdk动态代理和cglib动态代理。 jdk动态代理：主要通过Proxy.newProxyInstance()和InvocationHandler这两个类和方法实现 实现过程 实现InvocationHandler接口，重写invoke()方法 调用Proxy.newProxyInsatnce(classloader,interfaces,handler)方法生成代理类 生成的代理类为$Proxy0 extends Proxy implements Person 因为已经继承了Proxy,所以java动态代理只能对接口进行代理 总结： 代理类调用自己方法时，通过自身持有的中介类对象来调用中介类对象的invoke方法，从而达到代理执行被代理对象的方法。 IOC什么是 ioc？控制反转，将你设计好的对象交给容器控制，可以用来减低计算机代码之间的耦合度。 什么是DI？DI是依赖注入，是实现IOC的一种方式。 spring 常用的注入方式有哪些？ setter 属性注入 构造方法注入 注解方式注入 spring 中的 bean 是线程安全的吗？spring 中的 bean 默认是单例模式，spring 框架并没有对单例 bean 进行多线程的封装处理。实际上大部分时候 spring bean 无状态的（比如 dao 类），所有某种程度上来说 bean 也是安全的，但如果 bean 有状态的话（比如 view model 对象），那就要开发者自己去保证线程安全了，最简单的就是改变 bean 的作用域，把“singleton”变更为“prototype”，这样请求 bean 相当于 new Bean()了，所以就可以保证线程安全了。 有状态就是有数据存储功能。 无状态就是不会保存数据。 spring 自动装配 bean 有哪些方式？ no：默认值，表示没有自动装配，应使用显式 bean 引用进行装配。 byName：它根据 bean 的名称注入对象依赖项。 byType：它根据类型注入对象依赖项。 constructor：通过构造函数来注入依赖项，需要设置大量的参数。 Spring beanFactory 和 factoryBean 的区别？beanFactory 是Spring容器的顶层接口，用于管理Bean的一个工厂。 在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。FactoryBean 这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean，它能在需要的时候生产一个对象，且不仅仅限于它自身，它能返回任何Bean的实例。 通常情况下，bean 无须自己实现工厂模式，Spring 容器担任了工厂的 角色；但少数情况下，容器中的 bean 本身就是工厂，作用是产生其他 bean 实例。由工厂 bean 产生的其他 bean 实例，不再由 Spring 容器产生，因此与普通 bean 的配置不同，不再需要提供 class 元素。 BeanFactory和ApplicationContext是什么关系？最主要的区别是BeanFactory是延迟加载，ApplicationContext是即时加载。 spring 支持几种 bean 的作用域？ singleton：spring ioc 容器中只存在一个 bean 实例，bean 以单例模式存在，是系统默认值； prototype：每次从容器调用 bean 时都会创建一个新的示例，既每次 getBean()相当于执行 new Bean()操作； Web 环境下的作用域： request：每次 http 请求都会创建一个 bean； session：同一个 http session 共享一个 bean 实例； global-session：用于 portlet 容器，因为每个 portlet 有单独的 session，globalsession 提供一个全局性的 http session。 spring bean 容器的生命周期是什么样的？ Spring 容器根据配置中的 bean 定义，通过构造方法反射来实例化 bean。 Spring 使用依赖注入填充所有属性，如 bean 中所定义的配置。 如果 bean 实现 BeanNameAware 接口，则工厂通过传递 bean 的 ID 来调用 setBeanName()。 如果 bean 实现 BeanFactoryAware 接口，工厂通过传递自身的实例来调用 setBeanFactory()。 如果存在与 bean 关联的任何 BeanPostProcessors，则调用 preProcessBeforeInitialization() 方法。 如果为 bean 指定了 init 方法（ 的 init-method 属性），那么将调用它。 最后，如果存在与 bean 关联的任何 BeanPostProcessors，则将调用 postProcessAfterInitialization() 方法。 如果 bean 实现 DisposableBean 接口，当 spring 容器关闭时，会调用 destory()。 如果为 bean 指定了 destroy 方法（ 的 destroy-method 属性），那么将调用它。 1：实例化一个ApplicationContext的对象；2：调用bean工厂后置处理器完成扫描；3：循环解析扫描出来的类信息；4：实例化一个BeanDefinition对象来存储解析出来的信息；5：把实例化好的beanDefinition对象put到beanDefinitionMap当中缓存起来，以便后面实例化bean；6：再次调用bean工厂后置处理器；7：当然spring还会干很多事情，比如国际化，比如注册BeanPostProcessor等等，如果我们只关心如何实例化一个bean的话那么这一步就是spring调用finishBeanFactoryInitialization方法来实例化单例的bean，实例化之前spring要做验证，需要遍历所有扫描出来的类，依次判断这个bean是否Lazy，是否prototype，是否abstract等等；8：如果验证完成spring在实例化一个bean之前需要推断构造方法，因为spring实例化对象是通过构造方法反射，故而需要知道用哪个构造方法；9：推断完构造方法之后spring调用构造方法反射实例化一个对象；注意我这里说的是对象、对象、对象；这个时候对象已经实例化出来了，但是并不是一个完整的bean，最简单的体现是这个时候实例化出来的对象属性是没有注入，所以不是一个完整的bean；10：spring处理合并后的beanDefinition(合并？是spring当中非常重要的一块内容)；11：判断是否支持循环依赖，如果支持则提前把一个工厂存入singletonFactories——map；12：判断是否需要完成属性注入13：如果需要完成属性注入，则开始注入属性14：判断bean的类型回调Aware接口15：调用生命周期回调方法16：如果需要代理则完成代理17：put到单例池——bean完成——存在spring容器当中 Spring中的aware接口什么作用？BeanNameAware接口是为了让自身Bean能够感知到，获取到自身在Spring容器中的id属性，也就是beanName；其他的Aware接口也是为了能够感知到自身的一些属性。比如实现了ApplicationContextAware接口的类，能够获取到ApplicationContext。实现了BeanFactoryAware接口的类，能够获取到BeanFactory对象。 Spring 在什么时候完成的依赖注入？在初始化的时候。也就是在new AnnotationConfigApplicationContext的时候 Spring中的循环依赖是怎么解决的？spring 单例情况下是支持循环依赖的。假设A对象和B对象循环依赖，那么在初始化时会经历如下步骤：创建A -&gt; 实例化A -&gt; 填充A的属性 -&gt; getBean(B) -&gt; 创建B -&gt; 实例化B -&gt; 填充B的属性-&gt; getBean(A)-&gt; 若二级缓存中存在，将从二级缓存工厂中生产出的A放入三级缓存-&gt;从三级缓存中获取A-&gt;。。。 @Resource 和 @Autowired 的区别？ 默认装配类型不一样： @Autowired默认按type装配：默认情况下它要求依赖对象必须存在，如果允许null值，可以设置它required属性为false。 @Resource 默认按照名称进行装配：当找不到与名称匹配的bean才会按照类型装配 bean 在初始化时 调用的后置处理器不一样， @Autowired 调用的是 AutowiredAnnotationBeanPostProcessor() @Resource 调用的是 CommonAnnotationBeanPostProcessor() Spring 三级缓存？每一级缓存都是一个map。一级：singletonObjects 单例池，主要存放单例 bean二级：singletonFactories 主要存放ObjectFactory类型工厂对象三级：earlySingletonObjects 从工厂中获取的spring 对象，也就是半成品bean三级缓存存在的意义是防止工厂重复执行对对象的操作，提高性能。 @Qualifier 注解有什么用？当您创建多个相同类型的 bean 并希望仅使用属性装配其中一个 bean 时，您可以使用@Qualifier 注解和 @Autowired 通过指定应该装配哪个确切的 bean 来消除歧义。 什么是事务传播行为？事务传播行为（propagation behavior）指的就是当一个事务方法被另一个事务方法调用时，这个事务方法应该如何运行。例如：methodA方法调用methodB方法时，methodB是继续在调用者methodA的事务中运行呢，还是为自己开启一个新事务运行，这就是由methodB的事务传播行为决定的。 PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，这是最常见的选择，也是Spring默认的事务传播行为。 methodA 调用 methodB时，设置B为当前传播行为，A未开启事物，B在自己的事物中运行，A开启事物，则B加入A的事物，若A发生异常，则B会进行回滚操作。 PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。 A开启事物，B也创建自己的事物，若A发生异常，B不影响。 PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。 A未开启事物，则B以非事物方式执行，若A发生异常，B不影响；若A开启事物，则B加入A的事物，若A发生异常，则B会进行回滚操作。 PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 A未开启事物，则直接抛出异常；若A开启事物，则B加入A的事物，若A发生异常，则B会进行回滚操作。 PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 无论A开不开启事物，B都以非事物方式执行 PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 A未开启事物，则B以非事物方式执行；A开启事物，则抛出异常； PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。 A未开启事物，B在自己的事物中运行；若A开启事物，则在嵌套事物中执行，若A抛出异常，则B会回滚。 Spring 中事务回滚机制？Spring事务回滚机制是这样的：当所拦截的方法有指定异常抛出，事务才会自动进行回滚！①被拦截方法-—— 注解式：方法或者方法所在类被@Transactional注解；②异常—— 该方法的执行过程必须出现异常，这样事务管理器才能被触发，并对此做出处理；③指定异常—— 默认配置下，事务只会对Error与RuntimeException及其子类这些UNChecked异常，做出回滚。一般的Exception这些Checked异常不会发生回滚（如果一般Exception想回滚要做出配置）；spring aop 异常捕获原理：被拦截的方法需显式抛出异常，并不能经任何处理（如果自己捕获就不能被声明式事务感知），这样aop代理才能捕获到方法的异常，才能进行回滚，默认情况下aop只捕获runtimeexception的异常，但可以通过 ---------------- 计算机网络 ---------------- 计算机网络基础OSI 的七层模型都有哪些？ 物理层：利用传输介质为数据链路层提供物理连接，实现比特流的透明传输。 数据链路层：负责建立和管理节点间的链路。 网络层：通过路由选择算法，为报文或分组通过通信子网选择最适当的路径。 传输层：向用户提供可靠的端到端的差错和流量控制，保证报文的正确传输。 会话层：向两个实体的表示层提供建立和使用连接的方法。 表示层：处理用户信息的表示问题，如编码、数据格式转换和加密解密等。 应用层：直接向用户提供服务，完成用户希望在网络上完成的各种工作。 http协议Hyper Text Transfer Protocol（超文本传输协议）的缩写,是用于从万维网（WWW:World Wide Web ）服务器传输超文本到本地浏览器的传送协 HTTP的特性 HTTP构建于TCP/IP协议之上，默认端口号是80 HTTP是无连接无状态的 无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 HTTP状态码{% note info %} 1xx：指示信息--表示请求已接收，继续处理 2xx：成功--表示请求已被成功接收、理解、接受 3xx：重定向--要完成请求必须进行更进一步的操作 4xx：客户端错误--请求有语法错误或请求无法实现 5xx：服务器端错误--服务器未能实现合法的请求 {% endnote %} 200 OK //客户端请求成功301 Moved Permanently //永久重定向。302 Found //暂时重定向。400 Bad Request //客户端请求有语法错误，不能被服务器所理解401 Unauthorized //请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用403 Forbidden //服务器收到请求，但是拒绝提供服务404 Not Found //请求资源不存在，eg：输入了错误的URL500 Internal Server Error //服务器发生不可预期的错误503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常 HTTP 请求/响应的步骤？ 1、客户端连接到Web服务器 一个HTTP客户端，通常是浏览器，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。例如，http://www.oakcms.cn。 2、发送HTTP请求 通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。 3、服务器接受请求并返回HTTP响应 Web服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。 4、释放连接TCP连接 若connection 模式为close，则服务器主动关闭TCP连接，客户端被动关闭连接，释放TCP连接;若connection 模式为keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求; 5、客户端浏览器解析HTML内容 客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。 浏览器请求过程？ 1、浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址; 2、解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立TCP连接; 3、浏览器发出读取文件(URL 中域名后面部分对应的文件)的HTTP 请求，该请求报文作为 TCP三次握手的第三个报文的数据发送给服务器; 4、服务器对浏览器请求作出响应，并把对应的 html 文本发送给浏览器; 5、释放 TCP连接; 6、浏览器将该 html 文本并显示内容; get 和 post 请求有哪些区别？从原理性看： 根据HTTP规范，GET用于信息获取，而且应该是安全和幂等的 根据HTTP规范，POST请求表示可能修改服务器上资源的请求 从表面上看： GET请求的数据会附在URL后面，POST的数据放在HTTP包体，POST安全性比GET安全性高 GET请求会被浏览器主动缓存，并且传递参数有大小限制，POST没有。 TCP／UDP协议tcp 和 udp的区别？tcp 和 udp 是 OSI 模型中的运输层中的协议。tcp 提供可靠的通信传输，而 udp 则常被用于让广播和细节控制交给应用的通信传输。区别如下： tcp 面向连接，udp 面向非连接即发送数据前不需要建立链接； tcp 提供可靠的服务（数据传输），udp 无法保证； tcp 面向字节流，udp 面向报文； tcp 数据传输慢，udp 数据传输快； tcp协议是100%可靠的么？TCP 并不能保证数据一定会被对方接收到，因为这是不可能的。TCP 能够做到的是，如果有可能，就把数据递送到接收方，否则就（通过放弃重传并且中断连接这一手段）通知用户。因此准确说 TCP 也不是 100% 可靠的协议，它所能提供的是数据的可靠递送或故障的可靠通知。 三次握手三次握手的目的是连接服务器指定端口，建立 TCP 连接，并同步连接双方的序列号和确认号，交换 TCP 窗口大小信息。在 socket 编程中，客户端执行 connect() 时。将触发三次握手。 第一次握手(SYN=1, seq=x):客户端发送一个 TCP 的 SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号 X,保存在包头的序列号(Sequence Number)字段里。发送完毕后，客户端进入 SYN_SEND 状态。 第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):服务器发回确认包(ACK)应答。即 SYN 标志位和 ACK 标志位均为1。服务器端选择自己 ISN 序列号，放到 Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的 ISN 加1，即X+1。发送完毕后，服务器端进入 SYN_RCVD 状态。 第三次握手(ACK=1，seq=x+1,ACKnum=y+1)客户端再次发送确认包(ACK)，SYN 标志位为0，ACK 标志位为1，并且把服务器发来 ACK 的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN的+1发送完毕后，客户端进入 ESTABLISHED 状态，当服务器端接收到这个包时，也进入 ESTABLISHED 状态，TCP 握手结束。 为什么需要三次握手，两次不行吗？如果是用两次握手，则会出现下面这种情况：如客户端发出连接请求，但因连接请求报文丢失而未收到确认，于是客户端再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接。客户端共发出了两个连接请求报文段，其中第一个丢失，第二个到达了服务端，但是第一个丢失的报文段只是在某些网络结点长时间滞留了，延误到连接释放以后的某个时间才到达服务端此时服务端误认为客户端又发出一次新的连接请求，于是就向客户端发出确认报文段，同意建立连接。因此：不采用三次握手，只要服务端发出确认，就建立新的连接了，此时客户端忽略服务端发来的确认，也不发送数据，则服务端一致等待客户端发送数据，浪费资源。 什么是半连接队列？服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。 当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。 ISN(Initial Sequence Number)是固定的吗？当一端为建立连接而发送它的SYN时，它为连接选择一个初始序号。ISN随时间而变化，因此每个连接都将具有不同的ISN。ISN可以看作是一个32比特的计数器，每4ms加1 。这样选择序号的目的在于防止在网络中被延迟的分组在以后又被传送，而导致某个连接的一方对它做错误的解释。 三次握手的其中一个重要功能是客户端和服务端交换 ISN(Initial Sequence Number)，以便让对方知道接下来接收数据的时候如何按序列号组装数据。如果 ISN 是固定的，攻击者很容易猜出后续的确认号，因此 ISN 是动态生成的。 三次握手过程中可以携带数据吗？其实第三次握手的时候，是可以携带数据的。但是，第一次、第二次握手不可以携带数据 第一次握手不可以放数据，其中一个简单的原因就是会让服务器更加容易受到攻击了。而对于第三次的话，此时客户端已经处于 ESTABLISHED 状态。对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了，所以能携带数据也没啥毛病。 SYN攻击是什么？在三次握手过程中，服务器发送 SYN-ACK 之后，收到客户端的 ACK 之前的 TCP 连接称为半连接(half-open connect)。此时服务器处于 SYN_RCVD 状态。当收到 ACK 后，服务器才能转入 ESTABLISHED 状态. SYN 攻击指的是，攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送SYN包，服务器回复确认包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，导致目标系统运行缓慢，严重者会引起网络堵塞甚至系统瘫痪。 四次挥手TCP 的连接的拆除需要发送四个包，因此称为四次挥手(Four-way handshake)，也叫做改进的三次握手。客户端或服务器均可主动发起挥手动作，在 socket 编程中，任何一方执行 close() 操作即可产生挥手操作。 第一次挥手(FIN=1，seq=x)假设客户端想要关闭连接，客户端发送一个 FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。发送完毕后，客户端进入 FIN_WAIT_1 状态。 第二次挥手(ACK=1，ACKnum=x+1)服务器端确认客户端的 FIN 包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。发送完毕后，服务器端进入 CLOSE_WAIT 状态，客户端接收到这个确认包之后，进入 FIN_WAIT_2 状态，等待服务器端关闭连接。 第三次挥手(FIN=1，seq=y)服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN 置为1。发送完毕后，服务器端进入 LAST_ACK 状态，等待来自客户端的最后一个ACK 第四次挥手(ACK=1，ACKnum=y+1)客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入 TIME_WAIT 状态，等待可能出现的要求重传的 ACK 包。服务器端接收到这个确认包之后，关闭连接，进入 CLOSED 状态。客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的 ACK ，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入 CLOSED 状态。 挥手为什么需要四次？因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉客户端，”你发的FIN报文我收到了”。只有等到我服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次挥手。 2MSL等待状态TIME_WAIT状态也成为2MSL等待状态。它是任何报文段被丢弃前在网络内的最长时间。 保证客户端发送的最后一个ACK报文段能够到达服务端。这个ACK报文段有可能丢失，使得处于LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认，服务端超时重传FIN+ACK报文段，而客户端能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重新启动2MSL计时器，最后客户端和服务端都进入到CLOSED状态。 防止“已失效的连接请求报文段”出现在本连接中。 客户端在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。 滑动窗口TCP协议里窗口机制有2种：一种是固定的窗口大小；一种是滑动的窗口。这个窗口大小就是我们一次传输几个数据。对所有数据帧按顺序赋予编号，发送方在发送过程中始终保持着一个发送窗口，只有落在发送窗口内的帧才允许被发送；同时接收方也维持着一个接收窗口，只有落在接收窗口内的帧才允许接收。这样通过调整发送方窗口和接收方窗口的大小可以实现流量控制。每个TCP/IP主机支持全双工数据传输，因此TCP有两个滑动窗口：一个用于接收数据，另一个用于发送数据。 首先是第一次发送数据这个时候的窗口大小是根据链路带宽的大小来决定的。我们假设这个时候窗口的大小是3。 这个时候接受方收到数据以后会对数据进行确认告诉发送方我下次希望手到的是数据是多少。这里我们看到接收方发送的ACK=3(这是发送方发送序列2的回答确认，下一次接收方期望接收到的是3序列信号)。 这个时候发送方收到这个数据以后就知道我第一次发送的3个数据对方只收到了2个。就知道第3个数据对方没有收到。下次在发送的时候就从第3个数据开始发。这个时候窗口大小就变成了2 。 看到接收方发送的ACK是5就表示他下一次希望收到的数据是5，发送方就知道我刚才发送的2个数据对方收了这个时候开始发送第5个数据。 只有在接收窗口向前滑动时（与此同时也发送了确认），发送窗口才有可能向前滑动。 当发送窗口和接收窗口的大小都等于1时，就是停止等待协议。 如果已经建立了连接，但是客户端突然出现故障了怎么办？TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 拥塞控制和流量控制的区别？ 拥塞控制：拥塞控制是作用于网络的，它是防止过多的数据注入到网络中，避免出现网络负载过大的情况；常用的方法就是：（ 1 ）慢开始、拥塞避免（ 2 ）快重传、快恢复。 流量控制：流量控制是作用于接收者的，它是控制发送者的发送速度从而使接收者来得及接收，防止分组丢失的。 TCP确认延迟机制？接收方在收到数据后，并不会立即回复ACK,而是延迟一定时间。一般ACK延迟发送的时间为200ms，但这个200ms并非收到数据后需要延迟的时间。系统有一个固定的定时器每隔200ms会来检查是否需要发送ACK包。 ACK是可以合并的，也就是指如果连续收到两个TCP包，并不一定需要ACK两次，只要回复最终的ACK就可以了，可以降低网络流量。 如果接收方有数据要发送，那么就会在发送数据的TCP数据包里，带上ACK信息。这样做，可以避免大量的ACK以一个单独的TCP包发送，减少了网络流量。 TCP拥塞控制机制？ 慢启动： TCP在连接过程的三次握手完成后，开始传数据，并不是一开始向网络通道中发送大量的数据包，这样很容易导致网络中路由器缓存空间耗尽，从而发生拥塞 而是根据初始的cwnd大小逐步增加发送的数据量，cwnd初始化为1个最大报文段(MSS)大小（这个值可配置不一定是1个MSS）；每当有一个报文段被确认，cwnd大小指数增长。 开始 —&gt; cwnd = 1 1个RTT(往返延迟)后 —&gt; cwnd = 21 = 2 2个RTT后 —&gt; cwnd = 22= 4 3个RTT后 —&gt; cwnd = 4*2 = 8 拥塞避免 cwnd不能一直这样无限增长下去，一定需要某个限制。TCP使用了一个叫慢启动门限(ssthresh)的变量，一旦cwnd&gt;=ssthresh（大多数TCP的实现，通常大小都是65536），慢启动过程结束，拥塞避免阶段开始； 此时窗口大小不再呈指数上升，而是以加法增加，避免增长过快导致网络拥塞。 拥塞发生：当发生丢包进行数据包重传时，表示网络已经拥塞。分两种情况进行处理： * 等到RTO（重传超时时间）超时，重传数据包 * sshthresh = cwnd /2，cwnd 重置为 1 快重传 对于接收方来说，如果接收方收到一个失序的报文段，就立即回送一个 ACK 给发送方 当发送方收到了3个重复的ACK时，则发送方快速重传丢失的包（所谓失序的报文是指，用户没有按照顺序收到TCP报文段，比如接收方收到了报文 M1, M2, M4，那么M4就称为失序报文，也就是M3被丢失），而不必等待M3的重传计时器到期 快恢复 一旦出现超时重传，或者收到第三个重复的 ack 时（快重传），TCP 会把慢启动门限 ssthresh 的值设置为 cwnd 值的一半，同时 cwnd = ssthresh ---------------- 设计模式 ---------------- 设计模式单例模式策略模式代理模式装饰器模式观察者模式享元模式原型模式装饰器模式与代理模式的区别？相同：都是增强被代理对象的功能。区别：是否进行功能增强、何时进行功能增强，这个决策权交给谁是不同的。 * 如果是别人A已经给你一个类，你需要做好增强给第三方C使用增强的类，且C不可以直接使用未增强的类对象，那么你这个开发者应该使用代理模式，把你可以获得的类对象封装到自己的代理类； * 如果你希望把何时增强功能的这个决策权交给C，即C既可以使用增强的类对象，也可以使用不增强的对象，决定权在于C，那么这个时候你应该使用装饰器模式。应用场景上： * 代理模式是为了帮助目标类增强一些自己不关心的事，比如日志代理，在目标类前后加一些日志 * 装饰模式则是用来增强自身的功能，比如Java的InputStream那些的子类装饰类，提供了一些更方便的接口给我们调用。使用方式上： * 代理模式一般在代理类中确定了要被代理的目标对象，客户端根本不知道被代理类的存在。 * 而装饰模式中被装饰者对象需要客户端创建提供，并且可以层层嵌套，层层装饰。 ---------------- MyBatis ---------------- MyBatis什么是 MyBatis?MyBatis 是一个可以自定义 SQL、存储过程和高级映射的持久层框架。 MyBatis 中 #{}和 ${}的区别是什么？#{}是预编译处理，${}是字符替换。 在使用 #{}时，MyBatis 会将 SQL 中的 #{}替换成“?”，配合 PreparedStatement 的 set 方法赋值，这样可以有效的防止 SQL 注入，保证程序的运行安全。Mybatis 在处理${}时，就是把${}替换成变量的值。 MyBatis 有几种分页方式？逻辑分页： 使用 MyBatis 自带的 RowBounds 进行分页，它是一次性查询很多数据，然后在数据中再进行检索。 物理分页： 自己手写 SQL 分页或使用分页插件 PageHelper，去数据库查询指定条数的分页数据的形式。 MyBatis 逻辑分页和物理分页的区别是什么？ 逻辑分页是一次性查询很多数据，然后再在结果中检索分页的数据。这样做弊端是需要消耗大量的内存、有内存溢出的风险、对数据库压力较大。 物理分页是从数据库查询指定条数的数据，弥补了一次性全部查出的所有数据的种种缺点，比如需要大量的内存，对数据库查询压力较大等问题。 RowBounds 是一次性查询全部结果吗？为什么？RowBounds 表面是在“所有”数据中检索数据，其实并非是一次性查询出所有数据，因为 MyBatis 是对 jdbc 的封装，在 jdbc 驱动中有一个 Fetch Size 的配置，它规定了每次最多从数据库查询多少条数据，假如你要查询更多数据，它会在你执行 next()的时候，去查询更多的数据。就好比你去自动取款机取 10000 元，但取款机每次最多能取 2500 元，所以你要取 4 次才能把钱取完。只是对于 jdbc 来说，当你调用 next()的时候会自动帮你完成查询工作。这样做的好处可以有效的防止内存溢出。 MyBatis 是否支持延迟加载？延迟加载的原理是什么？MyBatis 支持延迟加载，设置 lazyLoadingEnabled=true 即可。延迟加载的原理的是调用的时候触发加载，而不是在初始化的时候就加载信息。比如调用 a. getB(). getName()，这个时候发现 a. getB() 的值为 null，此时会单独触发事先保存好的关联 B 对象的 SQL，先查询出来 B，然后再调用 a. setB(b)，而这时候再调用 a. getB(). getName() 就有值了，这就是延迟加载的基本原理。 说一下 MyBatis 的一级缓存和二级缓存？ 一级缓存：是SqlSession级别的缓存。在操作数据库时需要构造sqlSession对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的sqlSession之间的缓存数据区域（HashMap）是互相不影响的。 二级缓存：是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的，使用二级缓存属性类需要实现 Serializable 序列化接口(可用来保存对象的状态)。。开启二级缓存数据查询流程：二级缓存 -&gt; 一级缓存 -&gt; 数据库。缓存更新机制：当某一个作用域(一级缓存 Session/二级缓存 Mapper)进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。 ---------------- Kafka ---------------- Kafkakafka 流程？ 生产者：可以声明主题Topic、分区Partition、键 Key以及值 Value，主题和值是必须要声明的，分区和键可以不用指定。拥有相同key的消息将会被写到同一分区，若没有指定key则由分区器分配。 topic：Topic中数据是顺序不可变序列，采用log追加方式写入，Topic的数据可存储在多个partition中。 partition：每个 Partition 中的消息都是有序的，生产的消息被不断追加到 Partition log 上，其中的每一个消息都被赋予了一个唯一的 offset 值。 因此数据不会因消费而丢失，所以只要consumer指定offset，一个消息可被不同的consumer多次消费。kafka中只能保证partition中记录是有序的，而不保证topic中不同partition的顺序。 Replication：同一个 partition 可能会有多个 replication，需要在这些 replication 之间选出一个 leader，producer 和 consumer 只与这个 leader 交互，其它 replication 作为 follower 从 leader 中复制数据。 消费者：订阅topic是以一个消费组来订阅的，一个消费组里面可以有多个消费者。一个partition，只能被消费组里的一个消费者消费，但是可以同时被多个消费组消费。 kafka 可以脱离 zookeeper 单独使用吗？为什么？kafka 不能脱离 zookeeper 单独使用，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器。 kafka 有几种数据保留的策略？kafka 有两种数据保存策略：按照过期时间保留和按照存储的消息大小保留。 kafka 同时设置了 7 天和 10G 清除数据，到第五天的时候消息达到了 10G，这个时候 kafka 将如何处理？这个时候 kafka 会执行数据清除工作，时间和大小不论那个满足条件，都会清空数据。 什么情况会导致 kafka 运行变慢？ cpu 性能瓶颈 磁盘读写瓶颈 网络瓶颈 消息重复解决方案 针对消息重复：将消息的唯一标识保存到外部介质中，每次消费时判断是否处理过即可。比如redis中消息可以使用唯一id标识 生产者（ack=all 代表至少成功发送一次) 消费者 （offset手动提交，业务逻辑成功处理后，提交offset） 落表（主键或者唯一索引的方式，避免重复数据） 业务逻辑处理（选择唯一主键存储到Redis或者mongdb中，先查询是否存在，若存在则不处理；若不存在，先插入Redis或Mongdb,再进行业务逻辑处理） 怎么解决 Kafka 数据丢失的问题 消费端弄丢了数据 唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了offset，让kafka以为你已经消费好了这个消息，其实你刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是一样么，大家都知道kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。但是此时确实还是会重复消费，比如你刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 kafka弄丢了数据 kafka某个broker宕机，然后重新选举partiton的leader时。大家想想，要是此时其他的follower刚好还有些数据没有同步，结果此时leader挂了，然后选举某个follower成leader之后，他不就少了一些数据？这就丢了一些数据啊。 所以此时一般是要求起码设置如下4个参数： 给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本 在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧 在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了 在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了 生产者会不会弄丢数据 如果按照上述的思路设置了ack=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 ---------------- MySQL ---------------- MySQL基础数据库的三范式是什么？ 第一范式：强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项。 第二范式：要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性。 第三范式：任何非主属性不依赖于其它非主属性。https://zhuanlan.zhihu.com/p/92991575 char 和 varchar 的区别是什么？ char(n) ：固定长度类型，比如订阅 char(10)，当你输入”abc”三个字符的时候，它们占的空间还是 10 个字节，其他 7 个是空字节。 varchar(n) ：可变长度，存储的值是每个值占用的字节再加上一个用来记录其长度的字节的长度。chat 优点：效率高；缺点：占用空间；适用场景：存储密码的 md5 值，固定长度的，使用 char 非常合适。所以，从空间上考虑 varcahr 比较合适；从效率上考虑 char 比较合适，二者使用需要权衡。 float 和 double 的区别是什么？ float 最多可以存储 8 位的十进制数，并在内存中占 4 字节。 double 最可可以存储 16 位的十进制数，并在内存中占 8 字节。 MySQL 常用的引擎？ InnoDB 引擎：mysql 5.1 后默认的数据库引擎，提供了对数据库 acid 事务的支持，并且还提供了行级锁和外键的约束，它的设计的目标就是处理大数据容量的数据库系统。MySQL 运行的时候，InnoDB 会在内存中建立缓冲池，用于缓冲数据和索引。 但是该引擎是不支持全文搜索，同时启动也比较的慢，它是不会保存表的行数的，所以当进行 select count(*) from table 指令的时候，需要进行扫描全表。 由于锁的粒度小，写操作是不会锁定全表的,所以在并发度较高的场景下使用会提升效率的。 MyIASM 引擎：不提供事务的支持，也不支持行级锁和外键。。因此当执行插入和更新语句时，即执行写操作的时候需要锁定这个表，所以会导致效率会降低。MyIASM 引擎是保存了表的行数，于是当进行 select count(*) from table 语句时，可以直接的读取已经保存的值而不需要进行扫描全表。 所以，如果表的读操作远远多于写操作时，并且不需要事务的支持的，可以将 MyIASM 作为数据库引擎的首选。 如何做 MySQL 的性能优化？ 为搜索字段创建索引。 避免使用 select *，列出需要查询的字段。 垂直分割分表。 选择正确的存储引擎。 在mysql服务器运行缓慢的情况下输入什么命令能缓解服务器压力? 第一步 检查系统的状态 通过操作系统的一些工具检查系统的状态，比如CPU、内存、交换、磁盘的利用率，根据经验或与系统正常时的状态相比对，有时系统表面上看起来看空闲，这也可能不是一个正常的状态，因为cpu可能正等待IO的完成。除此之外，还应观注那些占用系统资源(cpu、内存)的进程。 1.1 使用sar来检查操作系统是否存在IO问题 1.2 使用vmstat监控内存 cpu资源 1.3 磁盘IO问题，处理方式：做raid10提高性能 1.4 网络问题，telnet一下MySQL对外开放的端口，如果不通的话，看看防火墙是否正确设置了。另外，看看MySQL是不是开启了skip-networking的选项，如果开启请关闭。 第二步 检查mysql参数 2.1 max_connect_errors：如果MySQL服务器连续接收到了来自于同一个主机的请求，而且这些连续的请求全部都没有成功的建立连接就被中断了，当这些连续的请求的累计值大于max_connect_errors的设定值时，MySQL服务器就会阻止这台主机后续的所有请求。 2.2 connect_timeout：获取MySQL连接是多次握手的结果，除了用户名和密码的匹配校验外，还有IP-&gt;HOST-&gt;DNS-&gt;IP验证，任何一步都可能因为网络问题导致线程阻塞。为了防止线程浪费在不必要的校验等待上，超过connect_timeout的连接请求将会被拒绝。 2.3 skip-name-resolve 2.4 slave-net-timeout=seconds 2.5 master-connect-retry 第三步 检查mysql 相关状态值3.1 关注连接数 3.2 关注下系统锁情况 3.3 关注慢查询（slow query）日志 MySQL 数据库 CPU 飙升到 500% 的话，怎么处理？当 CPU 飙升到 500% 时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。如果是 mysqld 造成的，使用 show processlist 命令，看看里面跑的 Session 情况，是不是有消耗资源的 SQL 在运行。找出消耗高的 SQL ，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。一般来说，肯定要 kill 掉这些线程(同时观察 CPU 使用率是否下降)，等进行相应的调整(比如说加索引、改 SQL 、改内存参数)之后，再重新跑这些 SQL。也可以查看 MySQL 慢查询日志，看是否有慢 SQL 。也有可能是每个 SQL 消耗资源并不多，但是突然之间，有大量的 Session 连进来导致 CPU 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等。 MySQL binlog 的几种日志录入格式以及区别？ Statement：每一条会修改数据的 SQL 都会记录在 binlog 中。 优点：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO，提高性能。(相比 row 能节约多少性能与日志量，这个取决于应用的 SQL 情况，正常同一条记录修改或者插入 row 格式所产生的日志量还小于 Statement 产生的日志量，但是考虑到如果带条件的 update 操作，以及整表删除，alter 表等操作，ROW 格式会产生大量日志，因此在考虑是否使用 ROW 格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的 IO 性能问题。) 缺点：由于记录的只是执行语句，为了这些语句能在 slave 上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在 slave 得到和在 master 端执行时候相同 的结果。另外 MySQL 的复制，像一些特定函数功能，slave 可与 master 上要保持一致会有很多相关问题(如 sleep() 函数，last_insert_id()，以及 user-defined functions(udf) 会出现问题)。 Row：不记录 SQL 语句上下文相关信息，仅保存哪条记录被修改。 优点：binlog 中可以不记录执行的 SQL 语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以 rowlevel 的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或 function ，以及 trigger 的调用和触发无法被正确复制的问题。 缺点：所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比如一条 Update 语句，修改多条记录，则 binlog 中每一条修改都会有记录，这样造成 binlog 日志量会很大，特别是当执行 alter table 之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。 Mixedlevel：是以上两种 level 的混合使用。一般的语句修改使用 Statement 格式保存 binlog 。如一些函数，statement 无法完成主从复制的操作，则采用 Row 格式保存 binlog 。 每一种日志格式在复制中的优劣？ Statement 可能占用空间会相对小一些，传送到 slave 的时间可能也短，但是没有 Row 模式的可靠。 Row 模式在操作多行数据时更占用空间，但是可靠。 MySQL 主从复制的流程是怎么样的？1、Master 上面的 binlog dump 线程，该线程负责将 master 的 binlog event 传到 slave 。2、Slave 上面的 IO 线程，该线程负责接收 Master 传过来的 binlog，并写入 relay log 。3、Slave 上面的 SQL 线程，该线程负责读取 relay log 并执行。4、如果是多线程复制，无论是 5.6 库级别的假多线程还是 MariaDB 或者 5.7 的真正的多线程复制， SQL 线程只做 coordinator ，只负责把 relay log 中的 binlog 读出来然后交给 worker 线程， woker 线程负责具体 binlog event 的执行。 MySQL 如何保证复制过程中数据一致性？left join、right join、inner join的区别 left join(左联接) 返回包括左表中的所有记录和右表中联结字段相等的记录 right join(右联接) 返回包括右表中的所有记录和左表中联结字段相等的记录 inner join(等值连接) 只返回两个表中联结字段相等的行 什么是MVCC？多版本并发控制InnoDB中，在每一行数据中额外保存两个隐藏的列：当前行创建时的版本号和删除时的版本号（可能为空，其实还有一列称为回滚指针，用于事务回滚，不在本文范畴）。这里的版本号并不是实际的时间值，而是系统版本号。每开始新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询每行记录的版本号进行比较。每个事务又有自己的版本号，这样事务内执行CRUD操作时，就通过版本号的比较来达到数据版本控制的目的。 数据库瓶颈？ IO瓶颈 第一种：磁盘读IO瓶颈，热点数据太多，数据库缓存放不下，每次查询时会产生大量的IO，降低查询速度 -&gt;分库和垂直分表。 第二种：网络IO瓶颈，请求的数据太多，网络带宽不够 -&gt; 分库。2、CPU瓶颈 第一种：SQL问题，如SQL中包含join，group by，order by，非索引字段条件查询等，增加CPU运算的操作 -&gt; SQL优化，建立合适的索引，在业务Service层进行业务计算。 第二种：单表数据量太大，查询时扫描的行太多，SQL效率低，CPU率先出现瓶颈 -&gt; 水平分表 数据库分库分表有哪些?https://zhuanlan.zhihu.com/p/83674503 水平分库 概念：以字段为依据，按照一定策略（hash、range等），将一个库中的数据拆分到多个库中。 结果： 每个库的结构都一样； 每个库的数据都不一样，没有交集； 所有库的并集是全量数据； 场景：系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。 分析：库多了，io和cpu的压力自然可以成倍缓解。 水平分表 概念：以字段为依据，按照一定策略（hash、range等），将一个表中的数据拆分到多个表中。 结果： 每个表的结构都一样； 每个表的数据都不一样，没有交集； 所有表的并集是全量数据； 场景：系统绝对并发量并没有上来，只是单表的数据量太多，影响了SQL效率，加重了CPU负担，以至于成为瓶颈。 分析：表的数据量少了，单次SQL执行效率高，自然减轻了CPU的负担。 垂直分库 概念：以表为依据，按照业务归属不同，将不同的表拆分到不同的库中。 结果： 每个库的结构都不一样； 每个库的数据也不一样，没有交集； 所有库的并集是全量数据； 场景：系统绝对并发量上来了，并且可以抽象出单独的业务模块。 分析：到这一步，基本上就可以服务化了。例如，随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。 垂直分表 概念：以字段为依据，按照字段的活跃性，将表中字段拆到不同的表（主表和扩展表）中。 结果： 每个表的结构都不一样； 每个表的数据也不一样，一般来说，每个表的字段至少有一列交集，一般是主键，用于关联数据； 所有表的并集是全量数据； 场景：系统绝对并发量并没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。 分析：可以用列表页和详情页来帮助理解。垂直分表的拆分原则是将热点数据（可能会冗余经常一起查询的数据）放在一起作为主表，非热点数据放在一起作为扩展表。这样更多的热点数据就能被缓存下来，进而减少了随机读IO。拆了之后，要想获得全部数据就需要关联两个表来取数据。但记住，千万别用join，因为join不仅会增加CPU负担并且会讲两个表耦合在一起（必须在一个数据库实例上）。关联数据，应该在业务Service层做文章，分别获取主表和扩展表数据然后用关联字段关联得到全部数据。 分库分表步骤根据容量（当前容量和增长量）评估分库或分表个数 -&gt; 选key（均匀）-&gt; 分表规则（hash或range等）-&gt; 执行（一般双写）-&gt; 扩容问题（尽量减少数据的移动）。 innodbinnodb 体系结构？ 什么是redo log？当数据库对数据做修改的时候，需要把数据页从磁盘读到buffer pool中，然后在buffer pool中进行修改，那么这个时候buffer pool中的数据页就与磁盘上的数据页内容不一致，称buffer pool的数据页为dirty page 脏数据。如果发生非正常的DB服务重启，那么这些数据并没有同步到磁盘文件中（注意，同步到磁盘文件是个随机IO），会发生数据丢失。如果这个时候，能够有一个文件，当缓冲池中的data page变更结束后，把相应修改记录记录到这个文件（注意，记录日志是顺序IO），那么当DB服务发生crash的情况，恢复DB的时候，也可以根据这个文件的记录内容，重新应用到磁盘文件，数据保持一致。这个文件就是redo log ，用于记录 数据修改后的记录，顺序记录。重做日志支持以下三种情况触发刷新： * Master Thread每一秒将重做日志缓冲刷新到重做日志文件 * 每次事务提交时将重做日志缓冲刷新到重做日志文件 * 当重做日志缓冲池剩余空间小于1/2时，重做日志缓冲刷新到重做日志文件 什么是undo log？undo日志用于存放数据修改被修改前的值。假设修改表中 id=1 的行数据，把Name=’B’ 修改为Name = ‘B2’ ，那么undo日志就会用来存放Name=’B’的记录，如果这个修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。 undo 和 redo 事物实现过程？事务B要将字段A的值由原来的1修改为3，要将B的值由原来的2修改为4，redo日志记录的是： 1234567891011假设有A、B两个数据，值分别为1,2.1. 事务B开始2. 记录A=1到undo log3. 修改A=34. 记录A=3到 redo log5. 记录B=2到 undo log6. 修改B=47. 记录B=4到redo log8. 将redo log写入磁盘9. 事务提交，将数据写入磁盘10.事物B结束 如果上面事务B回滚（当做新的事务C），则redo记录的是： 1234567891011121314151. 事务C开始2. 记录A=1到undo log3. 修改A=34. 记录A=3到 redo log5. 记录B=2到 undo log6. 修改B=47. 记录B=4到redo log &lt;!--回滚--&gt;8. 修改B=29. 记录B=2到redo log10.修改A=111.记录A=1到redo log12.将redo log写入磁盘13.事务提交，将数据写入磁盘14.事物C结束 Double write解决了什么问题? 数据库IO的最小单位是16K（MySQL默认，oracle是8K） 文件系统IO的最小单位是4K（也有1K的） 磁盘IO的最小单位是512字节 一个数据页的大小是16K，假设在把内存中的脏页写到数据库的时候，写了8K突然宕机了，也就是说前8K数据是新的，后8K是旧的，那么磁盘数据库这个数据页就是不完整的，是一个坏掉的数据页，这种情况被称为部分写失效 为什么 redo log 不需要 doublewrite 的支持？因为 redo log 写入的单位就是 512 字节，也就是磁盘 IO 的最小单位，所以无所谓数据损坏。 页断裂可不可以通过 redo log 来进行恢复呢？redo记录的是对页的修改，只能恢复校验完整（还没写）的页，不能修复坏掉的数据页，所以这个数据就丢失了，可能会造成数据不一致，所以需要double write。 两次写工作流程？doublewrite由两部分组成，一部分为内存中的doublewrite buffer，其大小为2MB，另一部分是磁盘上共享表空间(ibdata x)中连续的128个页，即2个区(extent)，大小也是2M。 当一系列机制触发数据缓冲池中的脏页刷新时，并不直接写入磁盘数据文件中，而是先拷贝至内存中的doublewrite buffer中； 接着从两次写缓冲区分两次写入磁盘共享表空间中(连续存储，顺序写，性能很高)，每次写1MB； 待第二步完成后，再将doublewrite buffer中的脏页数据写入实际的各个表空间文件(离散写)；(脏页数据固化后，即进行标记对应doublewrite数据可覆盖) 为什么 double write 可以解决页断裂？ 磁盘还未写，此时可以通过 redo log 恢复； 磁盘正在进行从内存到共享表空间的写，此时数据文件中的页还没开始被写入，因此也同样可以通过 redo log 恢复； 磁盘正在写数据文件，此时共享表空间已经写完，可以从共享表空间拷贝页的副本到数据文件实现恢复。 事物什么是事务？一个事务是可以被看作一个单元的一系列SQL语句的集合。它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。 事务的特性（ACID） 是什么？ Atomicity（原子性）：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 数据库的事务隔离级别？ READ-UNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读、脏读、不可重复读）。 READ-COMMITTED：提交读，事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。（会造成幻读、不可重复读）。 REPEATABLE-READ：可重复读，默认级别。读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。（会造成幻读）。 SERIALIZABLE：序列化，代价最高最可靠的隔离级别，这个事务执行的时候不允许别的事务并发执行。事务只能一个接着一个地执行。该隔离级别能防止脏读、不可重复读、幻读。 脏读 ：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A。不可重复读 ：事务A读取数据后，事务B执行更新操作，使A无法再次读取结果。幻读：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。丢失修改：两个事务A，B读入同一数据并修改，B提交的结果被A破坏了，导致B的修改丢失。 锁从数据库系统的角度来看分为几种？ 共享锁（S） 共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。 更新锁（U） 更新锁可以防止通常形式的死锁。一般更新模式由一个事务组成，此事务读取记录，获取资源（页或行）的共享锁，然后修改行，此操作要求锁转换为排它锁。 排他锁（X） 排它锁可以防止并发事务对资源进行访问。其它事务不能读取或修改排它锁锁定的数据。 意向锁 意向锁就是说在屋（比如代表一个表）门口设置一个标识，说明屋子里有人（比如代表某些记录）被锁住了。另一个人想知道屋子里是否有人被锁，不用进屋子里一个一个的去查，直接看门口标识就行了。 意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁 意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁。 意向共享排他锁（SIX）：对一个数据对象加 SIX锁，表示对它加 S锁，再加IX锁，即 SIX=S+IX。例如对某个表加 SIX锁，则表示该事务要读整个表（所以要对该表加 S锁），同时会更新个别元组（所以要对该表加 IX锁）。 当一个表中的某一行被加上排他锁后，该表就不能再被加表锁。数据库程序如何知道该表不能被加表锁？一种方式是逐条的判断该表的每一条记录是否已经有排他锁，另一种方式是直接在表这一层级检查表本身是否有意向锁，不需要逐条判断。显然后者效率高。 什么是悲观锁？悲观锁，正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度(悲观)，因此，在整个数据处理过程中，将数据处于锁定状态。 悲观锁的流程 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。具体响应方式由开发者根据实际需要决定。 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 优点与不足 悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。 但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会； 另外，在只读型事务处理中由于不会产生冲突，也没必要使用锁，这样做只能增加系统负载,还有会降低了并行性. 一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数 什么是乐观锁？乐观锁 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。 实现方式 对记录加版本号：在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行+1操作。并判断当前版本号是不是该数据的最新的版本号。 对记录加时间戳：在数据初始化时使用时间戳（timestamp），在更新提交的时候检查当前数据库中数据的时间戳和自己更新前取到的时间戳进行对比，如果一致则OK。 对将要更新的数据进行提前读取、事后对比。 什么是锁的粒度？锁的粒度就是指锁的生效范围，就是说是行锁，还是页锁，还是整表锁. 锁的粒度同样既可以由数据库自动管理，也可以通过手工指定hint来管理。 封锁协议？ 一级封锁协议：对应READ-UNCOMMITTED 隔离级别，本质是在事务A中修改完数据M后，立刻对这个数据M加上共享锁(S锁)（当事务A继续修改数据M的时候，先释放掉S锁，再修改数据，再加上S锁），根据S锁的特性，事务B可以读到事务A修改后的数据(无论事务A是否提交，因为是共享锁，随时随地都能查到数据A修改后的结果)，事务B不能去修改数据M，直到事务A提交，释放掉S锁。 缺点：丢失更新。脏读。不可重复读。幻读。 二级封锁协议：对应READ-COMMITTED隔离级别，本质是事务A在修改数据M后立刻加X锁，事务B不能修改数据M，同时不能查询到最新的数据M(避免脏读)，查询到的数据M是上一个版本(Innodb MVCC快照)的。 缺点：丢失更新。不可重复读。幻读。 三级封锁协议：对应REPEATABLE-READ隔离级别,本质是二级封锁协议基础上，对读到的数据M瞬间加上共享锁M，直到事务结束才释放（保证了其他事务没办法修改该数据），这个级别是MySql 5.5 默认的隔离级别。 缺点：丢失更新。幻读。 最强封锁协议：对应Serialization隔离级别，本质是从MVCC并发控制退化到基于锁的并发控制，对事务中所有读取操作加S锁，写操作加X锁，这样可以避免脏读，不可重复读，幻读，更新丢失，开销也最大，会造成读写冲突，并发程度也最低。 MySQL 的行锁和表锁？MyISAM 只支持表锁，InnoDB 支持表锁和行锁，默认为行锁。 表级锁：开销小，加锁快，不会出现死锁。锁定粒度大，发生锁冲突的概率最高，并发量最低。 行级锁：开销大，加锁慢，会出现死锁。锁力度小，发生锁冲突的概率小，并发度最高。 说一下乐观锁和悲观锁？ 乐观锁：每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据。 悲观锁：每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻止，直到这个锁被释放。 数据库的乐观锁需要自己实现，在表里面添加一个 version 字段，每次修改成功值加 1，这样每次修改的时候先对比一下，自己拥有的 version 和数据库现在的 version 是否一致，如果不一致就不修改，这样就实现了乐观锁。 排他锁select … for update？在进行事务操作时，通过“for update”语句，MySQL会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞。排他锁包含行锁、表锁。InnoDB 是基于索引来完成行锁。例如：SELECT * FROM tab_with_index WHERE id = 1 FOR UPDATE 。FOR UPDATE 可以根据条件来完成行锁锁定，并且 id 是有索引键的列,如果 id 不是索引键那么 InnoDB 将完成表锁，并发将无从谈起。 MySQL 查询执行顺序？12345678910(1) SELECT(2) DISTINCT &lt;select_list&gt;(3) FROM &lt;left_table&gt;(4) &lt;join_type&gt; JOIN &lt;right_table&gt;(5) ON &lt;join_condition&gt;(6) WHERE &lt;where_condition&gt;(7) GROUP BY &lt;group_by_list&gt;(8) HAVING &lt;having_condition&gt;(9) ORDER BY &lt;order_by_condition&gt;(10) LIMIT &lt;limit_number&gt; 索引索引有什么好处？ 提高数据的检索速度，降低数据库IO成本：使用索引的意义就是通过缩小表中需要查询的记录的数目从而加快搜索的速度。 降低数据排序的成本，降低CPU消耗：索引之所以查的快，是因为先将数据排好序，若该字段正好需要排序，则正好降低了排序的成本。 索引有什么坏处？ 占用存储空间：索引实际上也是一张表，记录了主键与索引字段，一般以索引文件的形式存储在磁盘上。 降低更新表的速度：表的数据发生了变化，对应的索引也需要一起变更，从而减低的更新速度。否则索引指向的物理数据可能不对，这也是索引失效的原因之一。 索引的类型？1、普通索引：最基本的索引，没有任何约束。2、唯一索引：与普通索引类似，但具有唯一性约束。3、主键索引：特殊的唯一索引，不允许有空值。4、复合索引：将多个列组合在一起创建索引，可以覆盖多个列。5、外键索引：只有InnoDB类型的表才可以使用外键索引，保证数据的一致性、完整性和实现级联操作。6、全文索引：MySQL 自带的全文索引只能用于 InnoDB、MyISAM ，并且只能对英文进行全文检索，一般使用全文索引引擎。 B+tree和Btree的区别？btree是每个节点都保存数据，而b+tree只有叶子节点保存数据，并且每个叶子节点都有指向前后叶子节点的指针。 B+树能存多少数据？聚集索引：叶子节点保存主键所在的数据页，非叶子节点保存主键+指针非叶子节点一个数据页能存储的主键个数：一个数据页大小16K/(主键大小8bit+指针大小6bit) = 1170假设1行数据1kb，那么一个叶子结点数据页可以存16行数据，那么高度为2的b+树能存117016=18720行数据高度为3的b+树能存11701170*16 = 2190 2400行数据。 自适应哈希索引？哈希：一次就可以定位数据自适应哈希索引 AHI（adaptive hash index）建立条件：观察到一个访问模式访问频繁，就会建立哈希索引 通过该模式访问了 100 次（模式：where x = ?） 页通过该模式访问了 N 次，其中 N = 页的记录总数⁄16 InnoDB索引原理？数据库中的B+树索引可以分为聚集索引和辅助索引。聚集索引(主键索引)：按照每张表的主键构造一颗B+树，同时叶子结点存放的即为整张表的行纪录数据也称为数据页。辅助索引(非聚集索引)：按照每张表的索引构造一颗B+树，叶子节点存放该行数据的主键,非叶子节点保存（索引值+主键）(辅助索引有重复，用来保证唯一性)+指针(指向数据页)。 当通过索引数据来查找数据的时候，存储引擎会遍历辅助索引并且通过叶级别的指针获取到指向主键索引的的主键，然后再通过主键索引来找到一个完整的记录；innodb索引在创建的时候，默认主键索引会按照主键顺序排序，非聚集索引会按照索引字段组成的字符串字典序进行排序。 MyISAM 索引实现？MyISAM 索引的实现，和 InnoDB 索引的实现是一样使用 B+Tree ，差别在于 MyISAM 索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。 性别为什么不能作为数据库索引?对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。在使用普通索引查询时，会先加载普通索引，通过普通索引查询到实际行的主键。再使用主键通过聚集索引查询相应的行。以此循环查询所有的行。若直接全量搜索聚集索引，则不需要在普通索引和聚集索引中来回切换。相比两种操作的总开销可能扫描全表效率更高。 索引优化？ 尽量避免Like的参数以通配符开头，否则数据库引擎会放弃使用索引而进行全表扫描 select * from t_credit_detail where Flistid like ‘%0’\G 全表扫描，没有使用到索引，不建议使用 select * from t_credit_detail where Flistid like ‘2%’\G 用到了索引，是有范围的查找了，比以通配符开头的sql语句效率提高不少 总结：用到like 尽快在后面添加通配符 where条件不符合最左前缀原则时 最左前缀原则主要使用在联合索引中 abc是联合索引时，只有当a或者ab或者abc为条件时才能触发索引 如果出现ac 是因为使用了a字段 导致触发索引 如果是abc 都有字段的时候 顺序无所谓，可以触发索引总结：用到联合索引的时候，尽量和联合顺序一致 使用！= 或 &lt;&gt; 操作符时:尽量避免使用！= 或 &lt;&gt;操作符,使用&gt;或&lt;会比较高效，否则数据库引擎会放弃使用索引而进行全表扫描。 索引列参与计算:应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。 对字段进行null值判断 应尽量避免在where子句中对字段进行null值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： 低效：select * from t_credit_detail where Flistid is null ; 可以在Flistid上设置默认值0，确保表中Flistid列没有null值，然后这样查询： 高效：select * from t_credit_detail where Flistid =0; 使用or来连接条件 应尽量避免在where子句中使用or来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： 低效：select * from t_credit_detail where Flistid = ‘2000000608201108010831508721’ or Flistid = ‘10000200001’; 可以用下面这样的查询代替上面的 or 查询： 高效：select from t_credit_detail where Flistid = ‘2000000608201108010831508721’ union all select from t_credit_detail where Flistid = ‘10000200001’; 避免select * ,在解析的过程中，会将’*’ 依次转换成所有的列名，这个工作是通过查询数据字典完成的，这意味着将耗费更多的时间。 order by 语句优化 任何在Order by语句的非索引项或者有计算表达式都将降低查询速度。 方法： 1.重写order by语句以使用索引；2.为所使用的列建立另外一个索引3.绝对避免在order by子句中使用表达式。 用 exists 代替 in 使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些 判断sql语句是否使用索引查询?在查询语句前面加上explain 数据库建立索引原则?表的主键、外键必须有索引；先存数据，再建立索引（在有索引的前提下进行插入、更新操作会变慢）；不要对规模小的数据表建立索引，数据量超过300的表应该有索引（在表上建立的每一个索引都会增加存储开销，所以索引不是越多越好）；索引应该建在小字段上，对于大的文本字段甚至超长字段，不要建索引；索引应该建在选择性高的字段上（增加查询速度）；经常与其他表进行链接的表，在链接字段上应该建立索引。在SQL语句中经常进行GROUP BY、ORDER BY的字段上建立索引（避免全表扫描，特别是大表）；对于查询操作中频繁使用的列建立索引，对增删操作频繁的列应尽量避免建立索引。在建立复合索引时，应仔细考虑是否可以用单字段索引替代。 为什么 SELECT COUNT(*) FROM table 在 InnoDB 比 MyISAM 慢？对于 SELECT COUNT(*) FROM table 语句，在没有 WHERE 条件的情况下，InnoDB 比 MyISAM 可能会慢很多，尤其在大表的情况下。因为，InnoDB 是去实时统计结果，会全表扫描；而 MyISAM 内部维持了一个计数器，预存了结果，所以直接返回即可。 以下三条 SQL 如何建索引，只建一条怎么建？123WHERE a = 1 AND b = 1WHERE b = 1WHERE b = 1 ORDER BY time DESC 以顺序 b , a, time 建立复合索引，CREATE INDEX table1_b_a_time ON index_test01(b, a, time)。对于第一条 SQL ，因为最新 MySQL 版本会优化 WHERE 子句后面的列顺序，以匹配复合索引顺序。 ------------------------ Redis ------------------------ RedisRedis 是什么？都有哪些使用场景？Redis 是一个使用 C 语言开发的高速缓存数据库。Redis 使用场景： 记录帖子点赞数、点击数、评论数； 缓存近期热帖； 缓存文章详情信息； 记录用户会话信息。 Redis 有哪些功能？ 数据缓存功能 分布式锁的功能 支持数据持久化 支持事务 支持消息队列 Redis 支持的数据类型有哪些？Redis 支持的数据类型：string（字符串）、list（列表）、hash（字典）、set（集合）、zset（有序集合）。 Redis 为什么是单线程的？因为 cpu 不是 Redis 的瓶颈，Redis 的瓶颈最有可能是机器内存或者网络带宽。既然单线程容易实现，而且 cpu 又不会成为瓶颈，那就顺理成章地采用单线程的方案了。关于 Redis 的性能，官方网站也有，普通笔记本轻松处理每秒几十万的请求。 Redis 单机数据库的好处？ C 语言实现。 纯内存 单线程，避免频繁上下文切换 采用非阻塞IO多路复用？ Redis 持久化有几种方式？ RDB（Redis Database）：默认持久化方式，按一定的时间周期把内存的数据以快照的形式保存到硬盘的二进制文件 优点： 只有一个dump.rdb 方便持久化 容灾性好，一个文件可以保存到安全的磁盘 性能最大化，fork子进程来完成写操作，让主进程继续处理命令，IO最大化 数据集大时，比AOF的效率高 缺点： 数据安全性低，RDB隔一段时间就会进行持久化，如果持久化之间发生故障，会发生数据丢失 AOF（Append Only File）：每一个收到的写命令都通过write函数追加到文件中，当redis重启会重新执行文件保存的写命令来重建数据库 优点： 数据安全，aof可以配置为每进行一次命令就记录到AOF文件中 通过append模式写文件，即使中途服务器宕机，可以通过redis-check-aof工具进行恢复 rewrite模式：fork一个子进程进行AOF重写 缺点：AOF文件比RDB文件大，且恢复速度慢，数据集大时比RDB效率低。 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 Redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。 怎么保证缓存和数据库数据的一致性？ 合理设置缓存的过期时间。 新增、更改、删除数据库操作时同步更新 Redis，可以使用事物机制来保证数据的一致性。 Redis 淘汰策略有哪些？ volatile-lru：从已设置过期时间的数据集（server. db[i]. expires）中挑选最近最少使用的数据淘汰。 volatile-ttl：从已设置过期时间的数据集（server. db[i]. expires）中挑选将要过期的数据淘汰。 volatile-random：从已设置过期时间的数据集（server. db[i]. expires）中任意选择数据淘汰。 allkeys-lru：从数据集（server. db[i]. dict）中挑选最近最少使用的数据淘汰。 allkeys-random：从数据集（server. db[i]. dict）中任意选择数据淘汰。 no-enviction（驱逐）：禁止驱逐数据。 Redis 过期策略？ 在获取某个key时会检查，这个key如果设置了过期时间那么是否过期了，如果过期则删除 redis默认每100ms随机抽取进行检查是否有过期的key 当前已用内存超过 maxmemory 限定时，触发主动清理策略。 MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是热点数据？我们已经看到，“Redis 内存数据集大小上升到一定 maxmemory 的时候，就会进行数据淘汰策略。” 。那么，如果我们此时要保证热点数据不被淘汰，那么需要选择 volatile-lru 或 allkeys-lru 这两个基于 LRU 算法的淘汰策略。 什么是缓存雪崩？由于原有缓存失效，新缓存未到期间，原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成压力，严重会导致数据库宕机，整个系统崩溃解决方法： 1. 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。 2. 考虑用加锁或队列的方式来保证不会有大量线程对数据库一次性进行读写,但是假设在高并发下，缓存重建期间key是锁着的，这是过来1000个请求999个都在阻塞的。同样会导致用户等待超时，这是个治标不治本的方法！ 3. 缓存标记法：设置一个缓存标记，例如：缓存标记时间30分钟，数据缓存设置为60分钟。这样，当缓存标记key过期后，实际缓存还能把旧数据返回给调用端，直到另外的线程在后台更新完成后，才会返回新缓存。 什么是缓存穿透？数据在数据库中没有，自然在缓存中也不会有，这就导致用户查询时缓存中找不到，都要去数据库再查一遍解决方法： 1. 布隆过滤器，将所有可能存在的数据哈希到足够大的bitmap中，一个一定不存在的数据会被bitmap拦截 2. 如果查询数据为空（不管数据不存在还是系统故障）仍把这个空结果进行缓存 什么是缓存击穿？在缓存中没有，在数据库中有（一般是缓存到期了），这时由于并发量特别多，同时读缓存没有读到数据，又同时去数据库中取数据，引起数据库压力增大，造成过大压力。与缓存雪崩的区别：缓存击穿针对某一key缓存，缓存雪崩则是很多key。解决方法：（实际上数据量小的应用无需解决，不会对db造成太大压力） 1. 使用互斥锁 2. 设置热点数据永远不过期。 什么是缓存预热？系统上线后将相关的缓存数据直接加载到缓存系统解决方法： * 数据量不大，项目启动时自动进行加载 * 定时刷新缓存 Redis 常见的性能问题有哪些？该如何解决？ 主服务器写内存快照，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以主服务器最好不要写内存快照。 Redis 主从复制的性能问题，为了主从复制的速度和连接的稳定性，主从库最好在同一个局域网内。 Redis 主从复制？用户可执行slaveof命令或设置slaveof选项让一个服务器去复制另一个服务器 旧版复制 同步sync：用于将从服务器的数据库状态更新至主服务器当前所处的数据库状态 从服务器向主服务器发送sync命令 收到sync命令的主服务器执行bgsave在后台生成RDB文件，并用一个缓冲区记录从现在开始执行的所有写命令 当主服务器bgsave执行完毕时，主服务器会将RDB文件发送给从服务器，从服务器接收并载入rdb，将自己的数据库状态更新至主服务器执行bgsave时的数据状态 主服务器将缓冲区中写命令发送给从服务器，从服务器执行写命令，更新状态 命令传播：由于主服务器的数据库状态被修改，导致主从状态不一致时让主从服务器的数据库重新回到一致 主服务器会将自己执行的写命令发送给从服务器执行，从而使主从状态一致 缺陷：在断线后重复制的效率非常低，需要执行sync命令全量生成并加载rdb文件 新版复制：使用psync代替sync，可以部分同步和全部同步 全部同步：sync 部分同步：psync 主服务器的复制偏移量和从服务器的复制偏移量 主服务器每次向从服务器传播N个字节的数据时，就将自己的复制偏移量的值加上N 从服务器收到N子节数据时就将自己的复制偏移量的值加N 主服务器的复制积压缓冲区 主服务器维护的一个固定长度先进先出队列，大小1M 当主服务器命令传播时，还会将写命令入队到复制缓冲里，并且复制缓冲会为队列中每字节记录复制偏移量 当从服务器重新连上主服务器，从会通过psync将自己的offset发给主服务器 如果offset之后的数据仍在复制积压缓冲里，执行部分重同步 否则执行完整重同步 服务器的运行ID 当从服务器对主服务器进行初次复制时，主会将自己的运行ID传给从服务器，从服务器保存起来 当从服务器断线重连，从服务器向当前连接的主发送之前保存的运行ID 如果和当前主服务器的ID相同，则可以尝试执行部分重同步 否则执行完整重同步哨兵模式sentinel？sentinel是高可用的解决方案，由一个或多个sentinel实例组成的sentinel系统，可以监视任意多个主从服务器，并在被监视的主服务器进入下线时，将从升为主服务器 启动哨兵 初始化服务器：sentinel并不使用数据库，初始化不会加载RDB或AOF 使用sentinel专用代码、初始化sentinel状态、初始化sentinel状态的masters属性 创建连向主服务器的异步网络连接 命令连接，向主服务器发送命令并接收命令回复 订阅连接，专门用于订阅主服务器的sentinel:hello 频道 获取主服务器信息：默认每十秒一次，通过命令连接向被监视的主服务器发送INFO命令，并通过分析回复来获取状态，可以获取到主服务器信息，以及主服务器下所有从服务器信息 获取从服务器信息：根据INFO命令回复 向主从服务器发送信息：每两秒一次向服务器sentinel:hello频道发送一条信息 接收主从服务器的频道信息 更新sentinels字典 创建连向其他sentinel的命令连接 检测主观下线：每秒向所有与它创建了命令连接的实例发送PING，并通过返回的PING命令回复判断是否在线 检测客观下线： 当一个主服务器主观下线后，为了确保主服务器真下线了，会向同样监视这服务器的其他sentinel进行询问 当认为主服务器进入下线状态的sentinel数量超过配置中设置的quorum参数值，就进入客观下线状态 选举领头sentinel：当一个服务器客观下线时，监视该服务器的sentinel会选举出一个领头sentinel，并由领头进行故障转移 选举方式： 监视同一主服务器的在线sentinel都有资格 每次选举无论成功与否，配置纪元的值都会自增一次 在一个配置纪元里，所有sentinel都有一次将某个sentinel设置为局部头的机会，并且一旦设置不可更改 每个发现主服务器进入客观下线的sentinel都会要求其他sentinel将自己设为局部头 设置局部头的规则是先到先得，之后接收到的所有设置要求都会被拒绝 如果有某个sentinel被半数以上设置成了局部头，那么就成为领头 在给定时限内，没有一个被选举为领头，那么将在一段时间后再次选举 故障转移： 选出新的主服务器：从已下线主服务器的所有从服务器列表中筛选 删除列表中所有处于下线或者断线状态的从服务器 删除列表中5s内没有回复过领头sentinel的info命令的从服务器 删除所有与已下线服务器连接断开超过down-after-milliseconds*10毫秒的从服务器 根据从服务器优先级排序，相同优先级按照从服务器复制偏移量排序，相同偏移量按运行ID最小排序 向选出的从服务器发送SLAVEOF no one命令升级为主服务器 修改从服务器的赋值目标 向其他从服务器发送slaveof命令，让他们复制新主服务器 将旧的主服务器变成从服务器 当旧的主服务器重新上线时，会向他发出slaveof命令，让他变成从服务器 redis集群？Redis集群是Redis提供的分布式数据库方案，集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能。 节点：一个redis集训由多个节点构成 启动节点：redis 启动时会根据cluster-enabled配置是否为yes决定是否开启集训模式 集群数据结构： clusterNode：保存了节点的创建时间、名称、配置纪元、ip、端口号等 clusterLink：保存了连接节点所需的套接字描述符、输入缓冲区、输出缓冲区 redisClient：保存了连接客户端所需的套接字描述符、输入缓冲区、输出缓冲区 节点之间建立连接：通过cluster meet节点之间握手 节点A会为节点B创建一个clusterNode结构，并将该结构添加到自己的clusterState.nodes字典里面。 节点A将根据CLUSTER MEET命令给定的IP地址和端口号，向节点B发送一条MEET消息（message） 节点B将接收到节点A发送的MEET消息，节点B会为节点A创建一个clusterNode结构，并将该结构添加到自己的clusterState.nodes字典里面。 节点B将向节点A返回一条PONG消息。 节点A将接收到节点B返回的PONG消息，通过这条PONG消息节点A可以知道节点B已经成功地接收到了自己发送的MEET消息。 节点A将向节点B返回一条PING消息。 节点B将接收到节点A返回的PING消息，通过这条PING消息节点B可以知道节点A已经成功地接收到了自己返回的PONG消息，握手完成。 槽指派：集群的整个数据库被分为16384个槽（slot），数据库中的每个键都属于这16384个槽的其中一个，集群中的每个节点可以处理0个或最多16384个槽。 记录节点的槽指派信息：clusterNode结构的slots属性和numslot属性记录了节点负责处理哪些槽 slots属性是包含16384位的二进制数组，对应索引i上的值为1那么表示节点负责处理槽i。 传播节点的槽指派信息：将自己的slots数组通过消息发送给集群中的其他节点，以此来告知其他节点自己目前负责处理哪些槽。 记录集群中所有的槽指派信息：clusterState结构中的slots数组记录了集群中所有16384个槽的指派信息，slots数组包含16384个项，每个数组项都是一个指向clusterNode结构的指针 在集训中执行命令：客户端向节点发送与数据库键有关的命令时，接收命令的节点会计算出命令要处理的数据库键属于哪个槽，并检查这个槽是否指派给了自己 如果键所在的槽正好就指派给了当前节点，那么节点直接执行这个命令。 如果键所在的槽并没有指派给当前节点，那么节点会向客户端返回一个MOVED错误，指引客户端转向（redirect）至正确的节点，并再次发送之前想要执行的命令 重新分片：Redis集群的重新分片操作可以将任意数量已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽所属的键值对也会从源节点被移动到目标节点 ASK错误：在进行重新分片期间：属于被迁移槽的一部分键值对保存在源节点里面，而另一部分键值对则保存在目标节点里面，当客户端向源节点发送一个与数据库键有关的命令，若此时数据不在原节点上，返回ASK并指引客户端转向目标节点。 复制与故障转移： 设置从节点：向一个节点发送命令CLUSTER REPLICATE ，让接收命令的节点称为node_id的从节点 故障检测： 集群中的每个节点都会定期地向集群中的其他节点发送PING消息，如果没有在规定时间返回PONG消息，那么该节点会被疑似下线。 集群中的各个节点会通过互相发送消息的方式来交换集群中各个节点的状态信息，来确认某个节点是疑似下线还是已下线。 当一个主节点A通过消息得知主节点B认为主节点C进入了疑似下线状态时，会记录主节点C的状态。 当集群中半数以上的主节点都将某节点疑似下线，那么这个主节点被标记为已下线。 将主节点x标记为已下线的节点会向集群广播一条关于主节点x的FAIL消息，所有收到这条FAIL消息的节点都会立即将主节点x标记为已下线。 故障转移：当一个从节点发现自己正在复制的主节点进入了已下线状态时，从节点将开始对下线主节点进行故障转移 选举新的主节点 集群的配置纪元是一个自增计数器，它的初始值为0。 当集群里的某个节点开始一次故障转移操作时，集群配置纪元的值会被增一 对于每个配置纪元，集群里每个负责处理槽的主节点都有一次投票的机会，而第一个向主节点要求投票的从节点将获得主节点的投票。 当从节点发现自己正在复制的主节点进入已下线状态时，从节点会向集群广播一条CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST消息，要求所有收到这条消息、并且具有投票权的主节点向这个从节点投票 如果一个主节点具有投票权（它正在负责处理槽），并且这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，表示这个主节点支持从节点成为新的主节点。 如果集群里有N个具有投票权的主节点，那么当一个从节点收集到大于等于N/2+1张支持票时，这个从节点就会当选为新的主节点。 如果在一个配置纪元里面没有从节点能收集到足够多的支持票，那么集群进入一个新的配置纪元，并再次进行选举，直到选出新的主节点为止。 被选中的从节点会执行SLAVEOF no one命令，成为新的主节点 新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。 新的主节点向集群广播一条PONG消息，这条PONG消息可以让集群中的其他节点立即知道这个节点已经由从节点变成了主节点，并且这个主节点已经接管了原本由已下线节点负责处理的槽。 新的主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成 为什么redis cluster至少需要三个主节点？一个节点，不能给自己投票。两个节点 A 说 B 下线，B 认为 A 下线，两个人互相说我连接不上你，没有定论。至少三个节点，A、B 发现 C 不通，互相通知，得到一致性状态：C 的确下线。 Redis Cluster 方案什么情况下会导致整个集群不可用？有 A，B，C 三个节点的集群，在没有复制模型的情况下，如果节点 B 宕机了，那么整个集群就会以为缺少 5501-11000 这个范围的槽而不可用。当然，这种情况也可以配置 cluster-require-full-coverage=no ，整个集群无需所有槽位覆盖,当两个主节点都宕机时，集群停止服务。cluster-require-full-coverage=yes，有一个主节点宕机是，无从节点进行故障恢复，此时整个集群不可用。 一个 Redis 实例最多能存放多少的 keys？List、Set、Sorted Set 他们最多能存放多少元素？Redis 可以处理多达 2^32 的 keys ，并且在实际中进行了测试，每个实例至少存放了 2 亿 5 千万的 keys。任何 list、set、和 sorted set 都可以放 2^32 个元素。 假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？使用 keys 指令可以扫出指定模式的 key 列表。对方接着追问：如果这个 Redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问题？Redis 的单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的 key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 keys 指令长。 Redis 常见的性能问题都有哪些？如何解决？ Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件。 Master 写内存快照，save 命令调度 rdbSave 函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以 Master 最好不要写内存快照。 Master AOF 持久化，如果不重写 AOF 文件，这个持久化方式对性能的影响是最小的，但是 AOF 文件会不断增大，AOF 文件过大会影响 Master 重启的恢复速度。 所以，Master 最好不要做任何持久化工作，包括内存快照和 AOF 日志文件，特别是不要启用内存快照做持久化。如果数据比较关键，某个 Slave 开启AOF备份数据，策略为每秒同步一次。 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3… 。 这样的结构，也方便解决单点故障问题，实现 Slave 对 Master 的替换。如果 Master挂了，可以立刻启用 Slave1 做 Master ，其他不变。 skiplist与平衡树、哈希表的比较 skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。 在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 zookeeper它是一个分布式服务框架，主要用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务，集群管理、分布式应用配置项的管理等。简单来说：zk是一个拥有文件系统特点的数据库；zk是一个解决了数据一致性问题的分布式数据库；zk是一个具有发布和订阅功能的分布式数据库。 算法如何实现抢红包算法？发出一个固定金额的红包，由若干个人来抢，需要满足哪些规则？ 所有人抢到金额之和等于红包金额，不能超过，也不能少于。 每个人至少抢到一分钱。 要保证所有人抢到金额的几率相等。 方法1：二倍均值法剩余红包金额为M，剩余人数为N，那么有如下公式：每次抢到的金额 = 随机区间 （0， M / N X 2）这个公式，保证了每次随机金额的平均值是相等的，不会因为抢红包的先后顺序而造成不公平。举个栗子：假设有10个人，红包总额100元。100/10X2 = 20, 所以第一个人的随机范围是（0，20 )，平均可以抢到10元。假设第一个人随机到10元，那么剩余金额是100-10 = 90 元。90/9X2 = 20, 所以第二个人的随机范围同样是（0，20 )，平均可以抢到10元。假设第二个人随机到10元，那么剩余金额是90-10 = 80 元。80/8X2 = 20, 所以第三个人的随机范围同样是（0，20 )，平均可以抢到10元。以此类推，每一次随机范围的均值是相等的。 方法2：线段切割法何谓线段切割法？我们可以把红包总金额想象成一条很长的线段，而每个人抢到的金额，则是这条主线段所拆分出的若干子线段。如何确定每一条子线段的长度呢？由“切割点”来决定。当N个人一起抢红包的时候，就需要确定N-1个切割点。因此，当N个人一起抢总金额为M的红包时，我们需要做N-1次随机运算，以此确定N-1个切割点。随机的范围区间是（1， M）。当所有切割点确定以后，子线段的长度也随之确定。这样每个人来抢红包的时候，只需要顺次领取与子线段长度等价的红包金额即可。 排序各种排序算法的时间复杂度以及稳定性？ 冒泡排序 算法原理：相邻的数据进行两两比较，小(大)数放在前面，大(小)数放在后面，这样一趟下来，最小(大)的数就被排在了第一位，第二趟也是如此，如此类推，直到所有的数据排序完成。 时间复杂度：最坏：O(n2) 最好: O(n) 平均: O(n2) 空间复杂度：O(1) 稳定性： 稳定 选择排序 算法原理：先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 时间复杂度：最坏：O(n2) 最好: O(n) 平均: O(n2) 空间复杂度：O(1) 稳定性： 稳定 直接插入排序 算法原理：每次将一个待排序的数据按照其关键字的大小插入到前面已经排序好的数据中的适当位置，直到全部数据排序完成。 时间复杂度：最坏：O(n2) 最好: O(n) 平均: O(n2) 空间复杂度：O(1) 稳定性： 稳定 快速排序 算法原理： 从数列中挑出一个元素作为基准数。 分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。 再对左右区间递归执行第二步，直至各区间只有一个数。 时间复杂度：最坏：O(n2) 最好: O(nlogn) 平均: O(nlogn) 空间复杂度：O(logn) 稳定性： 不稳定 归并排序 算法原理：将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 时间复杂度：最坏：O(nlogn) 最好: O(n) 平均: O(nlogn) 空间复杂度：O(1) 稳定性： 稳定 堆排序 算法原理：堆排序就是把最大堆堆顶的最大数取出，将剩余的堆继续调整为最大堆，再次将堆顶的最大数取出，这个过程持续到剩余数只有一个时结束。 时间复杂度：最坏：O(nlogn) 最好: O(nlogn) 平均: O(nlogn) 空间复杂度：O(1) 稳定性： 不稳定 12345678910111213141516171819202122232425262728293031/* * * 堆排序（升序排列）-建立最大堆 * @param array */public static void heapSortAsc(int[] a) &#123; int len = a.length; for (int i = len / 2 - 1; i &gt;= 0; i--) &#123; maxHeapDown(a, i, len - 1); &#125; for (int i = len - 1; i &gt; 0; i--) &#123;//堆顶和末尾交换 swap(a, i, 0); maxHeapDown(a, 0, i - 1); &#125;&#125;public static void maxHeapDown(int[] a, int start, int end) &#123;//自顶向下调整成为最大堆 int son = start * 2 + 1;//左儿子 int root = a[start]; for (; son &lt;= end; start = son, son = 2 * son + 1) &#123; if (son &lt; end &amp;&amp; a[son] &lt; a[son + 1]) &#123; son++;//右儿子 &#125; if (root &gt;= a[son]) break; else &#123; swap(a, start, son); &#125; &#125;&#125; 负载均衡算法完全随机对所有机器随机选择缺点：服务器有好有坏，处理能力是不同的，我们希望性能好的服务器多处理些请求，性能差的服务器少处理一些请求。 加权随机为每台服务器设置了权重，权重大的服务器获得的概率大一些，权重小的服务器获得的概率小一些。 完全轮询按顺序一个一个执行 普通加权轮询有三个节点{a, b, c}，他们的权重分别是{a=5, b=1, c=1}。发送7次请求，a会被分配5次，b会被分配1次，c会被分配1次。轮训所有节点，找到一个最大权重节点；选中的节点权重-1；直到减到0，恢复该节点原始权重，继续轮询； 平滑加权轮询选中的服务器非固定权重：固定权重-各个服务器的权重之和其他服务器非固定权重 = 固定权重。每一轮将当前有效权重最大的实例减去所有实例的权重和，且变量 currentPos 指向此位将每个实例的 非固定权重 都加上 固定权重 一致性hash算法普通的hash算法：如果我们采用普通的hash算法进行路由，将数据映射到具体的节点上，如key%N，key是数据的key，N是机器节点数，如果有一个机器加入或退出这个集群，则所有的数据映射都无效了，如果是持久化存储则要做数据迁移，如果是分布式缓存，则其他缓存就失效了。一致性hash算法： * 环形hash空间：按照常用的hash算法来将对应的key哈希到一个具有2^32次方个节点的空间中，即0 ~ (2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。 * 映射服务器节点：将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或唯一主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。 * 映射数据：对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上,然后从数据所在位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。 * 服务器的删除与添加 * 如果此时NodeC宕机了，此时Object A、B、D不会受到影响，只有Object C会重新分配到Node D上面去，而其他数据对象不会发生变化 * 如果在环境中新增一台服务器Node X，通过hash算法将Node X映射到环中，通过按顺时针迁移的规则，那么Object C被迁移到了Node X中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。 平衡性问题：当服务器节点比较少的时候，会出现一个问题，就是此时必然造成大量数据集中到一个节点上面，极少数数据集中到另外的节点上面。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以先确定每个物理节点关联的虚拟节点数量，然后在ip或者主机名后面增加编号。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： 大数据问题给一个超过100G大小的log file,log中存着IP地址 ，设计算法找到出现次数最多的IP地址？但凡是大数据的问题，都可通过切分来解决它。如果我们将其分成1000个小文件，每个文件大概就是500M左右的样子，现在计算机肯定轻轻 松松就能装下。那么，问题又来了，怎样才能保证相同的IP被分到同一个文件中呢？这里我想到的是哈希切分，使用相同的散列函数（如 BKDRHash）将所有IP地址转换为一个整数key，再利用 index=key%1000就可将相同IP分到同一个文件。 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存 2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持 不变。扫描后，查看bitmap，把对应位是01的整数输出即可。 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？使用Bitmap，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位。由于2^32=42.9+亿，那么2^32bit才能存下40亿个数，也就需要2^32=4Gb=0.5GB=512M内存。读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。 智商题扩展分布式ID解决方案 基于UUID:对于数据库来说用作业务主键ID，它不仅是太长还是字符串，存储性能差查询也很耗时，所以不推荐用作分布式ID。 优点：生成足够简单，本地生成无网络消耗，具有唯一性 缺点：无序的字符串，不具备趋势自增特性;没有具体的业务含义;长度过长16 字节128位，36位长度的字符串，存储以及查询对MySQL的性能消耗较大，MySQL官方明确建议主键要尽量越短越好，作为数据库主键 UUID 的无序性会导致数据位置频繁变动，严重影响性能。 基于单数据库自增ID 优点：实现简单，ID单调自增，数值类型查询速度快 缺点：DB单点存在宕机风险，无法扛住高并发场景 基于数据库集群模式:那这样还会有个问题，两个MySQL实例的自增ID都从1开始，会生成重复的ID怎么办？ 操作：设置起始值和自增步长 优点：解决DB单点问题 缺点：不利于后续扩容，新增第三台MySQL实例需要人工修改一、二两台MySQL实例的起始值和步长。 基于Redis模式:利用redis的 incr命令实现ID的原子性自增 缺点：RDB会定时打一个快照进行持久化，假如连续自增但redis没及时持久化，而这会Redis挂掉了，重启Redis后会出现ID重复的情况。 缺点：AOF会对每条写命令进行持久化，即使Redis挂掉了也不会出现ID重复的情况，但由于incr命令的特殊性，会导致Redis重启恢复的数据时间过长。 基于数据库的号段模式：从数据库批量的获取自增ID，每次从数据库取出一个号段范围，例如 (1,1000] 代表1000个ID，具体的业务服务将本号段，生成1~1000的自增ID并加载到内存 数据库中存储 当前最大id, 号段的步长 多业务端可能同时操作，所以采用版本号version乐观锁方式更新， 优点：这种分布式ID生成方式不强依赖于数据库，不会频繁的访问数据库，对数据库的压力小很多。 缺点：id为连续号段，可能会被枚举，安全性不足；服务器出现问题重启，部分在内存中的号段会丢失。 优化方式：双buffer机制，在号段用完前并发获取下一个号段 基于雪花算法（Snowflake）模式 组成：Snowflake ID组成结构：正数位（占1比特）+ 时间戳（占41比特）+ 机器ID（占5比特）+ 数据中心（占5比特）+ 自增值（占12比特），总共64比特组成的一个Long类型。 第一个bit位（1bit）：Java中long的最高位是符号位代表正负，正数是0，负数是1，一般生成ID都为正数，所以默认为0。 时间戳部分（41bit）：毫秒级的时间，不建议存当前时间戳，而是用（当前时间戳 - 固定开始时间戳）的差值，可以使产生的ID从更小的值开始；41位的时间戳可以使用69年，(1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69年 工作机器id（10bit）：也被叫做workId，这个可以灵活配置，机房或者机器号组合都可以。 序列号部分（12bit），自增值支持同一毫秒内同一个节点可以生成4096个ID 操作：当序列号部分超过4096时，等待下一毫秒；当目前时间戳小于最后一次的时间戳时，时钟回拨，报错！！ 优点：解除对db的依赖；需要人工配置工作id；只需保证每个业务应用有自己的工作机器id即可，而不需要单独去搭建一个获取分布式ID的应用。 缺点：依赖时间戳，存在机器时钟回拨 百度（uid-generator）:自动生成workId 组成：正数位（占1比特） + 时间秒（占28位）+ workId（占22比特）+ 自增值（占13位）； 操作：依赖db，在服务器启动的时候，根据hostName，port，type和 date 插入db中取出对应的自增di作为workId。 CachedUidGenerator：采用RingBuffer环状数组，初始时按填充时的时间戳填充ring数组，取id时顺序从数组中取，异步填充数组。 美团(Leaf): 基于ZooKeeper的顺序Id生成workId 分布式锁基于MySQl实现假设 有3个进程 要进行购票，如何实现让三个进程排队买票？这时就采用MySql，可以建立一张表，用一个value字段表示锁，让三个进程先到mysql中insert value，谁插入value成功，谁就获取到了锁，就可以进行购票了，其他进程只能等待value删除。那如果此时获取锁的进程A宕机了，锁被该进程持有，而其他进程等待该锁，就形成了死锁，因此我们怎么处理呢？我们可以增加一个进程用来监视mysql嘛，当某线程持有锁的时间到达XXX时，由该监视进程将锁取消，就解除了死锁。那又有新的问题出现，如果设置时间过短监视进程将锁取消后，A进程又醒来，而B也刚好拿到锁去买票，就会产生一票多卖的情况。如果设置时间过长，那么其他线程又会持续等待，浪费性能。因此可以使用基于Zookeeper的实现方式。 基于ZooKeeper实现参考https://www.cnblogs.com/bailing80/p/11443409.html]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-HashMap源码解析]]></title>
    <url>%2FJAVA%2FJava-HashMap%2F</url>
    <content type="text"><![CDATA[HashMap 的存储结构HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;// key的hash值 final K key; // key V value; // value Node&lt;K,V&gt; next; //同一个hash值下的链表/红黑树 Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; TreeNode&lt;K,V&gt; 继承 LinkedHashMap.Entry&lt;K,V&gt;，用来实现红黑树相关的存储结构 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // 存储当前节点的父节点 TreeNode&lt;K,V&gt; left; // 存储当前节点的左孩子 TreeNode&lt;K,V&gt; right; // 存储当前节点的右孩子 TreeNode&lt;K,V&gt; prev; // prev则指向前一个节点（原链表中的前一个节点） boolean red; // 存储当前节点的颜色（红、黑） TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; final TreeNode&lt;K,V&gt; root() &#123; &#125; static &lt;K,V&gt; void moveRootToFront(Node&lt;K,V&gt;[] tab, TreeNode&lt;K,V&gt; root) &#123; &#125; final TreeNode&lt;K,V&gt; find(int h, Object k, Class&lt;?&gt; kc) &#123; &#125; final void treeify(Node&lt;K,V&gt;[] tab) &#123; &#125; final Node&lt;K,V&gt; untreeify(HashMap&lt;K,V&gt; map) &#123; &#125; final TreeNode&lt;K,V&gt; putTreeVal(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int h, K k, V v) &#123; &#125; final void removeTreeNode(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, boolean movable) &#123; &#125; final void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) &#123; &#125; /* ------------------------------------------------------------ */ // Red-black tree methods, all adapted from CLR // 红黑树相关操作 static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateLeft(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateRight(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; &#125; static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceDeletion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; &#125; static &lt;K,V&gt; boolean checkInvariants(TreeNode&lt;K,V&gt; t) &#123; &#125; &#125; 各常量、成员变量作用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * 默认初始容量 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * 最大容量， 当传入容量过大时将被这个值替换 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 默认负载因子 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 当链表的长度超过8，有可能会转化成树 */static final int TREEIFY_THRESHOLD = 8;/** * 当链表的长度小于6则会从红黑树转回链表 */static final int UNTREEIFY_THRESHOLD = 6;/** * 在转变成树之前，还会有一次判断，只有键值对数量大于 64 才会发生转换。 * 这是为了避免在哈希表建立初期，多个键值对恰好被放入了同一个链表中而导致不必要的转化。 */static final int MIN_TREEIFY_CAPACITY = 64;/** * 用来存储 key-value 的节点对象。在 HashMap 中它有个专业的叫法 buckets ，中文叫作桶。 */transient Node&lt;K,V&gt;[] table;/** * 同时封装了 keySet 和 values 的视图 */transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;/** * 容器中实际存放 Node 的大小 */transient int size;/** * HashMap 在结构上被修改的次数，结构修改是指改变HashMap中映射的次数，或者以其他方式修改其内部结构(例如，rehash)。 */transient int modCount;/** * HashMap的扩容阈值(=负载因子*table的容量) * 在HashMap中存储的Node键值对超过这个数量时，自动扩容容量为原来的二倍 * @serial */int threshold;/** * 负载因子 * @serial */final float loadFactor; 构造方法12345678910111213141516171819202122232425262728293031323334353637/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity 初始容量 * @param loadFactor 负载因子 * @throws IllegalArgumentException 如果初始容量为负或者负载因子非正数抛出该异常 */public HashMap(int initialCapacity, float loadFactor) &#123; // 当初始容量为负 if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); // 当初始容量大于最大容量 2^30 ，初始容量= 2^30 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 当负载因子非正数 或 负载因子是NaN(Not a Number，0.0f/0.0f的值就是NaN) if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // 赋值 this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125;/** * 获取比传入参数大的最小的2的N次幂。 */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; put方法put方法主要是调用putVal方法 12345678910111213141516/** * 使 key 和 value 产生关联，但如果有相同的 key 则新的会替换掉旧的。 */public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;/** * h &gt;&gt;&gt; 16:无符号右移动16位，意味着取高16位二进制 * 低16位与高16位进行异或 */static final int hash(Object key) &#123; int h; // 如果为 null 则返回的就是 0，否则就是 hashCode 异或上 hashCode 无符号右移 16 位 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; putVal方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * Implements Map.put and related methods * * @param hash key的hash值 * @param key the key * @param value the value to put * @param onlyIfAbsent 如果true代表不更改现有的值 * @param evict 如果为false表示table为创建状态 * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; /** * 判断table是否等于空或者table的长度等于0，如果是就进行初始化 * 此时通过resize()方法得到初始化的table */ if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; /** * 对hash码进行散列 ，对值的位置进行确认 * 如果tab[i] 为null 表示没有hash冲突，就新增一个元素 */ if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; // 如果tab[i] 不为null，表示该位置有值了。 Node&lt;K,V&gt; e; K k; //HashMap中判断key相同的条件是key的hash相同，并且符合equals方法。这里判断了p.key是否和插入的key相等，如果相等，则将p的引用赋给e //这里为什么要把p赋值给e，而不是直接覆盖原值呢？答案很简单，现在我们只判断了第一个节点，后面还可能出现key相同，所以需要在最后一并处理 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; /** * 判断是否是红黑树 * p是红黑树节点，那么肯定插入后仍然是红黑树节点，所以我们直接强制转型p后调用TreeNode.putTreeVal方法，返回的引用赋给e */ else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 否则就是链表 else &#123; // 如果是链表，要遍历到最后一个节点进行插入 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; // 插入到链尾 p.next = newNode(hash, key, value, null); // 判断节点的长度是否大于TREEIFY_THRESHOLD红黑树的阈值，是就进行转换 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 对链表中的相同 hash 值且 key 相同的进一步作检查 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; /** * 插入 */ if (e != null) &#123; // existing mapping for key // 取出旧值，onlyIfAbsent此时为 false，所以不管 oldValue 有与否，都拿新值来替换 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; // 记录修改次数 ++modCount; // 超过阈值 threshold = capacity * factor，调用 resize() 进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; resize 扩容兼初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * 扩容兼初始化 * @return the table */final Node&lt;K,V&gt;[] resize() &#123; //将原来的table指针保存 Node&lt;K,V&gt;[] oldTab = table; //获取原来数组的长度，oldTab为null说明还没有进行初始化 int oldCap = (oldTab == null) ? 0 : oldTab.length; //保存以前重构table的阈值 int oldThr = threshold; int newCap, newThr = 0; //oldCap &gt; 0表示已经初始化过了 if (oldCap &gt; 0) &#123; //当原来的容量已经达到最大容量的时候，将阈值设置为Integer.MAX_VALUE，这样就不会再发生重构的情况 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 否则将旧的容量扩大两倍 // 当它小于最大容量，并且旧的容量大于初始化最小容量的时候， // 将新的阈值设置为旧的阈值的两倍, 新的容量设置为旧容量的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //虽然还没有初始化，但是设置过了阈值，将旧的阈值设置为新的容量 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; //没有初始化阈值的时候采用默认算法计算阈值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 对应oldCap = 0 &amp;&amp; oldThr &gt; 0的情况 if (newThr == 0) &#123; //重新用默认负载因子计算 扩容阈值 float ft = (float)newCap * loadFactor; // 如果新容量小于最大容量 &amp;&amp; 新扩容阈值(ft) 小于最大容量 // 新阈值 = ft 否则 新阈值 = int的最大范围 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; // 把当前阈值设为新阈值 threshold = newThr; // 根据新容量，创建新table @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; // 将当前table 设置为新扩容的table table = newTab; // 如果已经被初始化过 if (oldTab != null) &#123; //将旧数组中的元素全部取出，重新映射到新数组中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) // 该节点没有next节点，表示没有链表，没有冲突，那重新计算下位置 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 冲突的是一棵树节点，分裂成 2 个树，或者如果树很小就转成链表 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 对原来的链表部分进行重构 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 索引不变 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到 tables 里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到 tables 里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; // 返回扩容后的table return newTab;&#125; split 扩容时重新划分树12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * 扩容时重新划分树 * @param map the map * @param tab 扩容后的数组 * @param index 数组索引 * @param bit 原数组容量 */final void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) &#123; //b设置为当前桶的头节点 TreeNode&lt;K,V&gt; b = this; // Relink into lo and hi lists, preserving order //低位树链表头节点，尾结点 TreeNode&lt;K,V&gt; loHead = null, loTail = null; //高位树链表头节点，尾结点 TreeNode&lt;K,V&gt; hiHead = null, hiTail = null; //lc低位树链表中节点个数，hc高位树链表中节点个数 int lc = 0, hc = 0; //e当前遍历节点 for (TreeNode&lt;K,V&gt; e = b, next; e != null; e = next) &#123; //当前遍历节点的后继节点 next = (TreeNode&lt;K,V&gt;)e.next; //将当前遍历节点后继设为null e.next = null; if ((e.hash &amp; bit) == 0) &#123; //==0表示当前遍历节点应该存放在低位 //如果低位中还没有节点 //将当前遍历节点设为低位头节点 if ((e.prev = loTail) == null) loHead = e; else //将当前遍历节点加入到链表的尾部 loTail.next = e; //设置尾节点为当前遍历节点 loTail = e; //低位链表中的节点数加1 ++lc; &#125; //!=0表示当前遍历节点应该存放在高位 else &#123; if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; &#125; &#125; //如果低位节点链表不为null if (loHead != null) &#123; //低位节点链表中节点的个数小于6，此时需要将树结构转为链表结构 if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123;//链表中节点的个数满足保持树型结构所需要的节点个数 //低位桶的头节点设为链表的头节点 tab[index] = loHead; // 高位节点链表不为null，说明有节点从原树结构中分离出去了 // 原有树结构被破坏 // 所以低位的节点链表需要重新构建树结构 if (hiHead != null) // (else is already treeified) loHead.treeify(tab); &#125; &#125; // 同理 if (hiHead != null) &#123; if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); &#125; &#125;&#125; putTreeVal 在红黑树中添加节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100/** * * @param map 当前节点所在的HashMap对象 * @param tab 当前HashMap对象的元素数组 * @param h key的hash值 * @param k key * @param v value * @return 指定key所匹配到的节点对象，针对这个对象去修改V（返回空说明创建了一个新节点） */final TreeNode&lt;K,V&gt; putTreeVal(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int h, K k, V v) &#123; // 定义k的Class对象 Class&lt;?&gt; kc = null; // 标识是否已经遍历过一次树，未必是从根节点遍历的，但是遍历路径上一定已经包含了后续需要比对的所有节点。 boolean searched = false; //父节点不为空那么查找根节点，为空那么自身就是根节点 TreeNode&lt;K,V&gt; root = (parent != null) ? root() : this; // 从根节点开始遍历，没有终止条件，只能从内部退出 for (TreeNode&lt;K,V&gt; p = root;;) &#123; // 声明方向、当前节点hash值、当前节点的键对象 int dir, ph; K pk; // 如果当前节点hash 大于 指定key的hash值,要添加的元素应该放置在当前节点的左侧 if ((ph = p.hash) &gt; h) dir = -1; // 如果当前节点hash 小于 指定key的hash值,要添加的元素应该放置在当前节点的右侧 else if (ph &lt; h) dir = 1; // 如果当前节点的键对象 和 指定key对象相同,那么就返回当前节点对象 else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; // 走到这一步说明 当前节点的hash值 和 指定key的hash值 是相等的，但是equals不等 else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; // 走到这里说明：指定key没有实现comparable接口 或者 实现了comparable接口并且和当前节点的键对象比较之后相等（仅限第一次循环） /** * searched 标识是否已经对比过当前节点的左右子节点了 * 如果还没有遍历过，那么就递归遍历对比，看是否能够得到那个键对象equals相等的的节点 * 如果得到了键的equals相等的的节点就返回 * 如果还是没有键的equals相等的节点，那说明应该创建一个新节点了 */ if (!searched) &#123;// 如果还没有比对过当前节点的所有子节点 // 定义要返回的节点、和子节点 TreeNode&lt;K,V&gt; q, ch; // 标识已经遍历过一次了 searched = true; /** * 红黑树也是二叉树，所以只要沿着左右两侧遍历寻找就可以了 * 这是个短路运算，如果先从左侧就已经找到了，右侧就不需要遍历了 * find 方法内部还会有递归调用 */ if (((ch = p.left) != null &amp;&amp; (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.find(h, k, kc)) != null)) // 找到了指定key键对应的 return q; &#125; // 走到这里就说明，遍历了所有子节点也没有找到和当前键equals相等的节点 dir = tieBreakOrder(k, pk);// 再比较一下当前节点键和指定key键的大小 &#125; // 定义xp指向当前节点 TreeNode&lt;K,V&gt; xp = p; /** * 如果dir小于等于0，那么看当前节点的左节点是否为空， * 如果为空，就可以把要添加的元素作为当前节点的左节点，如果不为空，还需要下一轮继续比较 * * 如果dir大于等于0，那么看当前节点的右节点是否为空， * 如果为空，就可以把要添加的元素作为当前节点的右节点，如果不为空，还需要下一轮继续比较 * * 如果以上两条当中有一个子节点不为空， * 这个if中还做了一件事，那就是把p已经指向了对应的不为空的子节点，开始下一轮的比较 */ if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; // 如果恰好要添加的方向上的子节点为空，此时节点p已经指向了这个空的子节点 // 获取当前节点的next节点 Node&lt;K,V&gt; xpn = xp.next; // 创建一个新的树节点 TreeNode&lt;K,V&gt; x = map.newTreeNode(h, k, v, xpn); if (dir &lt;= 0) // 左孩子指向到这个新的树节点 xp.left = x; else // 右孩子指向到这个新的树节点 xp.right = x; // 链表中的next节点指向到这个新的树节点 xp.next = x; // 这个新的树节点的父节点、前节点均设置为 当前的树节点 x.parent = x.prev = xp; // 如果原来的next节点不为空 if (xpn != null) // 那么原来的next节点的前节点指向到新的树节点 ((TreeNode&lt;K,V&gt;)xpn).prev = x; // 重新平衡，以及新的根节点置顶 moveRootToFront(tab, balanceInsertion(root, x)); // 返回空，意味着产生了一个新节点 return null; &#125; &#125;&#125; treeifyBin 链表转成红黑树123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111/** * 将指定hash节点处的链表替换成红黑树 * 除非table太小了，将用resizes（）改变树的容量 * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; // 如果 table 为null 或者 // table的长度小于 MIN_TREEIFY_CAPACITY（默认64））不进行树化，调用resize进行扩容 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); // 如果 该hash值对应的 tab[index] 不为null，证明该位置有值 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; // 遍历链表,将单项链表改为双向链表 do &#123; // node 节点转成 TreeNode节点 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); // 如果树为空 if (tl == null) // 设置树的根 hd = p; else &#123; // 设置新节点p的上一个节点 p.prev = tl; // 设置 上一节点的 next 指向当前节点 tl.next = p; &#125; //保存上一节点 tl = p; &#125; while ((e = e.next) != null); // 如果树的根节点不为null，调用treeify()进行树化 if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; /** * Forms tree of the nodes linked from this node. * @return root of tree */ final void treeify(Node&lt;K,V&gt;[] tab) &#123; // 树的根节点 TreeNode&lt;K,V&gt; root = null; //x是当前节点，next是后继 for (TreeNode&lt;K,V&gt; x = this, next; x != null; x = next) &#123; // 后继节点 next = (TreeNode&lt;K,V&gt;)x.next; // 左右儿子设为null x.left = x.right = null; // 如果根是 null if (root == null) &#123; // 设置父节点为null x.parent = null; // 设置为黑色 x.red = false; // 把当前节点设置为根节点 root = x; &#125; // 如果有根节点 else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; // 遍历树，进行二叉搜索树的插入 for (TreeNode&lt;K,V&gt; p = root;;) &#123; // p指向遍历中的当前节点， // x为待插入节点， // k是x的key， // h是x的hash值， // ph是p的hash值， // pk是p的key， // dir用来指示x节点与p的比较，-1表示比p小，1表示比p大 // 不存在相等情况，因为HashMap中是不存在两个key完全一致的情况。 int dir, ph; K pk = p.key; // 待插入x的hash值小于当前点p的hash值 if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; // 如果hash值相等，那么判断k是否实现了comparable接口， // 如果实现了comparable接口就使用compareTo进行进行比较， // 如果仍旧相等或者没有实现comparable接口，则在tieBreakOrder中比较 else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); // 将当前节点赋值给XP TreeNode&lt;K,V&gt; xp = p; // 如果dir &lt;= 0 ,p 等于 p的左儿子，否则p 等于 p的右儿子 // 如果 p 等于 null，则插入x节点 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; // x的父节点指向 xp x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; // 进行平衡处理，保证红黑树的性质 root = balanceInsertion(root, x); break; &#125; &#125; &#125; &#125; //root节点移动到桶中的第一个元素，也就是链表的首节点 moveRootToFront(tab, root); &#125; untreeify 树变链表12345678910111213141516/** * 树变链表 * //重新创建链表节点，并形成链表结构 */final Node&lt;K,V&gt; untreeify(HashMap&lt;K,V&gt; map) &#123; Node&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; q = this; q != null; q = q.next) &#123; Node&lt;K,V&gt; p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; &#125; return hd;&#125; balanceInsertion 红黑树插入平衡123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * 红黑树的插入平衡处理 * @param root 根节点 * @param x 新插入节点 * @return */static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; //插入的节点默认为红色 x.red = true; // xp：当前节点的父节点 // xpp：爷爷节点 // xppl：左叔叔节点 // xppr：右叔叔节点 for (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123; // L1：如果父节点为空、说明当前节点就是根节点，那么把当前节点标为黑色，返回当前节点 if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; // 父节点不为空 // L2：如果 父节点为黑色 那么插入节点为红色不影响树的平衡 // L3：或者 爷爷节点为空 即 父节点是根节点 // 返回根节点 else if (!xp.red || (xpp = xp.parent) == null) return root; // L4：父节点和祖父节点都存在，并且其父节点是祖父节点的左节点 if (xp == (xppl = xpp.left)) &#123; // L4.1 插入节点的叔叔节点是红色 if ((xppr = xpp.right) != null &amp;&amp; xppr.red) &#123; // 将叔叔设置为黑色，父亲设为黑色，爷爷设为红色 xppr.red = false; xp.red = false; xpp.red = true; // 将 爷爷 设为当前插入节点，自底向上，重新变色 x = xpp; &#125; //L4.2：插入节点的叔叔节点是黑色或不存在 else &#123; // L4.2.1 插入节点是其父节点的右孩子 if (x == xp.right) &#123; // x 设置为 父节点 // 将 父节点 左旋 // 旋转之后，原父子关系对调，因此，x还是子节点 root = rotateLeft(root, x = xp); // 将xp 设为父节点，xpp设为爷爷节点 xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // L4.2.1 插入节点是其父节点的左孩子 if (xp != null) &#123; // 父节点设为黑色 xp.red = false; // 如果 有爷爷节点 if (xpp != null) &#123; // 爷爷节点设置为红色 xpp.red = true; // 爷爷节点右旋 root = rotateRight(root, xpp); &#125; &#125; &#125; &#125; // L5：插入的节点父节点和祖父节点都存在，并且其 父节点是祖父节点的右节点 else &#123; // L5.1：插入节点的叔叔节点是红色 if (xppl != null &amp;&amp; xppl.red) &#123; // 将叔叔设置为黑色，父亲设为黑色，爷爷设为红色 xppl.red = false; xp.red = false; xpp.red = true; // 将 爷爷 设为当前插入节点，自底向上，重新变色 x = xpp; &#125; //L5.2：插入节点的叔叔节点是黑色或不存在 else &#123; // L5.2.1 插入节点是其父节点的左孩子 if (x == xp.left) &#123; // x 设置为 父节点 // 将 父节点 右旋 // 旋转之后，原父子关系对调，因此，x还是子节点 root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // L5.2.2 插入节点是其父节点的右孩子 if (xp != null) &#123; // 父节点设为黑色 xp.red = false; // 如果 有爷爷 节点 if (xpp != null) &#123; // 爷爷节点设置为红色 xpp.red = true; // 爷爷节点右旋 root = rotateLeft(root, xpp); &#125; &#125; &#125; &#125; &#125;&#125; get方法1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; getNode123456789101112131415161718192021222324252627282930313233/** * Implements Map.get and related methods * * @param hash key 的hash值 * @param key the key * @return the node, or null if none */final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // table 不为null &amp;&amp; 不为空，并且 (n - 1) &amp; hash 算出来的index位置有值 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断第一个存在的节点的key 和 hash值是否与查询的key的相等，如果是直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 遍历该链表/红黑树直到next为null if ((e = first.next) != null) &#123; // 如果该点是红黑树 if (first instanceof TreeNode) // 在树上寻找对应的值 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; // 遍历链表 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; //否则不存在，返回null return null;&#125; remove 及其相关方法12345public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125; removeNode 删除节点12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Implements Map.remove and related methods * * @param hash key的hash值 * @param key the key * @param value 传入匹配的value值，如果matchValue=false，直接忽略 * @param matchValue 为true时，会去进一步匹配value * @param movable if false do not move other nodes while removing * @return the node, or null if none */final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //确定table已经被初始化，并且其中有元素，并且对应的 hash值有元素 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; //判断当前这个找到的元素是不是目标元素，如果是的话赋值给node if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; //不是的话，就从相同hash值的所有元素中去查找 else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) //该列表已经转换成红黑树的情况 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; //在链表中查找的情况 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //找到了目标节点 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) //是红黑树节点的情况 ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) //是一个链表开头元素的情况 tab[index] = node.next; else //是一个链表中间元素的情况 p.next = node.next; //结构改变，修改次数需要加一 ++modCount; //元素减少size减1 --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; removeTreeNode 红黑树中删除节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148final void removeTreeNode(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, boolean movable) &#123; int n; // 如果 table为空 或者为0 则无法删除，返回 if (tab == null || (n = tab.length) == 0) return; int index = (n - 1) &amp; hash; // first 和 root 目前都为 根结点 TreeNode&lt;K,V&gt; first = (TreeNode&lt;K,V&gt;)tab[index], root = first, rl; // succ 是要删除节点的下一个，prev是要删除节点的前一个 TreeNode&lt;K,V&gt; succ = (TreeNode&lt;K,V&gt;)next, pred = prev; // 首先先从TreeNode链表里将这个节点删除掉。 if (pred == null) tab[index] = first = succ; else pred.next = succ; if (succ != null) succ.prev = pred; if (first == null) return; if (root.parent != null) root = root.root(); // 根结点为空 或 根节点的右儿子为空 或 根结点的做儿子为空 或 根节点的做儿子的做儿子为空 if (root == null || root.right == null || (rl = root.left) == null || rl.left == null) &#123; // 树太小了，从红黑树转换为链表 tab[index] = first.untreeify(map); // too small return; &#125; // 从红黑树删除这个节点 // p是要删除的节点 pl 是 p 的左节点, pr 是 P 的右节点。 TreeNode&lt;K,V&gt; p = this, pl = left, pr = right, replacement; // 当 p 的左右节点都不为null if (pl != null &amp;&amp; pr != null) &#123; // s 是 p的右节点 的 左叶子节点 的左节点。。。循环找到叶子节点 // 就是查找所有比当前节点大的节点当中最小的一个节点 TreeNode&lt;K,V&gt; s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; //交换 s 和 p 节点的颜色 boolean c = s.red; s.red = p.red; p.red = c; // swap colors // sr ：s的右节点 TreeNode&lt;K,V&gt; sr = s.right; // pp ：p的父节点 TreeNode&lt;K,V&gt; pp = p.parent; //2： 如果s 等于pr 证明 p的 右节点 没有 左节点 if (s == pr) &#123; // p was s's direct parent //当前节点的父节点设为找到的节点 p.parent = s; //找到节点的右孩子设为当前节点 s.right = p; &#125; //3： p 的右节点有左儿子 // （以下操作就是将s和p对换了个位置和颜色， // 如果s有右儿子则设置为平衡点，否则p为平衡点） else &#123; // sp ： s的父亲 TreeNode&lt;K,V&gt; sp = s.parent; // 将 p 的父亲设为 sp // 如果sp 不为null if ((p.parent = sp) != null) &#123; // 当前节点替代 s 节点的位置，与 s 节点的父节点进行关联 // s 是 sp 的左儿子，左儿子设置为 p if (s == sp.left) sp.left = p; else// s 是 sp 的右儿子，右儿子设置为 p sp.right = p; &#125; // s 的右节点设置为 p 的右孩子 // 如果 p 的右孩子不为null // 右孩子的父节点设为 s 节点 if ((s.right = pr) != null) pr.parent = s; &#125; //当前节点的左孩子设为null p.left = null; //将 p 节点的右孩子设置为 s节点的右孩子 if ((p.right = sr) != null) sr.parent = p; //右孩子的父节点设为当前节点 // 将 s 节点的左儿子设置为 p节点的左儿子 if ((s.left = pl) != null) pl.parent = s;// 左孩子的父节点设为找到节点 // 将s的父亲 设置为 p 节点的父亲 // 如果 p是根结点，那么根节点变为s if ((s.parent = pp) == null) root = s; // 父节点不为null，当前节点是父节点的左孩子 else if (p == pp.left) // 父亲的左儿子改为s pp.left = s; else // 父亲的右儿子改为s pp.right = s; // s节点的右孩子不为null if (sr != null) // 则s节点的右孩子节点作为平衡删除的初始节点 replacement = sr; else // 否则平衡删除的初始节点为 p。 replacement = p; &#125; //当前节点左孩子不为null，右孩子为null //当前节点的左孩子作为平衡删除的节点 else if (pl != null) replacement = pl; //当前节点右孩子不为null，左孩子为null // 当前节点的右孩子作为平衡删除的节点 else if (pr != null) replacement = pr; else//左右孩子都为null，当前节点作为平衡删除的节点 replacement = p; //如果当前节点不是作为平衡删除的节点 if (replacement != p) &#123; //此时p.parent有三种情况，1.其原来的父节点，2.它的右子节点，3.s节点的父节点sp //将当前节点的父节点设为平衡删除节点的父节点 TreeNode&lt;K,V&gt; pp = replacement.parent = p.parent; //如果当前节点的父节点为null，则表明当前节点原来是根节点，此时当前节点的父节点还是其原来的父节点 if (pp == null) root = replacement;//设置作为平衡删除的节点为根节点 // 如果父节点不为null，并且当前节点是父节点的左子节点 else if (p == pp.left)//设置父节点的左子节点为作为平衡删除的节点 pp.left = replacement; else//设置父节点的右子节点为作为平衡删除的节点 pp.right = replacement; //将当前节点与树结构断开，因为当前节点不是作为平衡删除的节点 p.left = p.right = p.parent = null; &#125; // 获取根节点，如果删除节点是红色，则不需要做平衡删除，根节点不会再变 // 如果删除节点是黑色，则需要做平衡删除，根节点可能会发生改变 TreeNode&lt;K,V&gt; r = p.red ? root : balanceDeletion(root, replacement); //做平衡删除的节点是当前要删除的节点 //平衡删除做完后将该节点从树结构中脱离出来 if (replacement == p) &#123; // detach TreeNode&lt;K,V&gt; pp = p.parent; p.parent = null; if (pp != null) &#123; if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; &#125; &#125; //是否需要从新设置桶的头节点 if (movable) //将桶的头节点设为根节点 moveRootToFront(tab, r);&#125; balanceDeletion 红黑树删除平衡123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130/** * 红黑树平衡删除，只有在删除的节点是黑色时才会调用此方法 * @param root 根结点 * @param x 平衡节点 * @return */static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceDeletion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; for (TreeNode&lt;K,V&gt; xp, xpl, xpr;;) &#123; //平衡节点为null或者平衡节点是根节点，无需做平衡处理，返回根节点 if (x == null || x == root) return root; //平衡节点不是根，但平衡节点的父节点是null //则将平衡节点将成为根节点，将其颜色设置为黑色 else if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; //不是根节点，父节点也不为null， //若做平衡的节点就是要删除的节点，则设什么颜色都无所谓，直接返回根节点 //若做平衡的节点不是要删除的节点，而且还是红色 //则其要替换的节点就是其父节点，替换的父节点就是要删除的节点， //而做平衡的节点还是红色，则其父一定是黑色，其替换其父所以要变成黑色 //因为做平衡的节点是红，它替换掉了其父（在进入此方法之前已经替换掉了） //其父是黑，所以只需要确保其是黑，就能确保该树枝上的黑色节点数不变 //替换完直接返回根节点即可 else if (x.red) &#123; x.red = false; return root; &#125; //x的节点不是根，父也不为null，其颜色是黑色 //x的节点是其父的左子节点 else if ((xpl = xp.left) == x) &#123; //如果它有红色的兄弟节点xpr，那么它的父亲节点xp一定是黑色节点 // 将兄弟节点设为黑色，父亲节点设为红色，以父节点进行旋转 if ((xpr = xp.right) != null &amp;&amp; xpr.red) &#123; xpr.red = false; xp.red = true; //对父节点xp做左旋转 root = rotateLeft(root, xp); //重新将xp指向x的父节点，xpr指向xp新的右孩子 xpr = (xp = x.parent) == null ? null : xp.right; &#125; //如果xpr为空，则向上继续调整，将x的父节点xp作为新的x继续循环 if (xpr == null) x = xp; else &#123; //sl和sr分别为其兄弟节点左儿子和右儿子 TreeNode&lt;K,V&gt; sl = xpr.left, sr = xpr.right; //若sl和sr都为黑色或者不存在，即xpr没有红色孩子，则将xpr染红 if ((sr == null || !sr.red) &amp;&amp; (sl == null || !sl.red)) &#123; xpr.red = true; x = xp;//本轮结束，继续向上循环 &#125; else &#123; //否则的话，就需要进一步调整 if (sr == null || !sr.red) &#123; //若左孩子为红，右孩子不存在或为黑 if (sl != null) //左孩子染黑 sl.red = false; //将xpr染红 xpr.red = true; //右旋 root = rotateRight(root, xpr); //右旋后，xpr指向xp的新右孩子，即上一步中的sl xpr = (xp = x.parent) == null ? null : xp.right; &#125; // 如果兄弟节点 不为null if (xpr != null) &#123; // 如果父节点为空，兄弟节点染为黑色，否则和父亲节点染色相同 xpr.red = (xp == null) ? false : xp.red; // sr 设置为兄弟节点的右孩子，如果sr不为null，将sr设置为黑色，防止出现两个红色相连 if ((sr = xpr.right) != null) sr.red = false; &#125; // 如果父节点不为null if (xp != null) &#123; //将父亲节点设置为黑色 xp.red = false; // 左旋父节点 root = rotateLeft(root, xp); &#125; x = root; &#125; &#125; &#125; //x为其父节点的右孩子，跟上面操作镜像 else &#123; // symmetric if (xpl != null &amp;&amp; xpl.red) &#123; xpl.red = false; xp.red = true; root = rotateRight(root, xp); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl == null) x = xp; else &#123; TreeNode&lt;K,V&gt; sl = xpl.left, sr = xpl.right; if ((sl == null || !sl.red) &amp;&amp; (sr == null || !sr.red)) &#123; xpl.red = true; x = xp; &#125; else &#123; if (sl == null || !sl.red) &#123; if (sr != null) sr.red = false; xpl.red = true; root = rotateLeft(root, xpl); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl != null) &#123; xpl.red = (xp == null) ? false : xp.red; if ((sl = xpl.left) != null) sl.red = false; &#125; if (xp != null) &#123; xp.red = false; root = rotateRight(root, xp); &#125; x = root; &#125; &#125; &#125; &#125;&#125; rotateLeft 红黑树左旋（右旋转同理）123456789101112131415161718192021222324252627282930313233343536373839/** * 红黑树左旋 * @param root 根节点 * @param p 要左旋的节点 * * @return */static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateLeft(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; // pp : p的父亲 // r: p的右孩子 // rl = p 的右孩子的左孩子 -&gt; r的左孩子 TreeNode&lt;K,V&gt; r, pp, rl; // 要 p 以及 p的右孩子 不为null if (p != null &amp;&amp; (r = p.right) != null) &#123; // 将p 的右孩子 设置为 rl 并且 rl不为空 if ((rl = p.right = r.left) != null) // rl的父亲节点设置为p rl.parent = p; // 将 r 的父亲 设置为 p的父亲（此时r 与p为兄弟） // 如果父亲节点为null，则此时 r为跟节点 // 将root 设置为r ，并设置为黑色 if ((pp = r.parent = p.parent) == null) (root = r).red = false; // 如果父节点的左儿子是 p else if (pp.left == p) // 将父节点的 左儿子 设为r pp.left = r; else // 否则将 右儿子 设为r pp.right = r; // 将 r 的左节儿子 设置为p r.left = p; // p 的父亲设置为 r p.parent = r; &#125; // 返回根节点 return root;&#125;]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-单例模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Singleton%2F</url>
    <content type="text"><![CDATA[单例模式多个线程操作不同实例对象。多个线程要操作同一对象，要保证对象的唯一性 单例模式的特点 有一个实例化的过程（只有一次），产生实例化对象 提供返回实例对象的方法 单例模式的分类饿汉式1234567891011121314151617181920public class HungrySingleton &#123; //加载时就产生了实例对象 private static HungrySingleton instance = new HungrySingleton(); private HungrySingleton() &#123; &#125; //返回实例对象 public static HungrySingleton getInstance() &#123; return instance; &#125; public static void main(String[] args) &#123; for(int i = 0 ; i &lt; 10 ; i ++)&#123; new Thread(()-&gt;&#123; HungrySingleton hungerySingleton = HungrySingleton.getInstance(); System.out.println(hungerySingleton); &#125;).start(); &#125; &#125;&#125; 线程安全性：在加载的时候已经被实例化，所以只有这一次，线程安全的。 懒加载：没有延迟加载 性能：长时间不使用，数据一直放在堆中影响性能 懒汉式12345678910111213141516171819public class LazySingleton &#123; private static LazySingleton instance = null; private LazySingleton() &#123; &#125; public static LazySingleton getInstance() &#123; if(instance == null)&#123; instance = new LazySingleton(); &#125; return instance; &#125; public static void main(String[] args) &#123; for(int i = 0 ; i &lt; 10 ; i ++)&#123; new Thread(()-&gt; System.out.println(LazySingleton.getInstance())).start(); &#125; &#125;&#125; 线程安全性：不能保证实例对象的唯一性 懒加载：有延迟加载 性能：使用时才进行加载，性能较好 懒汉式+同步方法将懒汉式的get方法加上synchronized 123456public synchronized static LazySingleton getInstance() &#123; if(instance == null)&#123; instance = new LazySingleton(); &#125; return instance;&#125; 线程安全性：synchronized保证线程安全 懒加载：有延迟加载 性能：多个线程调用该方法时 synchronized 会使线程阻塞，退化到了串行执行 Double-Check-Locking12345678910111213141516171819202122public class DCLSingleton &#123; private static DCLSingleton instance = null; private DCLSingleton() &#123; &#125; public static DCLSingleton getInstance() &#123; if(instance == null)&#123; synchronized (DCLSingleton.class)&#123; if(instance == null)&#123; instance = new DCLSingleton(); &#125; &#125; &#125; return instance; &#125; public static void main(String[] args) &#123; for(int i = 0 ; i &lt; 10 ; i ++)&#123; new Thread(()-&gt; System.out.println(DCLSingleton.getInstance())).start(); &#125; &#125;&#125; 线程安全性：线程安全 懒加载：有延迟加载 性能：性能比较好 缺点：会因为指令重排，引起空指针异常。 Volatile+Double-check添加volatile 避免空指针异常。 1private volatile static DCLSingleton instance = null; Holder声明类的时候，成员变量中不声明实例变量，而放到内部静态类中， 1234567891011public class HolderSingleton &#123; private HolderSingleton() &#123; &#125; //通过内部类实现懒加载，只有调用时才会进行实例化，静态类只能实例一次，保证线程安全 private static class Holder&#123; private static HolderSingleton instance=new HolderSingleton(); &#125; public static HolderSingleton getInstance()&#123; return Holder.instance; &#125;&#125; 枚举12345678910111213141516public class EnumSingleton &#123; private EnumSingleton() &#123; &#125; //延迟加载 private enum EnumHolder&#123; INSTANCE; private EnumSingleton instance=null; EnumHolder() &#123; this.instance = new EnumSingleton(); &#125; &#125; public static EnumSingleton getInstance()&#123; return EnumHolder.INSTANCE.instance; &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-Volatile]]></title>
    <url>%2FJAVA%2FJava-Volatile%2F</url>
    <content type="text"><![CDATA[Volatile关键字java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致的更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁更加方便。如果一个字段被声明成volatile，java线程内存模型确保所有线程看到这个变量的值是一致的。 机器硬件CPU与JMM点击这里查看这篇文章 Volatile关键字的作用volatile作用：让其他线程能够马上感知到某一线程多某个变量的修改 保证可见性:对共享变量的修改，其他的线程马上能感知到 保证有序性:禁止重排序（编译阶段、指令优化阶段）volatile之前的代码不能调整到他的后面，volatile之后的代码不能调整到他的前面 不能保证原子性： 举个例子： 1234567891011121314151617181920212223public class Test &#123; public volatile int inc = 0; public void increase() &#123; inc++; &#125; public static void main(String[] args) &#123; final Test test = new Test(); for(int i=0;i&lt;10;i++)&#123; new Thread()&#123; public void run() &#123; for(int j=0;j&lt;1000;j++) test.increase(); &#125;; &#125;.start(); &#125; while(Thread.activeCount()&gt;1) //保证前面的线程都执行完 Thread.yield(); System.out.println(test.inc); &#125;&#125; 线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。 Volatile实现原理 Java代码 instance = new Singleton();//instance是volatile变量 汇编代码 0x01a3de1d: movb $0x0,0x1104800(%esi);0x01a3de24: lock addl $0x0,(%esp); 有volatile变量修饰的共享变量进行写操作的时候会多第二行汇编代码，通过查IA-32架构软件开发者手册可知，lock前缀的指令在多核处理器下会引发了两件事情。 将当前处理器缓存行的数据会写回到系统内存。 这个写回内存的操作会引起在其他CPU里缓存了该内存地址的数据无效。 Volatile的使用场景 状态标志（开关模式） 12345678910111213public class ShutDowsnDemmo extends Thread&#123; private volatile boolean started=false; @Override public void run() &#123; while(started)&#123; dowork(); &#125; &#125; public void shutdown()&#123; started=false; &#125;&#125; 双重检查锁定（double-checked-locking） 需要利用顺序性 volatile与synchronized的区别 使用上的区别: Volatile只能修饰变量，synchronized只能修饰方法和语句块 对原子性的保证: Volatile不能保证原子性，synchronized可以保证原子性 对可见性的保证: 都可以保证可见性，但实现原理不同;Volatile对变量加了lock，synchronized使用monitorEnter和monitorexit 对有序性的保证: Volatile能保证有序，synchronized可以保证有序性，但是代价（重量级）并发退化到串行 其他: Volatile不会引起阻塞，synchronized引起阻塞]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>并发</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-Synchronized]]></title>
    <url>%2FJAVA%2FJava-Synchronized%2F</url>
    <content type="text"><![CDATA[概念利用锁的机制来实现同步的。锁机制有如下两种特性：互斥性：即在同一时间只允许一个线程持有某个对象锁，通过这种特性来实现多线程中的协调机制，这样在同一时间只有一个线程对需同步的代码块(复合操作)进行访问。互斥性我们也往往称为操作的原子性。可见性：必须确保在锁被释放之前，对共享变量所做的修改，对于随后获得该锁的另一个线程是可见的（即在获得锁时应获得最新共享变量的值），否则另一个线程可能是在本地缓存的某个副本上继续操作从而引起不一致。 用法同步方法同步非静态方法123Public synchronized void methodName()&#123;&#125; 同步静态方法123Public synchronized static void methodName()&#123;&#125; 同步代码块获取对象锁1synchronized(this|object) &#123;&#125; 用来修饰非静态方法，在 Java 中，每个对象都会有一个 monitor 对象，这个对象其实就是 Java 对象的锁，通常会被称为“内置锁”或“对象锁”。类的对象可以有多个，所以每个对象有其独立的对象锁，互不干扰 获取类锁1synchronized(类.class) &#123;&#125; 用来修饰静态方法在 Java 中，针对每个类也有一个锁，可以称为“类锁”，类锁实际上是通过对象锁实现的，即类的 Class 对象锁。每个类只有一个 Class 对象，所以每个类只有一个类锁。 monitorMonitor其实是一种同步工具，也可以说是一种同步机制，它通常被描述为一个对象,也常被翻译为“监视器\管程”,每个对象都会有一个 monitor 某一线程占有一个对象的时候，先看该对象的 monitor 的计数器是不是0，如果是0表示这个对象还没有线程占有，这个时候线程占有这个对象，并且对这个对象的monitor+1；如果不为0，表示这个对象已经被其他线程占有，这个线程等待。当线程释放占有权的时候，monitor-1； 同一线程可以对同一对象进行多次加锁，+1，+1，体现了重入性 原理分析线程堆栈分析(互斥)JconsoleJConsole 是一个内置 Java 性能分析器Mac下位置：/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/bin/jconsol 测试代码： 12345678910111213141516171819public class SynchronizedDemo01 &#123; public void test()&#123; synchronized (SynchronizedDemo01.class)&#123; try &#123; TimeUnit.MINUTES.sleep(2); System.out.println(Thread.currentThread().getName() + "is running"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; SynchronizedDemo01 demo01 = new SynchronizedDemo01(); for (int i = 0 ; i &lt; 5 ; i ++)&#123; new Thread(demo01::test).start(); &#125; &#125;&#125; JVM指令分析JavapJavap -V 反编译 对代码块加锁以上是代码块的加锁monitorenter和monitorExit配合使用 对方法加锁通过ACC_SYNCHRONIZED实现 Java虚拟机对synchronized的优化对象头一个对象实例包含：对象头、实例变量、填充数据对象头结构：Mark Word 结构： 偏向锁 作用：偏向锁是为了消除无竞争情况下的同步原语，进一步提升程序性能。 与轻量级锁的区别：轻量级锁是在无竞争的情况下使用CAS操作来代替互斥量的使用，从而实现同步；而偏向锁是在无竞争的情况下完全取消同步。 与轻量级锁的相同点：它们都是乐观锁，都认为同步期间不会有其他线程竞争锁。 原理：当线程请求到锁对象后，将锁对象的状态标志位改为01，即偏向模式。然后使用CAS操作将线程的ID记录在锁对象的Mark Word中。以后该线程可以直接进入同步块，连CAS操作都不需要。但是，一旦有第二条线程需要竞争锁，那么偏向模式立即结束，进入轻量级锁的状态。 优点：偏向锁可以提高有同步但没有竞争的程序性能。但是如果锁对象时常被多条线程竞争，那偏向锁就是多余的。 偏向锁可以通过虚拟机的参数来控制它是否开启。 轻量级锁 本质：使用CAS取代互斥同步 背景：『轻量级锁』是相对于『重量级锁』而言的，而重量级锁就是传统的锁 轻量级锁与重量级锁的比较： 重量级锁是一种悲观锁，它认为总是有多条线程要竞争锁，所以它每次处理共享数据时，不管当前系统中是否真的有线程在竞争锁，它都会使用互斥同步来保证线程的安全； 而轻量级锁是一种乐观锁，它认为锁存在竞争的概率比较小，所以它不使用互斥同步，而是使用CAS操作来获得锁，这样能减少互斥同步所使用的『互斥量』带来的性能开销。 前提：轻量级锁比重量级锁性能更高的前提是，在轻量级锁被占用的整个同步周期内，不存在其他线程的竞争。若在该过程中一旦有其他线程竞争，那么就会膨胀成重量级锁，从而除了使用互斥量以外，还额外发生了CAS操作，因此更慢！ 实现原理： Mark Word中有个标志位用来表示当前对象所处的状态。 当线程请求锁时，若该锁对象的Mark Word中标志位为01（未锁定状态），则在该线程的栈帧中创建一块名为『锁记录』的空间，然后将锁对象的Mark Word拷贝至该空间；最后通过CAS操作将锁对象的Mark Word指向该锁记录； 若CAS操作成功，则轻量级锁的上锁过程成功； 若CAS操作失败，再判断当前线程是否已经持有了该轻量级锁；若已经持有，则直接进入同步块；若尚未持有，则表示该锁已经被其他线程占用，此时轻量级锁就要膨胀成重量级锁。 重量级锁 重量级锁是依赖对象内部的monitor锁来实现的，而monitor又依赖操作系统的MutexLock(互斥锁)来实现，所以重量级锁又称互斥锁，也称为阻塞同步、悲观锁 当系统检查到锁是重量级锁之后，会把等待想要获得锁的线程进行阻塞，被阻塞的线程不会消耗cup。但是阻塞或者唤醒一个线程时，都需要操作系统来帮忙，这就需要从用户态转换到内核态，线程开销很大。 自旋锁 背景：互斥同步对性能最大的影响是阻塞，挂起和恢复线程都需要转入内核态中完成；并且通常情况下，共享数据的锁定状态只持续很短的一段时间，为了这很短的一段时间进行上下文切换并不值得。 原理：当一条线程需要请求一把已经被占用的锁时，并不会进入阻塞状态，而是继续持有CPU执行权等待一段时间，该过程称为『自旋』。 优点：由于自旋等待锁的过程线程并不会引起上下文切换，因此比较高效； 缺点：自旋等待过程线程一直占用CPU执行权但不处理任何任务，因此若该过程过长，那就会造成CPU资源的浪费。 自适应自旋：自适应自旋可以根据以往自旋等待时间的经验，计算出一个较为合理的本次自旋等待时间。 锁消除编译器会清除一些使用了同步，但同步块中没有涉及共享数据的锁，从而减少多余的同步。 锁粗化若有一系列操作，反复地对同一把锁进行上锁和解锁操作，编译器会扩大这部分代码的同步块的边界，从而只使用一次上锁和解锁操作。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>并发</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea-Mac激活]]></title>
    <url>%2FIdea%2FIdea-Activation%2F</url>
    <content type="text"><![CDATA[首先下载jar包：百度网盘链接 密码:b8ye 将其放到合适的文件夹 进入idea（首次进入可以选择免费30天，激活码失效后进入免费30分钟）如果没有项目随便建个项目，点击菜单栏 Help -&gt; Edit Custom VM Options 注意：切记一定要通过 IDEA 来修改 .vmoptions 文件，不要手动直接去修改，现在 IDEA 针对反破解已经越来越严格 在末尾添加路径：-javaagent:/Users/XXXX/XXXX/jetbrainsCrack.jar 注意：补丁全路径中不要包含中文，否则，可能导致破解失败！ 重启idea！！！一定要重启 重启完成后，开始填入激活码，点击菜单栏 Help -&gt; Register: 1A82DEE284F-eyJsaWNlbnNlSWQiOiJBODJERUUyODRGIiwibGljZW5zZWVOYW1lIjoiaHR0cHM6Ly96aGlsZS5pbyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiJVbmxpbWl0ZWQgbGljZW5zZSB0aWxsIGVuZCBvZiB0aGUgY2VudHVyeS4iLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlMwIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiV1MiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSRCIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJDIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREMiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQiIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJNIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiRE0iLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJBQyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRQTiIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkdPIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUFMiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJDTCIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlBDIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlNVIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In1dLCJoYXNoIjoiODkwNzA3MC8wIiwiZ3JhY2VQZXJpb2REYXlzIjowLCJhdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlLCJpc0F1dG9Qcm9sb25nYXRlZCI6ZmFsc2V9-5epo90Xs7KIIBb8ckoxnB/AZQ8Ev7rFrNqwFhBAsQYsQyhvqf1FcYdmlecFWJBHSWZU9b41kvsN4bwAHT5PiznOTmfvGv1MuOzMO0VOXZlc+edepemgpt+t3GUHvfGtzWFYeKeyCk+CLA9BqUzHRTgl2uBoIMNqh5izlDmejIwUHLl39QOyzHiTYNehnVN7GW5+QUeimTr/koVUgK8xofu59Tv8rcdiwIXwTo71LcU2z2P+T3R81fwKkt34evy7kRch4NIQUQUno//Pl3V0rInm3B2oFq9YBygPUdBUbdH/KHROyohZRD8SaZJO6kUT0BNvtDPKF4mCT1saWM38jkw==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g== 13AGXEJXFK9-eyJsaWNlbnNlSWQiOiIzQUdYRUpYRks5IiwibGljZW5zZWVOYW1lIjoiaHR0cHM6Ly96aGlsZS5pbyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiIiLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkFDIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRQTiIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJQUyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJHTyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJETSIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJDTCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSUzAiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUkMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUkQiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUEMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUk0iLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiV1MiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREIiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlNVIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9XSwiaGFzaCI6IjEyNzk2ODc3LzAiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-WGTHs6XpDhr+uumvbwQPOdlxWnQwgnGaL4eRnlpGKApEEkJyYvNEuPWBSrQkPmVpim/8Sab6HV04Dw3IzkJT0yTc29sPEXBf69+7y6Jv718FaJu4MWfsAk/ZGtNIUOczUQ0iGKKnSSsfQ/3UoMv0q/yJcfvj+me5Zd/gfaisCCMUaGjB/lWIPpEPzblDtVJbRexB1MALrLCEoDv3ujcPAZ7xWb54DiZwjYhQvQ+CvpNNF2jeTku7lbm5v+BoDsdeRq7YBt9ANLUKPr2DahcaZ4gctpHZXhG96IyKx232jYq9jQrFDbQMtVr3E+GsCekMEWSD//dLT+HuZdc1sAIYrw==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g== 1KNBB2QUUR1-eyJsaWNlbnNlSWQiOiJLTkJCMlFVVVIxIiwibGljZW5zZWVOYW1lIjoiZ2hib2tlIiwiYXNzaWduZWVOYW1lIjoiIiwiYXNzaWduZWVFbWFpbCI6IiIsImxpY2Vuc2VSZXN0cmljdGlvbiI6IiIsImNoZWNrQ29uY3VycmVudFVzZSI6ZmFsc2UsInByb2R1Y3RzIjpbeyJjb2RlIjoiSUkiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiQUMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiRFBOIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlBTIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkdPIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRNIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkNMIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJTMCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSRCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJQQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSTSIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJXUyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQiIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSU1UiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In1dLCJoYXNoIjoiMTI3OTY4NzcvMCIsImdyYWNlUGVyaW9kRGF5cyI6NywiYXV0b1Byb2xvbmdhdGVkIjpmYWxzZSwiaXNBdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlfQ==-1iV7BA/baNqv0Q5yUnAphUmh66QhkDRX+qPL09ICuEicBqiPOBxmVLLCVUpkxhrNyfmOtat2LcHwcX/NHkYXdoW+6aS0S388xe1PV2oodiPBhFlEaOac42UQLgP4EidfGQSvKwC9tR1zL5b2CJPQKZ7iiHh/iKBQxP6OBMUP1T7j3Fe1rlxfYPc92HRZf6cO+C0+buJP5ERZkyIn5ZrVM4TEnWrRHbpL8SVNq4yqfc+NwoRzRSNC++81VDS3AXv9c91YeZJz6JXO7AokIk54wltr42FLNuKbozvB/HCxV9PA5vIiM+kZY1K0w5ytgxEYKqA87adA7R5xL/crpaMxHQ==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g== 或者随便在网上找个激活码 点击激活，可以看到激活日期到2089年]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-内存模型]]></title>
    <url>%2FJAVA%2FJava-MemoryModel%2F</url>
    <content type="text"><![CDATA[JVM内存模型点击这里查看这篇文章 Java内存模型Java内存模型(Java Memory Model，简称JMM)，本身是种抽象的概念，并不是像硬件架构一样真实存在的；它描述的是一组规则或规范，通过这组规范定义了程序中各个变量(包括实例字段、静态字段和构成数组对象的元素)的访问方式。 主内存：共享的信息 工作内存：私有信息，基本数据类型，直接分配到工作内存，引用的地址存放在工作内存，引用的对象存放在堆中 工作方式： 线程修改私有数据，直接在工作空间修改 线程修改共享数据，把数据复制到工作空间中去，在工作空间中修改，修改完成以后，刷新内存中的数据 硬件架构 多CPU：一个现代计算机通常由两个或者多个CPU。其中一些CPU还有多核。从这一点可以看出，在一个有两个或者多个CPU的现代计算机上同时运行多个线程是可能的。每个CPU在某一时刻运行一个线程是没有问题的。这意味着，如果你的Java程序是多线程的，在你的Java程序中每个CPU上一个线程可能同时（并发）执行。 CPU寄存器：每个CPU都包含一系列的寄存器，它们是CPU内内存的基础。CPU在寄存器上执行操作的速度远大于在主存上执行的速度。这是因为CPU访问寄存器的速度远大于主存。 高速缓存cache：由于计算机的存储设备与处理器的运算速度之间有着几个数量级的差距，所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存（Cache）来作为内存与处理器之间的缓冲：将运算需要使用到的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了。CPU访问缓存层的速度快于访问主存的速度，但通常比访问内部寄存器的速度还要慢一点。每个CPU可能有一个CPU缓存层，一些CPU还有多层缓存。在某一时刻，一个或者多个缓存行（cache lines）可能被读到缓存，一个或者多个缓存行可能再被刷新回主存。 内存：一个计算机还包含一个主存。所有的CPU都可以访问主存。主存通常比CPU中的缓存大得多。 运作原理：通常情况下，当一个CPU需要读取主存时，它会将主存的部分读到CPU缓存中。它甚至可能将缓存中的部分内容读到它的内部寄存器中，然后在寄存器中执行操作。当CPU需要将结果写回到主存中去时，它会将内部寄存器的值刷新到缓存中，然后在某个时间点将值刷新回主存。 解决方案： 总线加锁：所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。 缓存上的一致性协议(MESI) 缓存一致性协议(MESI)多核CPU硬件架构厂商，设计之初就预测到多线程操作数据不一致的问题，因此出现了——缓存一致性协议。 不同的CPU硬件生产厂商，具体的实现不一样。Intel的MESI协议最出名。MESI协议文档：https://en.wikipedia.org/wiki/MESI_protocol 在MESI协议中，每个Cache line有4个状态，可用2个bit表示，它们分别是： M(Modified): 这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。 E(Exclusive): 这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中。 S(Shared): 这行数据有效，数据和内存中的数据一致，数据存在于很多Cache中。 I(Invalid): 这行数据无效。 E状态示例如下：只有Core 0访问变量x，它的Cache line状态为E(Exclusive)。 S状态示例如下：3个Core都访问变量x，它们对应的Cache line为S(Shared)状态。 M状态和I状态示例如下：Core 0修改了x的值之后，这个Cache line变成了M(Modified)状态，其他Core对应的Cache line变成了I(Invalid)状态 有了MESI，为什么还需要JMM？既然有了MESI协议，是不是就不需要volatile的可见性语义了？当然不是，还有以下问题： 并不是所有的硬件架构都提供了相同的一致性保证，不同的硬件厂商实现不同，JVM需要volatile统一语义。 可见性问题不仅仅局限于CPU缓存内，JVM自己维护的内存模型(JMM)中也有可见性问题。使用volatile做标记，可以解决JVM层面的可见性问题。 Java线程与硬件处理器Java线程的实现是基于一对一的线程模型，实际上就是通过语言级别层面程序去间接调用系统内核的线程模型，即我们在使用Java线程时，Java虚拟机内部是转而调用当前操作系统的内核线程来完成当前任务。如图所示，每个线程最终都会映射到CPU中进行处理，如果CPU存在多核，那么一个CPU将可以并行执行多个线程任务。 Java内存模型与硬件内存架构的关系多线程的执行最终都会映射到硬件处理器上进行执行，但Java内存模型和硬件内存架构并不完全一致。对于硬件内存来说只有寄存器、缓存内存、主内存的概念，并没有工作内存(线程私有数据区域)和主内存(堆内存)之分，也就是说Java内存模型对内存的划分对硬件内存并没有任何影响,不管是工作内存的数据还是主内存的数据，对于计算机硬件来说都会存储在计算机主内存中，当然也有可能存储到CPU缓存或者寄存器中，因此总体上来说，Java内存模型和计算机硬件内存架构是一个相互交叉的关系，是一种抽象概念划分与真实物理硬件的交叉。 Java内存模型的必要性如下图，主内存中存在一个共享变量x，现在有A和B两线程分别对该变量x=1进行操作,A线程想要修改x的值为2，而B线程却想要读取x的值,那么B线程读取到到是1还是2呢？答案：都可能，这是不确定的，这也就是所谓的线程安全问题。为了解决类似上述的问题，JVM定义了一组规则，通过这组规则来决定一个线程对共享变量的写入何时对另一个线程可见。 JMM对三个特征的保证原子性操作不可分割 X=10 如果是私有数据具有原子性，如果是共享数据没原子性（需要先把10读到共享空间再把10写入x） Y=x 没有原子性 把数据X读到工作空间（原子性） 把X的值写到Y（原子性） I++ 没有原子性 读i到工作空间 +1 刷新结果到内存 多个原子性的操作合并到一起没有原子性,但是可以通过Synchronized和JUC中Lock的lock来保证原子性。 可见性线程只能操作自己工作空间中的数据，当一个线程修改了某个共享变量的值，其他线程是否能够马上得知这个修改的值。 Volatile:在JMM模型上实现MESI协议 Synchronized:加锁 JUC Lock的lock 有序性有序性是指对于单线程的执行代码，我们总是认为代码的执行是按顺序依次执行的，对于单线程而言确实如此，但对于多线程环境，则可能出现乱序现象，因为程序编译成机器码指令后可能会出现指令重排现象，重排后的指令与原指令的顺序未必一致。 Volatile Synchronized Happens-before原则 程序次序原则，即在一个线程内必须保证语义串行性，也就是说按照代码顺序执行。 锁定原则：后一次加锁必须等前一次解锁 Volatile原则：volatile变量的写，先发生于读，这保证了volatile变量的可见性，简单的理解就是，volatile变量在每次被线程访问时，都强迫从主内存中读该变量的值，而当该变量发生变化时，又会强迫将最新的值刷新到主内存，任何时刻，不同的线程总是能够看到该变量的最新值。 传递原则：A先于B ，B先于C 那么A必然先于C 线程启动规则：如果线程A在执行线程B的start方法之前修改了共享变量的值，那么当线程B执行start方法时，线程A对共享变量的修改对线程B可见 线程终止规则：假设在线程B终止之前，修改了共享变量，线程A从线程B的join方法成功返回后，线程B对共享变量的修改将对线程A可见。 线程中断规则：对线程 interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测线程是否中断。 对象终结规则：对象的构造函数执行，结束先于finalize()方法]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>并发</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机]]></title>
    <url>%2FJVM%2FJVM-Fundamentals%2F</url>
    <content type="text"><![CDATA[Java虚拟机(java virtual machine，JVM)，一种能够运行java字节码的虚拟机。作为一种编程语言的虚拟机，实际上不只是专用于Java语言，只要生成的编译文件匹配JVM对加载编译文件格式要求，任何语言都可以由JVM编译运行。 比如kotlin、scala等。 JVM基本结构JVM由三个主要的子系统构成 类加载子系统 运行时数据区(内存结构) 执行引擎 类加载机制类的生命周期 加载：将.class文件从磁盘读到内存 通过类的全限定名(com.xxx.xxx)+类加载器确定唯一的类，来获取定义此类的二进制字节流 将这个类字节流代表的静态存储结构转为方法区的运行时数据结构 在堆中生成一个代表此类的java.lang.Class对象，作为访问方法区这些数据结构的入口。 连接 验证：验证字节码文件的正确性 文件格式验证：基于字节流验证，验证字节流是否符合Class文件格式的规范，并且能被当前虚拟机处理。 元数据验证：基于方法区的存储结构验证，对字节码描述信息进行语义验证。 字节码验证：基于方法区的存储结构验证，进行数据流和控制流的验证。 符号引用验证：基于方法区的存储结构验证，发生在解析中，是否可以将符号引用成功解析为直接引用。 准备：给类的静态变量分配内存，并赋予默认值（不包括实例变量） public static int value = 123; //此时在准备阶段过后的初始值为0而不是123，在初始化过程才会被赋值为123 public static final int value = 123;//value的值在准备阶段过后就是123。 解析：类装载器装入类所引用的其它所有类 初始化：为类的静态变量赋予正确的初始值，上述的准备阶段为静态变量赋予的是虚拟机默认的初始值，此处赋予的才是程序编写者为变量分配的真正的初始值，执行静态代码块 使用 卸载 类加载器的种类总体上分为两种：启动类加载器（C++实现） 和 其他类加载器（JAVA实现） 启动类加载器(Bootstrap ClassLoader)负责加载JRE的核心类库，如JRE目标下的rt.jar，charsets.jar等 扩展类加载器(Extension ClassLoader)负责加载JRE扩展目录ext中jar类包 系统类加载器(Application ClassLoader)负责加载ClassPath路径下的类包 用户自定义加载器(User ClassLoader)负责加载用户自定义路径下的类包 类加载机制全盘负责委托机制当一个ClassLoader加载一个类的时候，除非显示的使用另一个ClassLoader，该类所依赖和引用的类也由这个 ClassLoader载入 双亲委派机制指先委托父类加载器寻找目标类，在找不到的情况下，在自己的路径中查找并载入目标类 当有类需要加载，系统类加载器先判断有没有父类，有交给扩展类加载器加载 扩展类加载器判断有没有父类，有交给启动类加载器 启动类加载器没有父类，去实际加载该类，该类不是JRE包下的类，交给子类扩展类加载器去加载 扩展类加载器去加载该类，发现该类不是ext中的包，交给系统类加载器加载 系统类加载器加载，发现是classPath路径下的包，进行加载。 双亲委派模式的优势 沙箱安全机制:比如自己写的String.class类不会被加载，这样可以防止核心库被随意篡改 避免类的重复加载:当父ClassLoader已经加载了该类的时候，就不需要子ClassLoader再加载一次 为什么要打破双亲委派模式例如：tomcatTomcat是个web容器,可能需要部署两个应用程序，不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求同一个类库在同一个服务器只有一份，因此要保证每个应用程序的类库都是独立的，保证相互隔离。如果使用默认的类加载器机制，那么是无法加载两个相同类库的不同版本的，默认的类加载器是不管你是什么版本的，只在乎你的全限定类名，并且只有一份。 如何打破双亲委派模式 继承ClassLoader 重写findClass()方法 重写loadClass()方法 运行时数据区(内存结构) 虚拟机栈java虚拟机栈是线程私有的，每个方法执行都会创建一个栈帧，栈帧包含局部变量表、操作数栈、动态连接、方法出口等。 栈与栈帧每一个方法的执行到执行完成，对应着一个栈帧在虚拟机中从入栈到出栈的过程。java虚拟机栈栈顶的栈帧就是当前执行方法的栈帧。PC寄存器会指向该地址。当这个方法调用其他方法的时候久会创建一个新的栈帧，这个新的栈帧会被方法Java虚拟机栈的栈顶，变为当前的活动栈，在当前只有当前活动栈的本地变量才能被使用，当这个栈帧所有指令都完成的时候，这个栈帧被移除，之前的栈帧变为活动栈，前面移除栈帧的返回值变为这个栈帧的一个操作数。 栈帧栈帧包含局部变量表、操作数栈、动态连接、方法返回地址 局部变量表 局部变量表是变量值的存储空间，用于存放方法参数和方法内部定义的局部变量。在java编译成class文件的时候，就在方法的Code属性的max_locals数据项中确定该方法需要分配的最大局部变量表的容量。 局部变量表的容量以变量槽（Slot）为最小单位，32位虚拟机中一个Slot可以存放32位（4 字节）以内的数据类型（ boolean、byte、char、short、int、float、reference和returnAddress八种） 对于64位长度的数据类型（long，double），虚拟机会以高位对齐方式为其分配两个连续的Slot空间，也就是相当于把一次long和double数据类型读写分割成为两次32位读写。 reference类型虚拟机规范没有明确说明它的长度，但一般来说，虚拟机实现至少都应当能从此引用中直接或者间接地查找到对象在Java堆中的起始地址索引和方法区中的对象类型数据。 Slot是可以重用的，当Slot中的变量超出了作用域，那么下一次分配Slot的时候，将会覆盖原来的数据。Slot对对象的引用会影响GC（要是被引用，将不会被回收）。 系统不会为局部变量赋予初始值（实例变量和类变量都会被赋予初始值）。也就是说不存在类变量那样的准备阶段。 操作数栈 操作数栈和局部变量表一样，在编译时期就已经确定了该方法所需要分配的局部变量表的最大容量。 操作数栈的每一个元素可用是任意的Java数据类型，包括long和double。32位数据类型所占的栈容量为1，64位数据类型占用的栈容量为2。 当一个方法刚刚开始执行的时候，这个方法的操作数栈是空的，在方法执行的过程中，会有各种字节码指令往操作数栈中写入和提取内容，也就是出栈 / 入栈操作（例如：在做算术运算的时候是通过操作数栈来进行的，又或者在调用其它方法的时候是通过操作数栈来进行参数传递的）。 动态连接直接引用：有具体引用地址的指针，被引用的类、方法或者变量已经被加载到内存中符号引用：即用用字符串符号的形式来表示引用，其实被引用的类、方法或者变量还没有被加载到内存中。举个例子： 123456789/*** 符号引用*/String str = "abc";System.out.println("str=" + str);/*** 直接引用*/System.out.println("str=" + "abc"); 动态链接：在程序运行过程中，由符号引用转化为直接引用。静态链接：在类加载过程中，由符号引用转化为直接引用。 方法返回地址当一个方法开始执行时，可能有两种方式退出该方法： 正常完成出口 正常完成出口是指方法正常完成并退出，没有抛出任何异常(包括Java虚拟机异常以及执行时通过throw语句显示抛出的异常)。如果当前方法正常完成，则根据当前方法返回的字节码指令，这时有可能会有返回值传递给方法调用者(调用它的方法)，或者无返回值。具体是否有返回值以及返回值的数据类型将根据该方法返回的字节码指令确定。 异常完成出口 异常完成出口是指方法执行过程中遇到异常，并且这个异常在方法体内部没有得到处理，导致方法退出。 无论方法采用何种方式退出，在方法退出后都需要返回到方法被调用的位置，程序才能继续执行，方法返回时可能需要在当前栈帧中保存一些信息，用来帮他恢复它的上层方法执行状态。方法退出过程实际上就等同于把当前栈帧出栈，因此退出可以执行的操作有：恢复上层方法的局部变量表和操作数栈，把返回值(如果有的话)压如调用者的操作数栈中，调整PC计数器的值以指向方法调用指令后的下一条指令。一般来说，方法正常退出时，调用者的PC计数值可以作为返回地址，栈帧中可能保存此计数值。而方法异常退出时，返回地址是通过异常处理器表确定的，栈帧中一般不会保存此部分信息。 123456789101112public class Demo &#123; public int math()&#123; int a = 1; int b = 2; int c = (a + b)*10; return c; &#125; public static void main(String[] args) &#123; Demo demo = new Demo(); demo.math(); &#125;&#125; 当执行👆Demo的math方法时，主线程内存会如何操作第一步，现将1放入操作数栈第二步，将1放入局部变量表中第一个槽里第三步，第四步同上，最终将2放入局部变量表中第二个槽里第五步，将1复制一份放入操作数栈的栈顶第六步，将2复制一份放入操作数栈的栈顶第七步，将2，1弹出操作数栈交给cpu去运算得到3，放到操作数栈的栈顶第八步，从常量池（-128～127）里拿到10，放入操作数栈顶第九步，弹出10，3交给cpu去运算得到30，放到操作数栈的栈顶（jvm1.6开始进行了指令优化，第8、9步合并成了一步操作）第十步，将30放到局部变量表中第3个槽里。第十一步，将30复制一份放入操作数栈第栈顶。第十二步，将30弹出操作数栈，通过返回地址返回。 程序计数器就是一个指针，指向方法区中的方法字节码(用来存储指向下一跳指令的地址，也就是当前线程将要执行的指令代码)，由执行引擎读取下一条指令，是一个非常小的内存空间，几乎可以忽略不计，用来保证线程间切换后正确执行。 本地方法栈和栈作用很相似，区别不过是Java栈为JVM执行Java方法服务，而本地方法栈为JVM执行native方法服务。登记native方法，在Execution Engine执行时加载本地方法库。 方法区（永久代/持久代，元空间）类的所有字段和方法字节码，以及一些特殊方法如构造函数，接口代码也在这里定义。简单来说，所有定义的方法的信息都保存在该区域，静态变量+常量+类信息(构造方法/接口定义)+运行时常量池都存在方法区中，虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap(非堆)，目的应该是为了和Java的堆区分开(jdk1.8以前hotspot虚拟机叫永久代、持久代，jdk1.8时叫元空间) 永久代和元空间区别在于元数据区不在虚拟机当中，而是用的本地内存，永久代在虚拟机当中，永久代逻辑结构上也属于堆，但是物理上不属于。 为什么移除了永久代?参考官方解释http://openjdk.java.net/jeps/122大概意思是移除永久代是为融合HotSpot与 JRockit而做出的努力，因为JRockit没有永久代，不需要配置永久代。 堆虚拟机启动时自动分配创建，用于存放对象的实例，几乎所有对象都在堆上分配内存，当对象无法在该空间申请到内存是将抛出OutOfMemoryError异常。同时也是垃圾收集器管理的主要区域。 新生代(Young Generation)类出生、成长、消亡的区域，一个类在这里产生，应用，最后被垃圾回收器收集，结束生命。新生代分为两部分:伊甸区(Eden space)和幸存者区(Survivor space)，所有的类都是在伊甸区被new出来的。幸存区(Survivor space):分为From和To区,TO区永远保持空。当Eden区的空间用完是，程序又需要创建对象，JVM的垃圾回收器将Eden区进行垃圾回收(Minor GC)，将Eden区中的不再被其它对象应用的对象进行销毁。然后将Eden区中剩余的对象移到From Survivor区。若From Survivor区也满了，再对该区进行垃圾回收，然后移动到To Survivor区，From区为空后，将To和From区转换，保证To区为空，并且对象年龄加一。当对象年龄默认加到15（因为对象头只有4个bits是存对象年龄，最大为15）时将剩下的对象移到老年代。 老年代(Old Generation)新生代经过多次GC仍然存货的对象移动到老年区。若老年代也满了，这时候将发生Major GC(也可以叫Full GC)， 进行老年区的内存清理。若老年区执行了Full GC之后发现依然无法进行对象的保存，就会抛出 OOM(OutOfMemoryError)异常. GC算法和收集器几种常见GC：MinorGC/YoungGC 新生代OldGC CMS特有FullGC/MajorGC 回收所有MixedGC（FullGC+YoungGC） G1特有 如何判断对象可以被回收堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡(即不能再被任何途径使用的对象) 引用计数法给对象添加一个引用计数器，每当有一个地方引用，计数器就加1。当引用失效，计数器就减1。任何时候计数器为0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中没有选择这个算法来管理内存，最主要的原因是它很难解决对象之前相互循环引用的问题。所谓对象之间的相互引用问题，通过下面代码所示:除了对象a和b相互引用着对方之外，这两个对象之间再无任何引用。但是它们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数器法无法通知GC回收器回收它们。 1234567891011public class CounterGC&#123; Object instance = null; public static void main(String[] args)&#123; CounterGC a = new CounterGC(); CounterGC b = new CounterGC(); a.instance = b; b.instance = a; a = null; b = null; &#125;&#125; 可达性分析算法这个算法的基本思想就是通过一系列的称为”GC Roots”的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连的话，则证明此对象时不可用的。 GC Roots根节点:类加载器、Thread、虚拟机栈的局部变量表、static成员、常量引用、本地方法栈的变量等等. 如何判断一个常量是废弃常量运行时常量池主要回收的是废弃的常量。那么，我们怎么判断一个常量时废弃常量呢?假如在常量池中存在字符串”abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量”abc”就是废弃常量，如果这时发生内存回收的话而且有必要的话（内存不够用时才会发生回收），”abc”会被系统清理出常量池。 如何判断一个类是无用的类需要满足以下三个条件: 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。虚拟机可以对满足上述3个条件的无用类进行回收，这里仅仅是”可以“，而并不是和对象一样不适用了就必然会被回收。 垃圾回收算法 标记-清除算法它是最基础的收集算法，这个算法分为两个阶段，标记和清除。首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它有两个不足的地方: 效率问题，标记和清除两个过程的效率都不高; 空间问题，标记清除后会产生大量不连续的碎片; 复制算法为了解决效率问题，复制算法出现了。它可以把内存分为大小相同的两块，每次只使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块区，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收 标记-整理算法根据老年代的特点提出的一种标记算法，标记过程和“标记-清除”算法一样，但是后续步骤不是直接对可回收对象进行回收，而是让所有存活的对象向一段移动，然后直接清理掉边界以外的内存 分代收集算法现在的商用虚拟机的垃圾收集器基本都采用”分代收集”算法，这种算法就是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。在新生代中，每次收集都有大量对象死去，所以可以选择复制算法，只要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率时比较高的，而且没有额外的空间对它进行分配担保，就必须选择“标记-清除”或 者“标记-整理”算法进行垃圾收集。 垃圾收集器 Serial收集器Serial(串行)收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的单线程的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程( “Stop The World” )，直到它收集结束。新生代采用复制算法，老年代采用标记-整理算法。虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短(仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续)。但是Serial收集器有没有优于其他垃圾收集器的地方呢?当然有，它简单而高效(与其他收集器的单线程相比)。 Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是个不错的选择。 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为(控制参数、收集算法、回收策略等等)和Serial收集器完全一样。新生代采用复制算法，老年代采用标记-整理算法。它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器(真正意义上的并发收集器，后面会介绍到)配合工作。 Parallel Scavenge收集器(JDK1.8)Parallel Scavenge 收集器类似于ParNew收集器。Parallel Scavenge收集器关注点是吞吐量(高效率的利用CPU)。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间(提高用户体验)。 Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。新生代采用复制算法，老年代采用标记-整理算法。 Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器，采用标记-整理算法。它主要有两大用途:一种用途是在JDK1.5以及以前的版本 中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。 Parallel Old收集器Parallel Scavenge收集器的老年代版本。使用多线程和标记-整理算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。 CMS收集器并行和并发概念补充: 并行(Parallel) :指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发(Concurrent):指用户线程与垃圾收集线程同时执行(但不一定是并行，可能会交替执行)，用户程序 在继续运行，而垃圾收集器运行在另一个CPU上。 CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用。CMS(Concurrent Mark Sweep)收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程(基本上)同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种标记-清除算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤: 初始标记(CMS initial mark): 暂停所有的其他线程，并记录下直接与root相连的对象，速度很快 并发标记(CMS concurrent mark): 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记(CMS remark): 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶 段时间短 并发清除(CMS concurrent sweep): 开启用户线程，同时GC线程开始对为标记的区域做清扫。CMS主要优点:并发收集、低停顿。但是它有下面三个明显的缺点: 对CPU资源敏感; 无法处理浮动垃圾; 它使用的回收算法-标记-清除算法会导致收集结束时会有大量空间碎片产生。 G1收集器G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC停顿时间要求的同时,还具备高吞吐量性能特征.被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备一下特点: 并行与并发:G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU(CPU或者CPU核心)来缩短Stop- The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行 分代收集:虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。 空间整合:与CMS的“标记–清理”算法不同，G1从整体来看是基于标记整理算法实现的收集器;从局部上来看是基于标记复制算法实现的 可预测的停顿:这是G1相对于CMS的另一个大优势，降低停顿时间是G1 和 CMS 共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内 G1收集器的运作大致分为以下几个步骤: 初始标记 并发标记 最终标记 筛选回收 G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名 字Garbage-First的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率(把内存化整为零)。 怎么选择垃圾收集器?（尽量由JVM自己选择） 优先调整堆的大小让服务器自己来选择 如果内存小于100m，使用串行收集器 如果是单核，并且没有停顿时间的要求，串行或JVM自己选择 如果允许停顿时间超过1秒，选择并行或者JVM自己选 如果响应时间最重要，并且不能超过1秒，使用并发收集器官方推荐ZGC(java最新版本垃圾收器器，可预测的停顿最低2ms)，性能高。 JDK性能调优监控工具虚拟机参数分析网站：https://www.perfma.com/product/opts jps显示当前系统的java进程情况 123456gdeMacBook-Pro:~ g$ jps94673 AppServiceApplication5499555011 RemoteMavenServer3694696 AppServiceApplication96956 Jps (空白的54995是idea) Jinfo查看正在运行的Java程序的扩展参数 查看JVM的参数123gdeMacBook-Pro:~ g$ jinfo -flags 94673VM Flags:-XX:-BytecodeVerificationLocal -XX:-BytecodeVerificationRemote -XX:CICompilerCount=4 -XX:ConcGCThreads=3 -XX:G1ConcRefinementThreads=10 -XX:G1HeapRegionSize=1048576 -XX:GCDrainStackTargetSize=64 -XX:InitialHeapSize=268435456 -XX:+ManagementServer -XX:MarkStackSize=4194304 -XX:MaxHeapSize=4294967296 -XX:MaxNewSize=2576351232 -XX:MinHeapDeltaBytes=1048576 -XX:NonNMethodCodeHeapSize=7549744 -XX:NonProfiledCodeHeapSize=244108496 -XX:ProfiledCodeHeapSize=0 -XX:ReservedCodeCacheSize=251658240 -XX:+SegmentedCodeCache -XX:TieredStopAtLevel=1 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC 查看java系统属性1jinfo -sysprops 94673 Jstatjstat命令可以查看堆内存各部分的使用量，以及加载类的数量。命令格式:jstat [-命令选项] [vmid] [间隔时间/毫秒] [查询次数] 123gdeMacBook-Pro:~ g$ jstat -gc 94673 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT CGC CGCT GCT 0.0 2048.0 0.0 2048.0 96256.0 68608.0 163840.0 132934.6 131280.0 127368.5 14720.0 13850.3 185 1.219 0 0.000 112 0.661 1.880 S0C：第一个幸存区的大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 OC：老年代大小 OU：老年代使用大小 MC：方法区大小 MU：方法区使用大小 CCSC:压缩类空间大小 CCSU:压缩类空间使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 Jmap可以用来查看内存信息 堆的对象统计1gdeMacBook-Pro:~ g$ jmap -histo 94673 &gt; xxx.txt 部分如下 1234567891011121314151617181920212223242526272829303132 num #instances #bytes class name (module)------------------------------------------------------- 1: 380588 39857240 [B (java.base@11.0.4) 2: 71568 8387344 [Ljava.lang.Object; (java.base@11.0.4) 3: 301995 7247880 java.lang.String (java.base@11.0.4) 4: 25762 5316280 [I (java.base@11.0.4) 5: 47736 4200768 java.lang.reflect.Method (java.base@11.0.4) 6: 126275 4040800 java.util.concurrent.ConcurrentHashMap$Node (java.base@11.0.4) 7: 69142 3871952 java.util.LinkedHashMap (java.base@11.0.4) 8: 2483 3656120 [C (java.base@11.0.4) 9: 46566 3352752 io.netty.channel.DefaultChannelHandlerContext 10: 104582 3346624 java.util.HashMap$Node (java.base@11.0.4) 11: 20887 2551232 java.lang.Class (java.base@11.0.4) 12: 32461 2077504 java.util.concurrent.ConcurrentHashMap (java.base@11.0.4) 13: 48062 1922480 java.util.HashMap$KeyIterator (java.base@11.0.4) 14: 17400 1720040 [Ljava.util.HashMap$Node; (java.base@11.0.4) 15: 2184 1432704 io.netty.util.internal.shaded.org.jctools.queues.MpscArrayQueue 16: 35693 1427720 java.util.LinkedHashMap$Entry (java.base@11.0.4) 17: 88742 1419872 java.lang.Object (java.base@11.0.4) 18: 1671 1195280 [Ljava.util.concurrent.ConcurrentHashMap$Node; (java.base@11.0.4) 19: 10270 1150240 sun.nio.ch.SocketChannelImpl (java.base@11.0.4) 20: 10237 1064648 io.netty.channel.socket.nio.NioSocketChannel 21: 42359 1016616 java.util.ArrayList (java.base@11.0.4) 22: 60868 973888 java.lang.Integer (java.base@11.0.4) 23: 20436 817440 io.netty.util.DefaultAttributeMap$DefaultAttribute 24: 10533 758376 org.springframework.core.type.classreading.AnnotationMetadataReadingVisitor 25: 18570 742800 com.meituan.service.mobile.mtthrift.mtrace.MtraceServerTBinaryProtocol$Factory 26: 10239 737208 io.netty.channel.DefaultChannelPipeline$HeadContext 27: 10239 737208 io.netty.channel.DefaultChannelPipeline$TailContext 28: 30914 729768 [Ljava.lang.Class; (java.base@11.0.4) 29: 21894 700608 java.util.concurrent.locks.ReentrantLock$NonfairSync (java.base@11.0.4)------省略---- Num:序号 Instances:实例数量 Bytes:占用空间大小 Class Name:类名 堆信息123gdeMacBook-Pro:~ g$ jmap -heap 94673Error: -heap option usedCannot connect to core dump or remote debug server. Use jhsdb jmap instead jdk9及以上版本使用jmap -heap pid命令查看当前heap使用情况时，发现报错，提示需要使用jhsdb jmap来替代 1gdeMacBook-Pro:~ g$ jhsdb jmap --heap --pid 94673]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-装饰器模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Decorator%2F</url>
    <content type="text"><![CDATA[什么是装饰器模式装饰器模式（Decorator）,动态地给一个对象添加一些额外的职责，就增加功能来说，装饰器模式比生成子类更为灵活；它允许向一个现有的对象添加新的功能，同时又不改变其结构。 类图Component：接口，定义一个抽象接口，真实对象和装饰对象具有相同的接口，以便动态的添加职责。ConcreteComponent：具体的对象。Decorator：装饰类，继承了Component,从外类来扩展Component类的功能，并且持有一个构建引用，进行请求转发。ConcreteDecorator：具体装饰类，用于给实际对象添加职责。 代码实现现在考虑这样一个场景，现在有一个煎饼摊，人们去买煎饼（Pancake）,有些人要加火腿（Ham）的，有些人要加鸡蛋（Egg）的，有些人要加生菜（Lettuce）的，当然土豪可能有啥全给加上^_^。用上述的装饰器模式来进行编码。 定义煎饼接口IPancake 123456789/** * 定义一个煎饼接口 */public interface IPancake &#123; /** * 定义烹饪的操作 */ void cook();&#125; 定义具体的煎饼Pancake 12345678/** * 具体的煎饼对象，可用其他装饰类进行动态扩展。 */public class Pancake implements IPancake&#123; public void cook() &#123; System.out.println("的煎饼"); &#125;&#125; 定义抽象装饰类PancakeDecorator 1234567891011121314151617/** * 实现接口的抽象装饰类，建议设置成abstract. */public abstract class PancakeDecorator implements IPancake &#123; private IPancake pancake; public PancakeDecorator(IPancake pancake) &#123; this.pancake = pancake; &#125; public void cook() &#123; if (this.pancake != null) &#123; pancake.cook(); &#125; &#125;&#125; 具体装饰类EggDecorator 123456789101112131415161718/** * 对煎饼加鸡蛋的装饰类，继承PancakeDecorator，覆盖cook操作 */public class EggDecorator extends PancakeDecorator &#123; public EggDecorator(IPancake pancake) &#123; super(pancake); &#125; /** * 覆盖cook方法，加入自身的实现，并且调用父类的cook方法，也就是构造函数中 * EggDecorator(IPancake pancake)，这里传入的pancake的cook操作 */ @Override public void cook() &#123; System.out.println("加了一个鸡蛋，"); super.cook(); &#125;&#125; 具体装饰类HamDecorator 123456789101112131415161718/** * 对煎饼加火腿的装饰类，继承PancakeDecorator，覆盖cook操作 */public class HamDecorator extends PancakeDecorator &#123; public HamDecorator(IPancake pancake) &#123; super(pancake); &#125; /** * 覆盖cook方法，加入自身的实现，并且调用父类的cook方法，也就是构造函数中 * EggDecorator(IPancake pancake)，这里传入的pancake的cook操作 */ @Override public void cook() &#123; System.out.println("加了一根火腿，"); super.cook(); &#125;&#125; 具体装饰类LettuceDecorator 123456789101112131415161718/** * 对煎饼加生菜的装饰类，继承PancakeDecorator，覆盖cook操作 */public class LettuceDecorator extends PancakeDecorator &#123; public LettuceDecorator(IPancake pancake) &#123; super(pancake); &#125; /** * 覆盖cook方法，加入自身的实现，并且调用父类的cook方法，也就是构造函数中 * EggDecorator(IPancake pancake)，这里传入的pancake的cook操作 */ @Override public void cook() &#123; System.out.println("加了一颗生菜，"); super.cook(); &#125;&#125; 客户端调用以及结果 12345678910111213141516171819/** * 调用客户端 */public class Client &#123; public static void main(String[] args) &#123; System.out.println("=========我是土豪都给我加上==========="); IPancake pancake = new Pancake(); IPancake pancakeWithEgg = new EggDecorator(pancake); IPancake pancakeWithEggAndHam = new HamDecorator(pancakeWithEgg); IPancake panckeWithEggAndHamAndLettuce = new LettuceDecorator(pancakeWithEggAndHam); panckeWithEggAndHamAndLettuce.cook(); System.out.println("==========我是程序猿，加两个鸡蛋补补=============="); IPancake pancake2 = new Pancake(); IPancake pancakeWithEgg2 = new EggDecorator(pancake2); IPancake pancakeWithTwoEgg = new EggDecorator(pancakeWithEgg2); pancakeWithTwoEgg.cook(); &#125;&#125; 输出结果 123456789=========我是土豪都给我加上===========加了一颗生菜，加了一根火腿，加了一个鸡蛋，的煎饼==========我是程序猿，加两个鸡蛋补补==============加了一个鸡蛋，加了一个鸡蛋，的煎饼 四. 总结关于装饰器模式的使用，在我看来主要有一下几点需要注意的: * 抽象装饰器和具体被装饰的对象实现同一个接口 * 抽象装饰器里面要持有接口对象，以便请求传递 * 具体装饰器覆盖抽象装饰器方法并用super进行调用，传递请求 适用场景 扩展一个类的功能。 动态添加功能，动态撤销。 优点 装饰类和被装饰类都只关心自身的核心业务，实现了解耦。 方便动态的扩展功能，且提供了比继承更多的灵活性。 缺点 如果功能扩展过多，势必产生大量的类。 多层装饰比较复杂。 转自：https://brightloong.github.io/]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-观察者模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Observer%2F</url>
    <content type="text"><![CDATA[认识观察者模式我们看看报纸和杂志的订阅是怎么回事: 报社的业务就是出版报纸。 向某家报社订阅报纸，只要他们有新报纸出版，就会给你送来。只要你是他们的订户，你就会一直收到新报纸。 当你不想再看报纸的时候，取消订阅，他们就不会再送新报纸来。 只要报社还在运营，就会一直有人(或单位)向他们订阅报 纸或取消订阅报纸。 观察者模式出版者+订阅者=观察者模式出版者改称为“主题”(Subject)，订阅者改称为“观察者”(Observer)。观察者模式：定义了对象之间的一对多依赖，当一个对象改变状态时，它的所有依赖者都会收到通知并自动更新。 示例建立气象站：该气象站必须建立在WeatherData对象上，由WeatherData对象负责追踪目前的天气状况(温度、湿度、气压)。希望能建立一个应用，有三种布告板，分别显示目前的状况、气象统计及简单的预报。当WeatherObject对象获得最新的测量数据时，三种布告板 必须实时更新。而且，这是一个可以扩展的气象站，Weather-O-Rama气象 站希望公布一组API，好让其他开发人员可以写出自己的气象布告板，并插入此应用中。 12345678public interface Subject &#123; // 注册 public void registerObserver(Observer o); // 移除 public void removeObserver(Observer o); // 通知 public void notifyObservers();&#125; 123public interface Observer &#123; public void update(float temp, float humidity, float pressure);&#125; 1234public interface DisplayElement &#123; // 显示布告板 public void display();&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.ArrayList;public class WeatherData implements Subject&#123; private ArrayList observers; // 温度 private float temperature; // 湿度 private float humidity; // 气压 private float pressure; public WeatherData() &#123; observers = new ArrayList(); &#125; @Override public void registerObserver(Observer o) &#123; observers.add(o); &#125; @Override public void removeObserver(Observer o) &#123; int i = observers.indexOf(o); if (i &gt;= 0) &#123; observers.remove(i); &#125; &#125; @Override public void notifyObservers() &#123; for (int i = 0; i &lt; observers.size(); i++) &#123; Observer observer = (Observer)observers.get(i); observer.update(temperature, humidity, pressure); &#125; &#125; // 当从气象站得到更新观测值时，我们通知观察者。 public void measurementsChanged() &#123; notifyObservers(); &#125; public void setMeasurements(float temperature, float humidity, float pressure) &#123; this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; measurementsChanged(); &#125; //其他方法省略&#125; 布告板 12345678910111213141516171819202122232425public class CurrentConditionsDisplay implements Observer, DisplayElement&#123; private float temperature; private float humidity; private Subject weatherData; public CurrentConditionsDisplay(Subject weatherData) &#123; this.weatherData = weatherData; weatherData.registerObserver(this); &#125; @Override public void display() &#123; System.out.println("Current conditions: " + temperature + "F degrees and " + humidity + "% humidity"); &#125; @Override public void update(float temp, float humidity, float pressure) &#123; this.temperature = temp; this.humidity = humidity; display(); &#125;&#125; 气象站测试类 1234567891011public class WeatherStation &#123; public static void main(String[] args) &#123; WeatherData weatherData = new WeatherData(); CurrentConditionsDisplay currentDisplay = new CurrentConditionsDisplay(weatherData);// StatisticsDisplay statisticsDisplay = new StatisticsDisplay(weatherData);// ForecastDisplay forecastDisplay = new ForecastDisplay(weatherData); weatherData.setMeasurements(80, 65, 30.4f); weatherData.setMeasurements(82, 70, 29.2f); weatherData.setMeasurements(78, 90, 29.2f); &#125;&#125; 运行结果 123Current conditions: 80.0F degrees and 65.0% humidityCurrent conditions: 82.0F degrees and 70.0% humidityCurrent conditions: 78.0F degrees and 90.0% humidity JAVA内置的观察者模式Java API有内置的观察者模式。java.util包(package)内包含最基本的Observer接口与Observable类。 Observer接口 123456package java.util;public interface Observer &#123; void update(Observable o, Object arg);&#125; Observable类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package java.util;public class Observable &#123; private boolean changed = false;// 数据更新标记 private Vector&lt;Observer&gt; obs; // 观察者列表 //构造函数 public Observable() &#123; obs = new Vector&lt;&gt;(); &#125; //添加观察者 public synchronized void addObserver(Observer o) &#123; if (o == null) throw new NullPointerException(); if (!obs.contains(o)) &#123; obs.addElement(o); &#125; &#125; // 删除某一个观察者 public synchronized void deleteObserver(Observer o) &#123; obs.removeElement(o); &#125; // 通知观察者 public void notifyObservers() &#123; notifyObservers(null); &#125; // 如果changed = true 意味着数据被修改，通知每个观察者。 public void notifyObservers(Object arg) &#123; Object[] arrLocal; synchronized (this) &#123; if (!changed) return; arrLocal = obs.toArray(); clearChanged(); &#125; for (int i = arrLocal.length-1; i&gt;=0; i--) ((Observer)arrLocal[i]).update(this, arg); &#125; // 删除所有观察者 public synchronized void deleteObservers() &#123; obs.removeAllElements(); &#125; // 设置changed值 protected synchronized void setChanged() &#123; changed = true; &#125; // 清除changed值 protected synchronized void clearChanged() &#123; changed = false; &#125; // 获取changed值 public synchronized boolean hasChanged() &#123; return changed; &#125; // 返回观察者个数 public synchronized int countObservers() &#123; return obs.size(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Observable;public class WeatherData extends Observable &#123; // 温度 private float temperature; // 湿度 private float humidity; // 气压 private float pressure; public WeatherData() &#123; &#125; // 当从气象站得到更新观测值时，我们通知观察者。 public void measurementsChanged() &#123; setChanged(); notifyObservers(); &#125; public void setMeasurements(float temperature, float humidity, float pressure) &#123; this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; measurementsChanged(); &#125; //使用“拉”的做法 public float getTemperature() &#123; return temperature; &#125; public float getHumidity() &#123; return humidity; &#125; public float getPressure() &#123; return pressure; &#125; //其他方法省略&#125; 12345678910111213141516171819202122232425262728293031import java.util.Observable;import java.util.Observer;public class CurrentConditionsDisplay implements Observer, DisplayElement&#123; private float temperature; private float humidity; Observable observable; public CurrentConditionsDisplay(Observable observable) &#123; this.observable = observable; observable.addObserver(this); &#125; @Override public void display() &#123; System.out.println("Current conditions: " + temperature + "F degrees and " + humidity + "% humidity"); &#125; @Override public void update(Observable o, Object arg) &#123; if (o instanceof WeatherData) &#123; WeatherData weatherData = (WeatherData)o; this.temperature = weatherData.getTemperature(); this.humidity = weatherData.getHumidity(); display(); &#125; &#125;&#125; 测试代码和测试结果同上 java.util.Observable的缺点java.util.Observable的实现 有许多问题，限制了它的使用和复用。 观察者是一个“类”而不是一个“接 口” 你必须设计一个类继承它。如果某类想同时 具有Observable类和另一个超类的行为，就会陷入两难，毕竟Java不支持多重继承。 Observable将关键的方法保护起来 setChanged()方法被保护起来了(被定义成 protected)。这意味着:除非你继承自Observable，否则你无法创建Observable实例并组合到你自己的对象中来。这个设计违反了第二个设计原 则:“多用组合，少用继承”。 要点 观察者模式定义了对象之间一对多的关系。 主题(也就是可观察者)用一个共同的接口来更新观察者。 观察者和可观察者之间用松耦合方式结合(loosecoupl- ing)，可观察者不知道观察者的细节，只知道观察者实现了观察者接口。 使用此模式时，你可从被观察者处推(push)或拉(pull)数据(然而，推的方式被认为更“正确”)。 有多个观察者时，不可以依赖特定的通知次序。 Java有多种观察者模式的实现，包括了通用的java.util.Observable。 要注意java.util.Observable实现上所带来的一些问题。 如果有必要的话，可以实现自己的Observable。 Swing大量使用观察者模式，许多GUI框架也是如此。 此模式也被应用在许多地方，例如:JavaBeans、RMI。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-策略模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Strategy%2F</url>
    <content type="text"><![CDATA[什么是策略模式策略这个词应该怎么理解，打个比方说，我们出门的时候会选择不同的出行方式，比如骑自行车、坐公交、坐火车、坐飞机、坐火箭等等，这些出行方式，每一种都是一个策略。 再比如我们去逛商场，商场现在正在搞活动，有打折的、有满减的、有返利的等等，其实不管商场如何进行促销，说到底都是一些算法，这些算法本身只是一种策略，并且这些算法是随时都可能互相替换的，比如针对同一件商品，今天打八折、明天满100减30，这些策略间是可以互换的。 策略模式（Strategy），定义了一组算法，将每个算法都封装起来，并且使它们之间可以互换。 示例模拟鸭子项目 12345678910public abstract class Duck &#123; public void Quack() &#123; System.out.println("~~gaga~~"); &#125; public abstract void display(); public void swim() &#123; System.out.println("~~im swim~~"); &#125;&#125; GreenHeadDuck继承Duck ： 1234567public class GreenHeadDuck extends Duck &#123; @Override public void display() &#123; System.out.println("**GreenHead**"); &#125;&#125; 新需求添加会飞的鸭子 123456public abstract class Duck &#123; //...; public void Fly() &#123; System.out.println("~~im fly~~"); &#125;&#125; 问题来了,这个Fly让所有子类都会飞了，这是不科学的。并非Duck所有的子类都会飞。在Duck超类中加上新的行为，会使得某些并不适合该行为的子类也具有该行为。这个导致，后面几十个鸭子不没有这个功能，不会飞，那么他们的都要去实现。工作量大，而且重复劳动。所以：超类挖的一个坑，每个子类都要来填，增加工作量，复杂度O(N^2)。不是好的设计方式 用策略模式来解决新需求需要新的设计方式，应对项目的扩展性，降低复杂度： 1）分析项目变化与不变部分，提取变化部分，然后把变化的部分抽象成接口+实现； 2）鸭子哪些功能是会根据新需求变化的？叫声、飞行… 重新设计模拟鸭子项目123456789101112131415161718192021222324public abstract class Duck &#123; FlyBehavior mFlyBehavior; QuackBehavior mQuackBehavior; public Duck() &#123; &#125; public void Fly() &#123; mFlyBehavior.fly(); &#125; public void Quack() &#123; mQuackBehavior.quack(); &#125; public abstract void display();&#125;public class GreenHeadDuck extends Duck &#123; public GreenHeadDuck() &#123; mFlyBehavior = new GoodFlyBehavior(); mQuackBehavior = new GaGaQuackBehavior(); &#125; @Override public void display() &#123; System.out.println("I’m a real GreenHeadDuck"); &#125;&#125; 总结 分析项目中变化部分与不变部分（方法论）——&gt;这个方法论不仅是策略模式中才可以用的，用来分析项目中变法的何不变化的，变化的就可以怎么来抽取替换。而且变化的抽离出来的行为族，行为族之间是可以来相互替换的。 多用组合少用继承；用行为类组合，而不是行为的继承。更有弹性 策略模式中的设计原则 开闭原则（Open-Closed Principle，缩写为OCP） 一个软件实体应当对扩展开放(例如对抽象层的扩展)，对修改关闭(例如对抽象层的修改)。即在设计一个模块的时候，应当使这个模块可以在不被修改的前提下被扩展。 开闭原则的关键，在于抽象。策略模式，是开闭原则的一个极好的应用范例。 里氏替换原则（Liskov Substitution Principle，缩写为LSP） 里氏替换原则里一个软件实体如果使用的是一个基类的话，那么一定适用于其子类，而且它根本不能察觉到基类对象和子类对象的区别。 比如，假设有两个类，一个是Base类，一个是Derived类，并且Derived类是Base类的子类。那么一个方法如果可以接受一个基类对象b的话：method1(Base b)，那么它必然可以接受一个子类对象d，也即可以有method1(d)。反之，则不一定成立。 里氏替换原则讲的是基类与子类的关系。只有当这种关系存在时，里氏替换关系才存在，反之则不存在。 策略模式之所以可行的基础便是里氏替换原则：策略模式要求所有的策略对象都是可以互换的，因此它们都必须是一个抽象策略角色的子类。在客户端则仅知道抽象策略角色类型，虽然变量的真实类型可以是任何一个具体策略角色的实例。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Guava-总结]]></title>
    <url>%2FGUAVA%2FGuava%2F</url>
    <content type="text"><![CDATA[项目相关信息官方首页：http://code.google.com/p/guava-libraries英文文档：http://www.ostools.net/apidocs/apidoc?api=guava中文文档：https://www.kancloud.cn/wizardforcel/java-opensource-doc/112616 博客 Guava 学习笔记：Google Guava 类库简介 Guava 学习笔记：Optional 优雅的使用null Guava学习笔记：Preconditions优雅的检验参数 Guava学习笔记：复写的Object常用方法 Guava学习笔记：Ordering犀利的比较器 Guava学习笔记：简化异常处理的Throwables类 Guava学习笔记：Immutable(不可变)集合 Guava学习笔记：Guava新增集合类型-Multiset Guava 学习笔记：Guava 新增集合类型 —— Multimap Guava学习笔记：Guava新增集合类型-Bimap Guava学习笔记：Guava新集合-Table等 Guava学习笔记：Guava cache Guava学习笔记：EventBus Guava学习笔记：Range]]></content>
      <categories>
        <category>GUAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>GUAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-CAS]]></title>
    <url>%2FJAVA%2FJava-Cas%2F</url>
    <content type="text"><![CDATA[CAS在看线程池源码的时候发现有很多CAS操作，那么什么是CAS？ 定义CAS是英文单词 Compare And Swap 的缩写，翻译过来就是比较并替换，它是一种原子操作，同时 CAS 是一种乐观机制。java.util.concurrent 包很多功能都是建立在 CAS 之上，如 ReenterLock 内部的 AQS，各种原子类，其底层都用 CAS来实现原子操作。 如何解决并发安全问题在我们认识 CAS 之前，我们是通过什么来解决并发带来的安全问题呢？volatile 关键字可以保证变量的可见性，但保证不了原子性；synchronized 关键字利用 JVM 字节码层面来实现同步机制，它是一个悲观锁机制。 123456public class Test &#123; public volatile int i; public void add() &#123; i++; &#125;&#125; 使用 javap -c Test.class 命令查看看add方法的字节码指令 123456789public void add(); Code: 0: aload_0 1: dup 2: getfield #2 // Field n:I 5: iconst_1 6: iadd 7: putfield #2 // Field n:I 10: return i++被拆分成了几个指令： 1. 执行getfield拿到原始i； 2. 执行iadd进行加1操作； 3. 执行putfield写把累加后的值写回i； 当线程 1 执行到加 1 步骤时，由于还没有执行赋值改变变量的值，这时候并不会刷新主内存区中的变量，如果此时线程 2 正好要拷贝该变量的值到自己私有缓存中，问题就出现了，当线程 2 拷贝完以后，线程1正好执行赋值运算，立马更新主内存区的值，那么此时线程 2 的副本就是旧的了，脏读又出现了。 怎么解决这个问题呢？在 add 方法加上 synchronized 修饰解决。 123456public class Test &#123; public volatile int i; public synchronized void add() &#123; i++; &#125;&#125; 这个方案当然可行，但是大大降低了性能。 CAS原理CAS机制当中使用了3个基本操作数：内存地址V，旧的预期值A，要修改的新值B。更新一个变量的时候，只有当变量的预期值A和内存地址V当中的实际值相同时，才会将内存值修改为 B 并返回 true，否则什么都不做，并返回 false。 源码分析下面以AtomicInteger的实现为例，分析一下CAS是如何实现的。 1234567891011121314151617public class AtomicInteger extends Number implements java.io.Serializable &#123; private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125; private volatile int value; // 省略部分代码&#125; Unsafe：是CAS的核心类(后门类，执行CPU指令)，它可以提供硬件级别的原子操作，它可以获取某个属性在内存中的位置，也可以修改对象的字段值，其底层是用 C/C++valueOffset：表示该变量值在内存中的偏移地址，因为Unsafe就是根据内存偏移地址获取数据的。value：要修改的值，用volatile修饰，保证了多线程之间的内存可见性。 看看AtomicInteger如何实现并发下的累加操作： 123456789101112131415161718192021// AtomicInteger.getAndAddpublic final int getAndAdd(int delta) &#123; return unsafe.getAndAddInt(this, valueOffset, delta);&#125;// unsafe.getAndAddInt/*** var1：要修改的值* var2：期望值偏移地址* var4：要增加的值* var5：当前值* var5 + var4： 当前值+要增加的值 = 目标值*/public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2);//获取对象中offset偏移地址对应的整型field的值 &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; 假设线程A和线程B同时执行getAndAdd操作（分别跑在不同CPU上）： AtomicInteger里面的value原始值为 n，根据Java内存模型，线程A和线程B各自持有一份value的副本，值为n。 线程A通过getIntVolatile(var1, var2)拿到value值 n，这时线程A被挂起。 线程B也通过getIntVolatile(var1, var2)方法获取到value值 n，运气好，线程B没有被挂起，并执行compareAndSwapInt方法比较内存值也为 n，成功修改内存值为 m。 这时线程A恢复，执行compareAndSwapInt方法比较，发现自己手里的值(n)和内存的值(m)不一致，说明该值已经被其它线程提前修改过了，那只能重新来一遍了。 重新获取value值，因为变量value被volatile修饰，所以其它线程对它的修改，线程A总是能够看到，线程A继续执行compareAndSwapInt进行比较替换，直到成功。 继续深入看看Unsafe类中的compareAndSwapInt方法实现。 1public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); Java 并没有直接实现 CAS，CAS 相关的实现是通过 C++ 内联汇编的形式实现的。Java 代码需通过 JNI 才能调用，位于 unsafe.cpp，在OpenJDK8里的路径为: openjdk/hotspot/src/share/vm/prims/unsafe.cpp。 123456UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper("Unsafe_CompareAndSwapInt"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;UNSAFE_END 逻辑执行流程： obj是AtomicInteger对象，通过 JNIHandles::resolve() 获取obj在内存中OOP实例p 根据成员变量value反射后计算出的内存偏移值offset去内存中取指针addr 获得更新值x、指针addr、期待值e三个参数后，调用Atomic::cmpxchg(x, addr, e) 通过Atomic::cmpxchg(x, addr, e)实现CAS对应OpenJDK8的路径是: openjdk/hotspot/src/share/vm/runtime/atomic.cpp 12345678910111213141516171819jbyte Atomic::cmpxchg(jbyte exchange_value, volatile jbyte* dest, jbyte compare_value) &#123; assert(sizeof(jbyte) == 1, "assumption."); uintptr_t dest_addr = (uintptr_t)dest; uintptr_t offset = dest_addr % sizeof(jint); volatile jint* dest_int = (volatile jint*)(dest_addr - offset); jint cur = *dest_int; jbyte* cur_as_bytes = (jbyte*)(&amp;cur); jint new_val = cur; jbyte* new_val_as_bytes = (jbyte*)(&amp;new_val); new_val_as_bytes[offset] = exchange_value; while (cur_as_bytes[offset] == compare_value) &#123; jint res = cmpxchg(new_val, dest_int, cur); if (res == cur) break; cur = res; new_val = cur; new_val_as_bytes[offset] = exchange_value; &#125; return cur_as_bytes[offset];&#125; 其中的cmpxchg为核心内容. 但是这句代码根据操作系统和处理器的不同, 使用不同的底层代码. 1#include "runtime/atomic.inline.hpp" atomic.inline.hpp中定义如下，可见不同不同操作系统, 不同的处理器, 都要走不同的cmpxchg()方法的实现. 123456789101112131415161718192021222324252627282930313233#include "runtime/atomic.hpp"// Linux#ifdef TARGET_OS_ARCH_linux_x86# include "atomic_linux_x86.inline.hpp"#endif#ifdef TARGET_OS_ARCH_linux_sparc# include "atomic_linux_sparc.inline.hpp"#endif#ifdef TARGET_OS_ARCH_linux_zero# include "atomic_linux_zero.inline.hpp"#endif#ifdef TARGET_OS_ARCH_linux_arm# include "atomic_linux_arm.inline.hpp"#endif#ifdef TARGET_OS_ARCH_linux_ppc# include "atomic_linux_ppc.inline.hpp"#endif// Solaris#ifdef TARGET_OS_ARCH_solaris_x86# include "atomic_solaris_x86.inline.hpp"#endif#ifdef TARGET_OS_ARCH_solaris_sparc# include "atomic_solaris_sparc.inline.hpp"#endif// Windows#ifdef TARGET_OS_ARCH_windows_x86# include "atomic_windows_x86.inline.hpp"#endif// ..省略 以其中的linux操作系统 x86处理器为例, atomic_linux_x86.inline.hpp在OpenJDK中路径如下: openjdk/hotspot/src/os_cpu/linux_x86/vm/atomic_linux_x86.inline.hpp 12345678910#define LOCK_IF_MP(mp) "cmp $0, " #mp "; je 1f; lock; 1: "inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; int mp = os::is_MP(); __asm__ volatile (LOCK_IF_MP(%4) "cmpxchgl %1,(%3)" : "=a" (exchange_value) : "r" (exchange_value), "a" (compare_value), "r" (dest), "r" (mp) : "cc", "memory"); return exchange_value;&#125; 已经开始内联汇编了，头疼 __asm__：表示汇编的开始volatile：表示禁止编译器优化cmpxchgl：就是汇编中x86的比较并交换指令了。LOCK_IF_MP：是个内联函数，根据当前系统是否为多核处理器决定是否为cmpxchg1指令添加lock前缀。 简单说下C内联汇编的语法格式： 1234567__asm__ volatile("Instruction List" : Output : Input : Clobber/Modify); instruction list：它是汇编指令列表Clobber/Modify：寄存器/内存修改标示。有时候,当你想通知GCC当前内联汇编语句可能会对某些寄存器或内存进行修改,希望GCC在编译时能够将这一点考虑进去;那么你就可以在Clobber/Modify部分声明这些寄存器或内存 所以上述汇编指令解释为：嵌入式汇编规定把输出和输入寄存器按统一顺序编号，顺序是从输出寄存器序列从左到右从上到下以%0开始，分别记为%0、%1···%9。也就是说，输出的eax是%0，输入的exchange_value、compare_value、dest、mp分别是%1、%2、%3、%4。然后看asm里的第一行指令，cmpxchgl %1,(%3)，比较eax(compare_value在eax中)与dest的值，如果相等，那么将exchange_value的值赋值给dest；否则，将dest的值赋值给eax。然后看输出: “=a” (exchange_value) 表示把eax中存的值(compare_value)写入exchange_value变量中。 Atomic::cmpxchg这个函数最终返回值是exchange_value，也就有两种情况： 如果cmpxchgl执行时compare_value和dest指针指向内存值相等则会使得dest指针指向内存值变成exchange_value，最终eax存的compare_value赋值给了exchange_value变量，即函数最终返回的值是原先的compare_value。此时Unsafe_CompareAndSwapInt的返回值(jint)(Atomic::cmpxchg(x, addr, e)) == e就是true，表明CAS成功。 如果cmpxchgl执行时compare_value和(dest)不等则会把当前dest指针指向内存的值写入eax，最终输出时赋值给exchange_value变量作为返回值，导致(jint)(Atomic::cmpxchg(x, addr, e)) == e得到false，表明CAS失败。 lock前缀在单处理器系统中是不需要加lock的，因为能够在单条指令中完成的操作都可以认为是原子操作，中断只能发生在指令与指令之间。在多处理器系统中,由于系统中有多个处理器在独立的运行，即使在能单条指令中完成的操作也可能受到干扰。 在所有的 X86 CPU 上都具有锁定一个特定内存地址的能力，当这个特定内存地址被锁定后，它就可以阻止其他的系统总线读取或修改这个内存地址。这种能力是通过 LOCK 指令前缀再加上前面的汇编指令来实现的。当使用 LOCK 指令前缀时，它会使 CPU 宣告一个 LOCK# 信号，这样就能确保在多处理器系统或多线程竞争的环境下互斥地使用这个内存地址。当指令执行完毕，这个锁定动作也就会消失。 缺点 CPU开销较大：在并发量比较高的情况下，如果许多线程反复尝试更新某一个变量，却又一直更新不成功，循环往复，会给CPU带来很大的压力。 不能保证代码块的原子性：CAS机制所保证的只是一个变量的原子性操作，而不能保证整个代码块的原子性。比如需要保证多个变量共同进行原子性的更新，就得使用Synchronized了。 ABA问题：这是CAS机制最大的问题所在。 ABA问题线程 1 从内存位置 V 取出 A，这时候线程 2 也从内存位置 V 取出 A，此时线程 1 处于挂起状态，线程 2 将位置 V 的值改成 B，最后再改成 A，这时候线程 1 再执行，发现位置 V 的值没有变化，尽管线程 1 也更改成功了，但内存地址V中的变量已经经历了A-&gt;B-&gt;A的改变。 举个例子：假设有一个遵循CAS原理的提款机，小灰有100元存款，要用这个提款机来提款50元。由于提款机硬件出了点小问题，小灰的提款操作被同时提交两次，开启了两个线程，两个线程都是获取当前值100元，要更新成50元。理想情况下，应该一个线程更新成功，另一个线程更新失败，小灰的存款只被扣一次。线程1首先执行成功，把余额从100改成50。线程2因为某种原因阻塞了。这时候，小灰的妈妈刚好给小灰汇款50元。线程2仍然是阻塞状态，线程3执行成功，把余额从50改成100。线程2恢复运行，由于阻塞之前已经获得了“当前值”100，并且经过compare检测，此时存款实际值也是100，所以成功把变量值100更新成了50。小灰凭空少了50元钱。 所以真正要做到严谨的CAS机制，我们在Compare阶段不仅要比较期望值A和地址V中的实际值，还要比较变量的版本号是否一致。在Java当中，AtomicStampedReference类就实现了用版本号做比较的CAS机制。 1234567891011private static class Pair&lt;T&gt; &#123; final T reference; final int stamp; private Pair(T reference, int stamp) &#123; this.reference = reference; this.stamp = stamp; &#125; static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) &#123; return new Pair&lt;T&gt;(reference, stamp); &#125;&#125; AtomicStampedReference 的内部类 Pair, reference 维护对象的引用，stamp 维护修改的版本号。 123456789101112public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) &#123; Pair&lt;V&gt; current = pair; return expectedReference == current.reference &amp;&amp; expectedStamp == current.stamp &amp;&amp; ((newReference == current.reference &amp;&amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp)));&#125; 从 compareAndSet 方法得知，如果要更改内存中的值，不但要值相同，还要版本号相同。 参考 https://www.jianshu.com/p/0e312402f6ca https://blog.csdn.net/dlh0313/article/details/52172833 https://www.cnblogs.com/noKing/p/9094983.html https://www.jianshu.com/p/fb6e91b013cc https://objcoding.com/2018/11/29/cas/ https://mp.weixin.qq.com/s/nRnQKhiSUrDKu3mz3vItWg]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>并发</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-线程池]]></title>
    <url>%2FJAVA%2FJava-ThreadPool%2F</url>
    <content type="text"><![CDATA[线程概念操作系统调度的最小单元是线程，也叫轻量级进程（Light Weight Process），在一个进程里可以创建多个线程， 这些线程都拥有各自的计数器、 堆栈和局部变量等属性， 并且能够访问共享的内存变量。 处理器在这些线程上高速切换， 让使用者感觉到这些线程在同时执行。 线程的创建 通过继承Thread类来创建一个线程 实现Runnable接口并重写run()方法，new Thread(runnable).start()，线程启动时就会自动调用该对象的run方法 实现Callable接口并实现call()方法，使用FutureTask类包装Callable对象，使用FutureTask对象作为Thread对象的targer创建并启动线程；也可以使用线程池启动Runnable 和 Callable 的区别 1. Runnable规定方法是run方法，Callable规定方法是call方法 2. Runnable任务执行后无返回值，Callable任务执行后可返回值 3. run方法无法抛出异常，call方法可以抛出异常 4. 运行Callable任务可以拿到一个Future对象，Future表示异步计算结果，他提供了检查计算是否完成的方法，以等待计算完成并获取结果。计算完成后用get()方法获取结果，如果线程没有执行完，get()方法会阻塞当前线程执行。如果线程出现异常，get()方法会抛出异常。 线程池：Executors类提供了方便的工厂方法来创建不同类型的 executor services。无论Runnable还是Callable都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行1. public static ExecutorService newCachedThreadPool() 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程，但是在之前构造的线程可用时将重用它们。 2. public static ExecutorService newFixedThreadPool(int nThreads) 创建一个定长线程池，可控制线程最大并发数，以共享的无界队列方式来运行线程，超出的线程会在队列中等待。 3. public static ExecutorService newSingleThreadExecutor() 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，以无界队列方式来运行线程，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 4. public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) 创建一个周期线程池，支持定时及周期性任务执行。 5. public static ExecutorService newWorkStealingPool() 创建持有足够线程的线程池来支持给定的并行级别，并通过使用多个队列，减少竞争，它需要穿一个并行级别的参数，如果不传，则被设定为默认的CPU数量，这个线程池实际上是ForkJoinPool的扩展，适合使用在很耗时的任务中，能够合理的使用CPU进行并行操作。 线程的管理 ForkJoinPool 的每个工作线程都维护了一个工作队列(WorkQueue)，这是一个双端队列，里面存放的对象是任务(ForkJoinTask) 每个工作线程在运行中产生新的任务(通常是因为调用了fork())，会放入工作队列的队尾，并且工作线程在处理自己的工作队列时，使用的是LIFO方式，也就是每次从队尾取任务执行。 每个工作线程在处理自己的工作队列时，会尝试窃取一个任务(或是来自刚刚提交到pool的任务，或是来自其他的工作队列)，窃取的任务位于其他线程工作队列的队首，也就是使用FIFO方式。 在遇到join()时如果join的任务尚未完成，则会先处理其他任务，并等待其完成。 ExecutorCompletionService 内部维护了一个阻塞队列(BlockingQueue), 只有完成的任务才被加入到队列中。如果队列中的数据为空时, 调用take()就会阻塞直到有完成的任务加入队列，基于FutureTask实现。 线程池原理ThreadPoolExecutor123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; corePoolSize 核心线程数量，当有新任务在exectue()方法提交时，会执行以下判断：1. 如果运行的线程少于 corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的； 2. 如果线程池中的线程数量大于等于 corePoolSize 且小于 maximumPoolSize，则只有当workQueue满时才创建新的线程去处理任务； 3. 如果设置的corePoolSize 和 maximumPoolSize相同，则创建的线程池的大小是固定的，这时如果有新任务提交，若workQueue未满，则将请求放入workQueue中，等待有空闲的线程去从workQueue中取任务并处理； 4. 如果运行的线程数量大于等于maximumPoolSize，这时如果workQueue已经满了，则通过handler所指定的策略来处理任务； 5. 所以，任务提交时，判断的顺序为 corePoolSize –&gt; workQueue –&gt; maximumPoolSize maximumPoolSize 最大线程数量； keepAliveTime 线程池维护线程所允许的空闲时间。当线程池中的线程数量大于corePoolSize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了keepAliveTime； TimeUnit 线程保持活动的时间单位 BlockingQueue workQueue 保存等待执行的任务的阻塞队列，当提交一个新的任务到线程池以后, 线程池会根据当前线程池中正在运行着的线程的数量来决定对该任务的处理方式，主要有以下几种处理方式: 直接切换：这种方式常用的队列是SynchronousQueue 使用无界队列：一般使用基于链表的阻塞队列LinkedBlockingQueue。如果使用这种方式，那么线程池中能够创建的最大线程数就是corePoolSize，而maximumPoolSize就不会起作用了。当线程池中所有的核心线程都是RUNNING状态时，这时一个新的任务提交就会放入等待队列中。 使用有界队列：一般使用ArrayBlockingQueue。使用该方式可以将线程池的最大线程数量限制为maximumPoolSize，这样能够降低资源的消耗，但同时这种方式也使得线程池对线程的调度变得更困难，因为线程池和队列的容量都是有限的值，所以要想使线程池处理任务的吞吐率达到一个相对合理的范围，又想使线程调度相对简单，并且还要尽可能的降低线程池对资源的消耗，就需要合理的设置这两个数量。 threadFactory 它是ThreadFactory类型的变量，用来创建新线程。默认使用Executors.defaultThreadFactory() 来创建线程。使用默认的ThreadFactory来创建线程时，会使新创建的线程具有相同的NORM_PRIORITY优先级并且是非守护线程，同时也设置了线程的名称。 handler 它是RejectedExecutionHandler类型的变量，表示线程池的饱和的拒绝策略。如果阻塞队列满了并且没有空闲的线程，这时如果继续提交任务，就需要采取一种策略处理该任务。线程池提供了4种策略： AbortPolicy：直接抛出异常，这是默认策略； CallerRunsPolicy：用调用者所在的线程来执行任务； DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy：直接丢弃任务； 线程池状态123456789private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; COUNT_BITS: 线程的最大位数29位 CAPACITY：线程的最大容量 RUNNING：运行状态，线程池被一旦被创建，就处于RUNNING状态，并且线程池中的任务数为0 SHUTDOWN：线程池处在SHUTDOWN状态时，不接收新任务，但能处理已添加的任务。 STOP：线程池处在STOP状态时，不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。 TIDYING：当所有的任务已终止，任务数量”为0，线程池会变为TIDYING状态。当线程池变为TIDYING状态时，会执行钩子函数terminated()。 TERMINATED：终止状态，当执行 terminated() 后会更新为这个状态。 核心源码线程池执行源码execute123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); // clt记录着runState和workerCount int c = ctl.get(); // workerCountOf方法取出低29位的值，表示当前活动的线程数； // 如果当前活动线程数小于corePoolSize，则新建一个线程放入线程池中，并把任务添加到该线程中； if (workerCountOf(c) &lt; corePoolSize) &#123; // addWorker中的第二个参数表示限制添加线程的数量是根据corePoolSize来判断还是maximumPoolSize来判断； // 如果为true，根据corePoolSize来判断； // 如果为false，则根据maximumPoolSize来判断 if (addWorker(command, true)) return; // 如果添加失败，则重新获取ctl值 c = ctl.get(); &#125; // 如果当前线程池是运行状态 并且 任务能够成功添加到工作队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; // 重新获取ctl值 int recheck = ctl.get(); // 再次判断线程池的运行状态，如果不是运行状态，由于之前已经把command添加到workQueue中了， // 这时需要移除该command // 执行过后通过handler使用拒绝策略对该任务进行处理，整个方法返回 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 获取线程池中的有效线程数，如果数量是0，则执行addWorker方法 // 1. 第一个参数为null，表示在线程池中创建一个线程，但不去启动； // 2. 第二个参数为false，将线程池的有限线程数量的上限设置为maximumPoolSize，添加线程时根据maximumPoolSize来判断； // 如果判断workerCount大于0，则直接返回，在workQueue中新增的command会在将来的某个时刻被执行。 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; // 如果执行到这里，有两种情况： // 1.线程池已经不是RUNNING状态； // 2.线程池是RUNNING状态，但workerCount &gt;= corePoolSize并且workQueue已满; // 这时，再次调用addWorker方法，但第二个参数传入为false，将线程池的有限线程数量的上限设置为maximumPoolSize； // 如果失败则拒绝该任务 else if (!addWorker(command, false)) reject(command);&#125; runState和workCount变量怎么存储在一个int中？参考：https://blog.csdn.net/weixin_34396902/article/details/94527424 addWorker123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105private boolean addWorker(Runnable firstTask, boolean core) &#123; // 循环CAS操作，将线程池中的线程数+1 retry: for (;;) &#123; // clt记录着runState和workerCount int c = ctl.get(); // 获取运行状态 int rs = runStateOf(c); // 如果rs &gt;= SHUTDOWN，则表示此时不再接收新任务； // 接着判断以下3个条件，只要有1个不满足，则返回false： // 1. rs == SHUTDOWN，这时表示关闭状态，不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务 // 2. firsTask为空 // 3. 阻塞队列不为空 // // rs == SHUTDOWN的情况 // 这种情况下不会接受新提交的任务，所以在firstTask不为空的时候会返回false； // 如果firstTask为空，并且workQueue也为空，因为队列中已经没有任务了，不需要再添加线程了，则返回false， if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; // 获取线程数 int wc = workerCountOf(c); // 如果wc超过CAPACITY(最大线程数线程数),也就是ctl的低29位的最大值（二进制是29个1），返回false； // core是addWorker方法的第二个参数,如果为true表示根据corePoolSize来比较，如果为false则根据maximumPoolSize来比较; if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // CAS操作尝试增加workerCount，修改clt的值+1，如果成功，则跳出第一个for循环 if (compareAndIncrementWorkerCount(c)) break retry; // 如果增加workerCount失败，则重新获取ctl的值 c = ctl.get(); // 如果当前的运行状态不等于rs，说明状态已被改变，返回第一个for循环继续执行 if (runStateOf(c) != rs) continue retry; &#125; &#125; // 新建线程，并加入到线程池workers中。 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; // 根据firstTask来创建Worker对象 w = new Worker(firstTask); // 每一个Worker对象都会创建一个线程 final Thread t = w.thread; if (t != null) &#123; // 对workers操作要通过加锁来实现 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 获取运行状态 int rs = runStateOf(ctl.get()); // rs &lt; SHUTDOWN表示是RUNNING状态； // 如果rs是RUNNING状态或者rs是SHUTDOWN状态并且firstTask为null，向线程池中添加线程。 // 因为在SHUTDOWN时不会在添加新的任务，但还是会执行workQueue中的任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; // 判断添加的任务状态,如果已经开始丢出异常 if (t.isAlive()) throw new IllegalThreadStateException(); // 将新建的线程加入到线程池中，workers是一个hashSet workers.add(w); int s = workers.size(); // largestPoolSize记录着线程池中出现过的最大线程数量 if (s &gt; largestPoolSize) largestPoolSize = s; //标记任务添加 workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 启动线程 t.start(); // 标记线程启动 workerStarted = true; &#125; &#125; &#125; finally &#123; // 线程添加线程池失败或者线程start失败，则需要调用addWorkerFailed函数 // 如果添加成功则需要移除线程，并恢复复clt的值 if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; t.start()这个语句，启动时会调用Worker类中的run方法，Worker本身实现了Runnable接口，所以一个Worker类型的对象也是一个线程。 Worker类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; private static final long serialVersionUID = 6138294804551838833L; /** 线程池中正真运行的线程。通过我们指定的线程工厂创建而来 **/ final Thread thread; /** 线程包装的任务。thread 在run时主要调用了该任务的run方法 */ Runnable firstTask; /** 记录当前线程完成的任务数 */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) &#123; setState(-1); // 在调用runWorker()前，禁止interrupt中断，在interruptIfStarted()方法中会判断 getState()&gt;=0 this.firstTask = firstTask; // 利用我们指定的线程工厂创建一个线程 this.thread = getThreadFactory().newThread(this); &#125; /** Delegates main run loop to outer runWorker */ public void run() &#123; runWorker(this); &#125; // Lock methods // // The value 0 represents the unlocked state. // The value 1 represents the locked state. protected boolean isHeldExclusively() &#123; return getState() != 0; &#125; /** * 尝试获取锁 */ protected boolean tryAcquire(int unused) &#123; //尝试一次将state从0设置为1，即“锁定”状态， if (compareAndSetState(0, 1)) &#123; //设置exclusiveOwnerThread=当前线程 setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; /** * 尝试释放锁 */ protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true; &#125; public void lock() &#123; acquire(1); &#125; public boolean tryLock() &#123; return tryAcquire(1); &#125; public void unlock() &#123; release(1); &#125; public boolean isLocked() &#123; return isHeldExclusively(); &#125; /** * 中断（如果运行） * shutdownNow时会循环对worker线程执行 * 且不需要获取worker锁，即使在worker运行时也可以中断 */ void interruptIfStarted() &#123; Thread t; // 如果state&gt;=0、t!=null、且t没有被中断 // new Worker()时state==-1，说明不能中断 if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125; &#125;&#125; Worker类投机取巧的继承了AbstractQueuedSynchronizer来简化在执行任务时的获取、释放锁,这样防止了中断在运行中的任务，只会唤醒(中断)在等待从workQueue中获取任务的线程.不直接执行execute(command)提交的command，而要在外面包一层Worker主要是为了使用用AQS锁控制中断，当运行时上锁，就不能中断，TreadPoolExecutor的shutdown()方法中断前都要获取worker锁，只有在等待从workQueue中获取任务getTask()时才能中断。 runWorker 方法在Worker类中的run方法调用了runWorker方法来执行任务. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); // 获取第一个任务 Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // 允许中断 // 是否因为异常退出循环 boolean completedAbruptly = true; try &#123; // 如果task为空，则通过getTask来获取任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt // 线程池处于stop状态或者当前线程被中断时，线程池状态是stop状态 // 但是当前线程没有中断，则发出中断请求 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; //开始执行任务前的Hook，类似回调函数 beforeExecute(wt, task); Throwable thrown = null; try &#123; //执行任务 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; //任务执行后的Hook，类似回调函数 afterExecute(task, thrown); &#125; &#125; finally &#123; //执行完毕后task重置，completedTasks计数器++，解锁 task = null; w.completedTasks++; w.unlock(); &#125; &#125; //标记正常退出 completedAbruptly = false; &#125; finally &#123; //线程空闲达到我们设定的值时，Worker退出销毁。 processWorkerExit(w, completedAbruptly); &#125;&#125; getTask 方法runWorker函数中最重要的是getTask()，不断的从阻塞队列中取任务交给线程执行，并且负责线程回收 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private Runnable getTask() &#123; // 表示上次从阻塞队列中取任务时是否超时 boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // 如果线程池处于shutdown状态， // 并且队列为空，或者线程池处于stop或者terminate状态， // 在线程池数量-1，返回null，回收线程 if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; // 获取线程数 int wc = workerCountOf(c); // timed变量用于判断是否需要进行超时控制。 // allowCoreThreadTimeOut默认是false，也就是核心线程不允许进行超时； // wc &gt; corePoolSize，表示当前线程池中的线程数量大于核心线程数量； // 对于超过核心线程数量的这些线程，需要进行超时控制 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; // 如果线程数目大于最大线程数目 或 当前操作需要进行超时控制，并且上次从阻塞队列中获取任务发生了超时 // 并且 线程数目大于1 或 工作队列为空 // 尝试将workerCount减1； if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; //**保证核心线程不被销毁** // 根据timed来判断，如果为true，则通过阻塞队列的poll方法进行超时控制，如果在keepAliveTime时间内没有获取到任务，则返回null； // 否则通过take方法，如果这时队列为空，则take方法会阻塞直到队列不为空。 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; // 如果 r == null，说明已经超时，timedOut设置为true，进入下一个循环 timedOut = true; &#125; catch (InterruptedException retry) &#123; // 如果获取任务时当前线程发生了中断，则设置timedOut为false并返回循环重试 timedOut = false; &#125; &#125;&#125; FutureTask源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; /** * state字段用来保存FutureTask内部的任务执行状态，一共有7中状态，每种状态及其对应的值如下 * NEW:表示是个新的任务或者还没被执行完的任务。这是初始状态。 * COMPLETING:任务已经执行完成或者执行任务的时候发生异常，但是任务执行结果或者异常原因还没有保存到outcome字段(outcome字段用来保存任务执行结果，如果发生异常，则用来保存异常原因)的时候，状态会从NEW变更到COMPLETING。但是这个状态会时间会比较短，属于中间状态。 * NORMAL:任务已经执行完成并且任务执行结果已经保存到outcome字段，状态会从COMPLETING转换到NORMAL。这是一个最终态。 * EXCEPTIONAL:任务执行发生异常并且异常原因已经保存到outcome字段中后，状态会从COMPLETING转换到EXCEPTIONAL。这是一个最终态。 * CANCELLED:任务还没开始执行或者已经开始执行但是还没有执行完成的时候，用户调用了cancel(false)方法取消任务且不中断任务执行线程，这个时候状态会从NEW转化为CANCELLED状态。这是一个最终态。 * INTERRUPTING: 任务还没开始执行或者已经执行但是还没有执行完成的时候，用户调用了cancel(true)方法取消任务并且要中断任务执行线程但是还没有中断任务执行线程之前，状态会从NEW转化为INTERRUPTING。这是一个中间状态。 * INTERRUPTED:调用interrupt()中断任务执行线程之后状态会从INTERRUPTING转换到INTERRUPTED。这是一个最终态。 * * NEW -&gt; COMPLETING -&gt; NORMAL 正常执行并返回 * NEW -&gt; COMPLETING -&gt; EXCEPTIONAL 执行过程中出现了异常 * NEW -&gt; CANCELLED 执行前被取消 * NEW -&gt; INTERRUPTING -&gt; INTERRUPTED 取消时被中断 */ private volatile int state; private static final int NEW = 0; private static final int COMPLETING = 1;//大于这个值就是完成状态 private static final int NORMAL = 2; private static final int EXCEPTIONAL = 3; private static final int CANCELLED = 4; private static final int INTERRUPTING = 5; private static final int INTERRUPTED = 6; /** The underlying callable; nulled out after running */ private Callable&lt;V&gt; callable; /** The result to return or exception to throw from get() */ private Object outcome; // non-volatile, protected by state reads/writes /** 执行callable的线程 **/ private volatile Thread runner; /** 使用Treiber算法实现的无阻塞的Stack，用于存放等待的线程 */ private volatile WaitNode waiters; @SuppressWarnings("unchecked") private V report(int s) throws ExecutionException &#123; // 拿到返回结果 Object x = outcome; // 判断状态 if (s == NORMAL) // 状态正常，就返回结果值 return (V)x; // 判断异常，就抛出异常。 if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x); &#125; /** * 构造方法 */ public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable &#125; /** * 这个构造方法会把传入的Runnable封装成一个Callable对象保存在callable字段中，同时如果任务执行成功的话就会返回传入的result。 * 这种情况下如果不需要返回值的话可以传入一个null。 */ public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable &#125; //判断任务是否被取消 public boolean isCancelled() &#123; return state &gt;= CANCELLED; &#125; //判断任务是否完成 public boolean isDone() &#123; return state != NEW; &#125; public boolean cancel(boolean mayInterruptIfRunning) &#123; // 1. 任务是new状态 并且 根据mayInterruptIfRunning把状态从NEW转化到INTERRUPTING或CANCELLED // 不符合上述状态，返回false if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // 2. 如果需要中断任务执行线程 if (mayInterruptIfRunning) &#123; try &#123; // runner保存着当前执行任务的线程 Thread t = runner; if (t != null) //中断任务执行线程 t.interrupt(); &#125; finally &#123; // final state // 修改状态为INTERRUPTED UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true; &#125; public V get() throws InterruptedException, ExecutionException &#123; int s = state; // 判断任务当前的state &lt;= COMPLETING是否成立。 if (s &lt;= COMPLETING) // 如果成立，表明任务还没有结束(这里的结束包括任务正常执行完毕，任务执行异常，任务被取消) // 调用awaitDone()进行阻塞等待。 s = awaitDone(false, 0L); // 任务已经结束，调用report()返回结果。 return report(s); &#125; public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; if (unit == null) throw new NullPointerException(); int s = state; // 如果awaitDone()超时返回之后任务还没结束，则抛出异常 if (s &lt;= COMPLETING &amp;&amp; (s = awaitDone(true, unit.toNanos(timeout))) &lt;= COMPLETING) throw new TimeoutException(); return report(s); &#125; protected void done() &#123; &#125; protected void set(V v) &#123; // 尝试CAS操作，把当前的状态从NEW变更为COMPLETING(中间状态)状态。 if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; // 把任务执行结果保存在outcome字段中。 outcome = v; // CAS的把当前任务状态从COMPLETING变更为NORMAL UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // final state finishCompletion(); &#125; &#125; protected void setException(Throwable t) &#123; // 尝试CAS操作，把当前的状态从NEW变更为COMPLETING(中间状态)状态。 if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; // 把异常原因保存在outcome字段中，outcome字段用来保存任务执行结果或者异常原因。 outcome = t; // CAS的把当前任务状态从COMPLETING变更为EXCEPTIONAL。 UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); // final state finishCompletion(); &#125; &#125; public void run() &#123; // 状态如果不是NEW，说明任务或者已经执行过，或者已经被取消，直接返回 // 状态如果是NEW，则尝试把当前执行线程保存在runner字段(runnerOffset)中，如果赋值失败则直接返回 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; // 只有初始状态才会执行 if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; // 执行任务 计算逻辑 result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; // 保存异常 setException(ex); &#125; if (ran) // 任务执行成功，保存返回结果 set(result); &#125; &#125; finally &#123; // 无论是否执行成功，把runner设置为null runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; // 如果任务被中断，执行中断处理 if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; &#125; /** * 与run方法类似，区别在于这个方法不会设置任务的执行结果值 * * @return &#123;@code true&#125; if successfully run and reset */ protected boolean runAndReset() &#123; if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return false; boolean ran = false; int s = state; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; s == NEW) &#123; try &#123; // 不获取和设置返回值 c.call(); // don't set result ran = true; &#125; catch (Throwable ex) &#123; setException(ex); &#125; &#125; &#125; finally &#123; runner = null; s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; // 是否正确的执行并复位 return ran &amp;&amp; s == NEW; &#125; private void handlePossibleCancellationInterrupt(int s) &#123; if (s == INTERRUPTING) while (state == INTERRUPTING) Thread.yield(); // wait out pending interrupt // 确保cancel(true)产生的中断发生在run或runAndReset方法执行的过程中。 //这里会循环的调用Thread.yield()来确保状态在cancel方法中被设置为INTERRUPTED。 &#125; /** * Simple linked list nodes to record waiting threads in a Treiber * stack. See other classes such as Phaser and SynchronousQueue * for more detailed explanation. */ static final class WaitNode &#123; volatile Thread thread; volatile WaitNode next; WaitNode() &#123; thread = Thread.currentThread(); &#125; &#125; /** * Removes and signals all waiting threads, invokes done(), and * nulls out callable. */ private void finishCompletion() &#123; // assert state &gt; COMPLETING; // 执行该方法时state必须大于COMPLETING // 依次遍历waiters链表 for (WaitNode q; (q = waiters) != null;) &#123; // 设置栈顶节点为null if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; for (;;) &#123; Thread t = q.thread; if (t != null) &#123; q.thread = null; // 唤醒等待线程 LockSupport.unpark(t); &#125; WaitNode next = q.next; // 如果next为空，说明栈空了，跳出循环 if (next == null) break; // 方便gc回收 q.next = null; // 重新设置栈顶node q = next; &#125; break; &#125; &#125; // 空方法，留给子类扩展 done(); callable = null; // to reduce footprint &#125; /** * Awaits completion or aborts on interrupt or timeout. * * @param timed true if use timed waits * @param nanos time to wait, if timed * @return state upon completion */ private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; // 计算等待截止时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; for (;;) &#123; // 1. 判断阻塞线程是否被中断 if (Thread.interrupted()) &#123; // 被中断则在等待队列中删除该节点 removeWaiter(q); // 抛出InterruptedException异常 throw new InterruptedException(); &#125; int s = state; // 2. 获取当前状态，如果状态大于COMPLETING if (s &gt; COMPLETING) &#123; // 说明任务已经结束(要么正常结束，要么异常结束，要么被取消) if (q != null) // 把thread显示置空 q.thread = null; // 返回结果 return s; &#125; // 3. 如果状态处于中间状态COMPLETING // 表示任务已经结束但是任务执行线程还没来得及给outcome赋值 else if (s == COMPLETING) // cannot time out yet Thread.yield();// 让出执行权让其他线程优先执行 // 4. 如果等待节点为空，则构造一个等待节点 else if (q == null) q = new WaitNode(); // 5. 如果还没有入队列，则把当前节点加入waiters首节点并替换原来waiters else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); else if (timed) &#123; // 如果需要等待特定时间，则先计算要等待的时间 // 如果已经超时，则删除对应节点并返回对应的状态 nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; // 6. 阻塞等待特定时间 LockSupport.parkNanos(this, nanos); &#125; // 6. 阻塞等待直到被其他线程唤醒 else LockSupport.park(this); &#125; &#125; private void removeWaiter(WaitNode node) &#123; if (node != null) &#123; // 将thread设置为null是因为下面要根据thread是否为null判断是否要把node移出 node.thread = null; // 这里自旋保证删除成功 retry: for (;;) &#123; // restart on removeWaiter race for (WaitNode pred = null, q = waiters, s; q != null; q = s) &#123; s = q.next; // q.thread != null说明该q节点不需要移除 if (q.thread != null) pred = q; // 如果q.thread == null，且pred != null，需要删除q节点 else if (pred != null) &#123; // 删除q节点 pred.next = s; // pred.thread == null时说明在并发情况下被其他线程修改了； // 返回第一个for循环重试 if (pred.thread == null) // check for race continue retry; &#125; // 如果q.thread != null且pred == null，说明q是栈顶节点 // 设置栈顶元素为s节点，如果失败则返回重试 else if (!UNSAFE.compareAndSwapObject(this, waitersOffset, q, s)) continue retry; &#125; break; &#125; &#125; &#125; // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long stateOffset; private static final long runnerOffset; private static final long waitersOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = FutureTask.class; stateOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("state")); runnerOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("runner")); waitersOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("waiters")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; 线程池中的线程初始化 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程； prestartAllCoreThreads()：初始化所有核心线程 线程池的关闭ThreadPoolExecutor提供了两个方法，用于线程池的关闭 shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务 线程池大小 粗略 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1 如果是IO密集型任务，参考值可以设置为2*NCPU 精确：（(线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 最佳：压测 任务缓存队列workQueue，它用来存放等待执行的任务。BlockingQueue 是个接口，你需要使用它的实现之一来使用BlockingQueue，java.util.concurrent包下具有以下 BlockingQueue 接口的实现类： ArrayBlockingQueue：ArrayBlockingQueue 是一个有界的阻塞队列，其内部实现是将对象放到一个数组里。有界也就意味着，它不能够存储无限多数量的元素。它有一个同一时间能够存储元素数量的上限。你可以在对其初始化的时候设定这个上限，但之后就无法对这个上限进行修改了(译者注：因为它是基于数组实现的，也就具有数组的特性：一旦初始化，大小就无法修改)。 LinkedBlockingQueue：LinkedBlockingQueue 内部以一个链式结构(链接节点)对其元素进行存储。如果需要的话，这一链式结构可以选择一个上限。如果没有定义上限，将使用 Integer.MAX_VALUE 作为上限。 DelayQueue：DelayQueue 对元素进行持有直到一个特定的延迟到期。注入其中的元素必须实现 java.util.concurrent.Delayed 接口。 PriorityBlockingQueue：PriorityBlockingQueue 是一个无界的并发队列。它使用了和类 java.util.PriorityQueue 一样的排序规则。你无法向这个队列中插入 null 值。所有插入到 PriorityBlockingQueue 的元素必须实现 java.lang.Comparable 接口。因此该队列中元素的排序就取决于你自己的 Comparable 实现。 SynchronousQueue：SynchronousQueue 是一个特殊的队列，它的内部同时只能够容纳单个元素。如果该队列已有一元素的话，试图向队列中插入一个新元素的线程将会阻塞，直到另一个线程将该元素从队列中抽走。同样，如果该队列为空，试图向队列中抽取一个元素的线程将会阻塞，直到另一个线程向队列中插入了一条新的元素。据此，把这个类称作一个队列显然是夸大其词了。它更多像是一个汇合点。 线程池总结 线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。 当调用 execute() 方法添加一个任务时，线程池会做如下判断： 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务； 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列； 如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务； 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会抛出异常RejectExecutionException。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2FTree%2F</url>
    <content type="text"><![CDATA[二叉查找树在介绍B+树前，需要先了解一下二叉查找树。B+树是通过二叉查找树，再由平衡二叉树，B树演化而来。 定义左孩子比父节点小，右孩子比父节点大，中序遍历可以得到键值的排序输出。 插入 删除单孩子的情况：如果删除的节点有左孩子那就把左孩子顶上去，如果有右孩子就把右孩子顶上去 左右都有孩子的情况：可以这么想象，如果我们要删除一个数组的元素，那么我们在删除后会将其后面的一个元素顶到被删除的位置。二叉树操作同样，我们根据中序遍历找到要删除结点的后一个结点，然后顶上去，原理跟数组一样。 查找查找的平均时间复杂度log(N)，在最坏的情况下会出现链表的形式，复杂度退化到O(N)。 平衡二叉树当二叉查找树以完全二叉树的形式展现，这样我才能做到查找是严格的O(logN)， 定义首先符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度最大差为1。 旋转节点再怎么失衡都逃不过4种情况 左子树的左边节点 右子树的右边节点 左子树的右边节点找到失衡点，失衡点的左子树进行右子树的右边节点情况旋转，然后进行左子树的左边节点旋转 右子树的左边节点 插入&amp;删除步骤同二叉查找树，只是在插入或删除节点之后多了一步旋转的过程 红黑树红黑树也是二叉查找树，因此查找操作与普通二叉查找树操作相同。红黑树插入和删除操作会导致不再匹配红黑树性质，需要进行颜色变更和旋转调整红黑树，使其恢复性质。 定义 每个节点要么是黑色，要么是红色。（节点非黑即红） 根节点是黑色。 每个叶子节点（NIL）是黑色。 如果一个节点是红色的，则它的子节点必须是黑色的。（也就是说父子节点不能同时为红色） 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。（这一点是平衡的关键） 旋转同平衡二叉树 插入场景一：空树根据红黑树性质第二点，红黑树根节点为黑色，即将插入节点修改成黑色即可； 场景二：插入节点 Key 已存在在插入节点之前，红黑树是保持着平衡状态，只需要将插入节点的颜色变为被替换节点的颜色，同时替换掉原节点； 场景三：插入节点的父节点是黑色节点插入的是红色节点 N，并不影响红黑树的平衡，插入之后不需要作其它处理。 场景四：插入节点的父节点是红色节点且叔叔是红色节点根据红黑树性质 4 ，两个红色节点不能直接相连；把父节点 P 及叔叔节点 S 由红色节点变成黑色节点，再把祖父节点 PP 变成红色，至此解决了插入节点与父节点两个红色节点直连的问题，并且黑色节点数量保持不变，但祖父节点由黑色变成了红色； 如果祖父节点的父节点是红色节点应如何处理？处理：将祖父节点 PP 当作新插入的红色节点，从祖父节点的父节点开始由底向上进行处理，直至插入节点的父节点为黑色节点或者插入节点为根节点。 祖父节点 PP 刚好为根结点时，应如何处理？处理：根据性质2，我们必须把PP重新设为黑色，那么树的红黑结构变为：黑黑红。换句话说，从根结点到叶子结点的路径中，黑色结点增加了。这也是唯一一种会增加红黑树黑色结点层数的插入情景。 场景五：插入节点的父节点是红色节点，且叔叔节点是空 (null) 节点或者是黑色节点 场景 5.1，插入节点 N 是父节点 P 的左节点且父节点 P 是祖父节点 PP 的左节点:处理：父节点 P 变成红黑色，祖父节点变成红色，并以祖父节点 PP 为支点进行右旋； 场景 5.2，插入节点是父节点的右节点且父节点 P 是祖父节点 PP 的左节点:处理：以插入节点的父节点 P 为支点进行左旋，转换到场景 5.1； 场景 5.3，插入节点 N 是父节点 P 的右子节点且父节点 P 是祖父节点 PP 的右节点:处理：与场景 5.1 互为镜像，父节点 P 变成黑色，祖父节点变成红色，并以祖父节点 PP 为支点进行左旋； 场景 5.4，插入节点的父节点的左子节点，父节点是祖父节点的右子节点：处理：与场景 5.2 互为镜像，以插入节点的父节点 P 为支点进行右旋，转换到场景 5.3； 删除删除分为两部：1.二叉搜索树的删除(见⬆️述二叉搜索树)2.红黑树的删除调整。R是即将被替换到删除结点的位置的替代结点，在删除前，它还在原来所在位置参与树的子平衡，平衡后再替换到删除结点的位置，才算删除完成。R最终可以看作是删除的 删除情景1：替换结点是红色结点由于替换结点时红色，删除也了不会影响红黑树的平衡。处理：颜色变为删除结点的颜色 删除情景2：替换结点是黑结点当替换结点是黑色时，我们就不得不进行自平衡处理了。我们必须还得考虑替换结点是其父结点的左子结点还是右子结点，来做不同的旋转操作，使树重新平衡。 删除情景2.1：替换结点是其父结点的左子结点 删除情景2.1.1：替换结点的兄弟结点是红结点若兄弟结点是红结点，那么根据性质4，兄弟结点的父结点和子结点肯定为黑色，按下图处理，得到删除情景2.1.2.3处理： 将S设为黑色 将P设为红色 对P进行左旋，得到情景2.1.2.3 删除情景2.1.2：替换结点的兄弟结点是黑结点当兄弟结点为黑时，其父结点和子结点的具体颜色也无法确定,此时又得考虑多种子情景。 删除情景2.1.2.1：替换结点的兄弟结点的右子结点是红结点，左子结点任意颜色即将删除的左子树的一个黑色结点，显然左子树的黑色结点少1了，然而右子树又有红色结点，那么我们直接向右子树“借”个红结点来补充黑结点就好啦，此时肯定需要用旋转处理了。处理： 将S的颜色设为P的颜色 将P设为黑色 将SR设为黑色 对P进行左旋平衡后的图怎么不满足红黑树的性质？前文提醒过，R是即将替换的，它还参与树的自平衡，平衡后再替换到删除结点的位置，所以R最终可以看作是删除的。另外图2.1.2.1是考虑到第一次替换和自底向上处理的情况，如果只考虑第一次替换的情况，根据红黑树性质，SL肯定是红色或为Nil，所以最终结果树是平衡的。 删除情景2.1.2.2：替换结点的兄弟结点的右子结点为黑结点，左子结点为红结点兄弟结点所在的子树有红结点，我们总是可以向兄弟子树借个红结点过来，显然该情景可以转换为情景2.1.2.1处理： 将S设为红色，将SL设为黑色 对S进行右旋，得到情景2.1.2.1 删除情景2.1.2.3：替换结点的兄弟结点的子结点都为黑结点此次兄弟子树都没红结点“借”了，兄弟帮忙不了，找父母呗，这种情景我们把兄弟结点设为红色，再把父结点当作替代结点，自底向上处理，去找父结点的兄弟结点去“借”。但为什么需要把兄弟结点设为红色呢？显然是为了在P所在的子树中保证平衡（R即将删除，少了一个黑色结点，子树也需要少一个），后续的平衡工作交给父辈们考虑了，还是那句，当每棵子树都保持平衡时，最终整棵总是平衡的。处理：将S设为红色,把P作为新的替换结点,重新进行删除结点情景处理 删除情景2.2：替换结点是其父结点的右子结点（和上述操作相反） 删除情景2.2.1：替换结点的兄弟结点是红结点处理： 将S设为黑色 将P设为红色 对P进行右旋，得到情景2.2.2.3 删除情景2.2.2：替换结点的兄弟结点是黑结点 删除情景2.2.2.1：替换结点的兄弟结点的左子结点是红结点，右子结点任意颜色处理： 将S的颜色设为P的颜色 将P设为黑色 将SL设为黑色 对P进行右旋 删除情景2.2.2.2：替换结点的兄弟结点的左子结点为黑结点，右子结点为红结点处理： 将S设为红色 将SR设为黑色 对S进行左旋，得到情景2.2.2.1 删除情景2.2.2.3：替换结点的兄弟结点的子结点都为黑结点处理： 将S设为红色,把P作为新的替换结点,重新进行删除结点情景处理 综上，红黑树删除后自平衡的处理可以总结为： 自己能搞定的自消化（情景1） 自己不能搞定的叫兄弟帮忙（除了情景1、情景2.1.2.3和情景2.2.2.3） 兄弟都帮忙不了的，通过父母，找远方亲戚（情景2.1.2.3和情景2.2.2.3） 请画出下图的删除自平衡处理过程。 参考：https://www.jianshu.com/p/e136ec79235c]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>树</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql-存储程序]]></title>
    <url>%2FMysql%2FMysql-storage%2F</url>
    <content type="text"><![CDATA[MySQL中的存储程序本质上封装了一些可执行的语句，然后给用户提供一种简单的调用方式来执行这些语句，根据调用方式的不同，我们可以把存储程序分为存储例程、触发器和事件这几种类型。其中，存储例程又可以被细分为存储函数和存储过程。 自定义变量MySQL中对我们自定义的变量的命名有个要求，那就是变量名称前必须加一个@符号。我们自定义变量的值的类型可以是任意MySQL支持的类型，例如我们自定义一个变量a： 12mysql&gt; SET @a = 1;Query OK, 0 rows affected (0.00 sec) 如果我们想查看这个变量的值的话，使用SELECT语句就好了，不过仍然需要在变量名称加一个@符号： 1234567mysql&gt; SELECT @a;+------+| @a |+------+| 1 |+------+1 row in set (0.00 sec) 同一个变量也可以存储存储不同类型的值，比方说我们再把一个字符串值赋值给变量a： 12345678910mysql&gt; SET @a = '啦';Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT @a;+------+| @a |+------+| 啦 |+------+1 row in set (0.00 sec) 除了把一个常量赋值给一个变量以外，我们还可以把一个变量赋值给另一个变量： 12345678910mysql&gt; SET @b = @a;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT @b;+------+| @b |+------+| 啦 |+------+1 row in set (0.00 sec) 我们还可以将某个查询的结果赋值给一个变量，前提是这个查询的结果只有一个值： 12mysql&gt; SET @a = (SELECT first_column FROM first_table LIMIT 1);Query OK, 0 rows affected (0.00 sec) 还可以用另一种形式的语句来将查询的结果赋值给一个变量： 12mysql&gt; SELECT first_column FROM first_table LIMIT 1 INTO @b;Query OK, 1 row affected (0.00 sec) 我们查看一下这两个变量的值： 1234567mysql&gt; SELECT @a, @b;+------+------+| @a | @b |+------+------+| 1 | 1 |+------+------+1 row in set (0.00 sec) 如果我们的查询结果是一条记录，该记录中有多个列的值的话，我们想把这几个值分别赋值到不同的变量中，只能使用INTO语句了： 12345678910mysql&gt; SELECT first_column, second_column FROM first_table LIMIT 1 INTO @a, @b;Query OK, 1 row affected (0.00 sec)mysql&gt; SELECT @a, @b; +------+------+| @a | @b |+------+------+| 1 | aaa |+------+------+1 row in set (0.00 sec) 复合语句在MySQL客户端的交互界面处，当我们完成键盘输入并按下回车键时，MySQL客户端会检测我们输入的内容中是否包含;、\g或者\G这三个符号之一，如果有的话，会把我们输入的内容发送到服务器。这样一来，如果我们想给服务器发送复合语句（也就是由一条或多条语句组成的语句）的话，就需要把这些语句写到一行中，比如这样： 123456789101112131415161718mysql&gt; SELECT first_column FROM first_table ;SELECT second_column FROM first_table;+--------------+| first_column |+--------------+| 1 || 2 || NULL |+--------------+3 rows in set (0.00 sec)+---------------+| second_column |+---------------+| aaa || NULL || ccc |+---------------+3 rows in set (0.00 sec) 我们也可以用delimiter命令来自定义MySQL的检测输入结束的符号，如下： 123456789101112131415161718192021mysql&gt; delimiter $mysql&gt; SELECT first_column FROM first_table ; -&gt; SELECT second_column FROM first_table; -&gt; $+--------------+| first_column |+--------------+| 1 || 2 || NULL |+--------------+3 rows in set (0.00 sec)+---------------+| second_column |+---------------+| aaa || NULL || ccc |+---------------+3 rows in set (0.00 sec) delimiter $命令意味着修改MySQL客户端检测输入结束的符号为$,也可以使用任何符号来作为MySQL客户端检测输入结束的符号，也包括多个字符，如下： 123456789101112131415161718192021mysql&gt; delimiter 666mysql&gt; SELECT first_column FROM first_table; -&gt; SELECT second_column FROM first_table; -&gt; 666+--------------+| first_column |+--------------+| 1 || 2 || NULL |+--------------+3 rows in set (0.00 sec)+---------------+| second_column |+---------------+| aaa || NULL || ccc |+---------------+3 rows in set (0.00 sec) 存储函数创建存储函数存储函数其实就是一种函数，只不过在这个函数里可以执行命令语句而已。MySQL中定义存储函数的语句如下： 12345CREATE FUNCTION 存储函数名称([参数列表])RETURNS 返回值类型BEGIN 函数体内容END 举个🌰： 123456789mysql&gt; delimiter $mysql&gt; CREATE FUNCTION second_column(a INT) -&gt; RETURNS VARCHAR(100) -&gt; BEGIN -&gt; RETURN (SELECT second_column FROM first_table WHERE first_column = a); -&gt; END $Query OK, 0 rows affected (0.00 sec)mysql&gt; delimiter ; 存储函数的调用我们自定义的函数和系统内置函数的使用方式是一样的，都是在函数名后加小括号()表示函数调用 1234567mysql&gt; SELECT second_column(1);+------------------+| second_column(1) |+------------------+| aaa |+------------------+1 row in set (0.00 sec) 查看存储函数查看定义了多少个存储函数: 1SHOW FUNCTION STATUS [LIKE 需要匹配的函数名] 查看某个函数的具体定义: 1SHOW CREATE FUNCTION 函数名 删除存储函数删除某个存储函数 1DROP FUNCTION 函数名 在函数体中定义变量在函数体中使用变量前必须先声明这个变量，函数体中的变量名不允许加@前缀,声明方式如下： 12345678910111213DECLARE 变量名 数据类型 [DEFAULT 默认值]; mysql&gt; delimiter $mysql&gt; CREATE FUNCTION var_demo(a INT) -&gt; RETURNS INT -&gt; BEGIN -&gt; DECLARE b INT; -&gt; SET b = 5; -&gt; RETURN b+a; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; delimiter ; 我们调用一下这个函数： 1234567mysql&gt; SELECT var_demo(2);+-------------+| var_demo(2) |+-------------+| 7 |+-------------+1 row in set (0.00 sec) 如果不对声明的变量赋值，它的默认值就是NULL，也可以通过DEFAULT子句来显式的指定变量的默认值. 123456789101112131415161718mysql&gt; delimiter $mysql&gt; CREATE FUNCTION var_default_demo()-&gt; RETURNS INT-&gt; BEGIN-&gt; DECLARE c INT DEFAULT 1;-&gt; RETURN c;-&gt; END $Query OK, 0 rows affected (0.00 sec)mysql&gt; delimiter ;mysql&gt; SELECT var_default_demo();+--------------------+| var_default_demo() |+--------------------+| 1 |+--------------------+1 row in set (0.00 sec) 参数定义比如我们上边编写的这个second_column函数： 12345mysql&gt; CREATE FUNCTION second_column(a INT) -&gt; RETURNS VARCHAR(100) -&gt; BEGIN -&gt; RETURN (SELECT second_column FROM first_table WHERE first_column = a); -&gt; END $ 需要注意的是，参数名不要和函数体语句中其他的变量名、命令语句的标识符冲突。并且函数参数不可以指定默认值，我们在调用函数的时候，必须显式的指定所有的参数，参数类型也一定要匹配 判断语句语法格式如下： 1234567IF 布尔表达式 THEN 处理语句[ELSEIF 布尔表达式 THEN 处理语句][ELSE 处理语句] END IF; 举个🌰： 12345678910111213141516171819202122232425262728mysql&gt; delimiter $mysql&gt; CREATE FUNCTION condition_demo(i INT) -&gt; RETURNS VARCHAR(10) -&gt; BEGIN -&gt; DECLARE result VARCHAR(10); -&gt; IF i = 1 THEN -&gt; SET result = '结果是1'; -&gt; ELSEIF i = 2 THEN -&gt; SET result = '结果是2'; -&gt; ELSEIF i = 3 THEN -&gt; SET result = '结果是3'; -&gt; ELSE -&gt; SET result = '非法参数'; -&gt; END IF; -&gt; RETURN result; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; delimiter;mysql&gt; SELECT condition_demo(2);+-------------------+| condition_demo(2) |+-------------------+| 结果是2 |+-------------------+1 row in set (0.00 sec) 循环语句while循环语法格式如下： 123WHILE 布尔表达式 DO 循环语句END WHILE; 举个🌰： 1234567891011121314151617181920212223mysql&gt; delimiter $mysql&gt; CREATE FUNCTION sum_all(n INT UNSIGNED) -&gt; RETURNS INT -&gt; BEGIN -&gt; DECLARE result INT DEFAULT 0; -&gt; DECLARE i INT DEFAULT 1; -&gt; WHILE i &lt;= n DO -&gt; SET result = result + i; -&gt; SET i = i + 1; -&gt; END WHILE; -&gt; RETURN result; -&gt; END $Query OK, 0 rows affected (0.00 sec)mysql&gt; delimiter;mysql&gt; select sum_all(10);+-------------+| sum_all(10) |+-------------+| 55 |+-------------+1 row in set (0.00 sec) REPEAT循环语法格式如下： 123REPEAT 循环语句UNTIL 布尔表达式 END REPEAT; 举个🌰： 123456789101112131415161718192021mysql&gt; CREATE FUNCTION sum_repeat(n INT UNSIGNED) -&gt; RETURNS INT -&gt; BEGIN -&gt; DECLARE result INT DEFAULT 0; -&gt; DECLARE i INT DEFAULT 1; -&gt; REPEAT -&gt; -- 循环开始 -&gt; SET result = result + i; -&gt; SET i = i + 1; -&gt; UNTIL i &gt; n END REPEAT; -&gt; RETURN result; -&gt; END $Query OK, 0 rows affected (0.02 sec)mysql&gt; select sum_repeat(5);+---------------+| sum_repeat(5) |+---------------+| 15 |+---------------+1 row in set (0.01 sec) LOOP循环语法格式如下： 1234循环标记:LOOP 循环语句 LEAVE 循环标记;END LOOP 循环标记; 举个🌰： 12345678910111213141516171819202122mysql&gt; CREATE FUNCTION sum_loop(n INT UNSIGNED) -&gt; RETURNS INT -&gt; BEGIN -&gt; DECLARE result INT DEFAULT 0; -&gt; DECLARE i INT DEFAULT 1; -&gt; LOOP_NAME:LOOP -- 循环开始 -&gt; IF i &gt; n THEN -&gt; LEAVE LOOP_NAME; -&gt; END IF; -&gt; SET result = result + i; -&gt; SET i = i + 1; -&gt; END LOOP LOOP_NAME; -&gt; RETURN result; -&gt; END $ mysql&gt; select sum_loop(10);+--------------+| sum_loop(10) |+--------------+| 55 |+--------------+1 row in set (0.00 sec) 存储过程存储函数侧重于执行语句并返回一个值，而存储过程更侧重于单纯的去执行语句。 创建存储过程1234CREATE PROCEDURE 存储过程名称([参数列表])BEGIN 需要执行的语句END 举个🌰： 1234567mysql&gt; CREATE PROCEDURE insert_first_table(c1 INT,c2 VARCHAR(100)) -&gt; BEGIN -&gt; SELECT * FROM first_table; -&gt; INSERT INTO first_table(first_column,second_column) VALUES(c1,c2); -&gt; SELECT * FROM first_table; -&gt; END $Query OK, 0 rows affected (0.02 sec) 存储过程的调用存储函数执行语句并返回一个值，所以常用在表达式中。存储过程偏向于调用那些语句，并不能用在表达式中。我们需要显式的使用CALL语句来调用一个存储过程： 1CALL 存储过程([参数列表]); 举个🌰： 123456789101112131415161718192021mysql&gt; CALL insert_first_table(4,'test'); +--------------+---------------+| first_column | second_column |+--------------+---------------+| 1 | aaa || 2 | NULL || NULL | ccc |+--------------+---------------+3 rows in set (0.00 sec)+--------------+---------------+| first_column | second_column |+--------------+---------------+| 1 | aaa || 2 | NULL || NULL | ccc || 4 | test |+--------------+---------------+4 rows in set (0.00 sec) 查看存储过程12345查看当前数据库中创建的存储过程都有哪些的语句：SHOW PROCEDURE STATUS [LIKE 需要匹配的函数名]查看某个存储过程定义的语句：SHOW CREATE PROCEDURE 存储过程名称 删除存储过程删除某个存储过程 1DROP PROCEDURE 存储过程名称 存储过程参数类型 参数类型 实际参数是否必须是变量 作用 IN 否 用于调用者向过程传递数据，如果该参数在过程中被修改，调用者不可见 OUT 是 用于把过程产生的结果放到此参数中，过程结束后调用者可以通过该参数来获取过程执行的结果 INOUT 是 综合IN和OUT特点，即可用于调用者向过程传递数据，也可用于存放过程中产生的结果 IN123456789101112131415161718192021mysql&gt; CREATE PROCEDURE test_in(IN num INT) -&gt; BEGIN -&gt; SELECT num; -&gt; SET num = 666; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; SET @a = 111;Query OK, 0 rows affected (0.01 sec)mysql&gt; CALL test_in(@a);+------+| num |+------+| 111 |+------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) IN参数类型的变量只能用于读取，对类型的变量赋值是不会被调用者看到的。如果我们不写明参数类型的话，该参数的类型默认是IN。 OUT12345678910111213141516171819202122232425mysql&gt; CREATE PROCEDURE test_out(OUT num INT) -&gt; BEGIN -&gt; SELECT num; -&gt; SET num = 666; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; CALL test_out(@a);+------+| num |+------+| NULL |+------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT @a;+------+| @a |+------+| 666 |+------+1 row in set (0.00 sec) OUT参数类型的变量只能用于赋值，对类型的变量赋值是会被调用者看到的,因此参数就不允许是常量。 存储过程中向调用者返回多个值，举个例子： 1234567891011121314151617mysql&gt; CREATE PROCEDURE data_out(OUT a INT,OUT b INT) -&gt; BEGIN -&gt; SET a = 100; -&gt; SET b = 200; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; CALL data_out(@a,@b);Query OK, 0 rows affected (0.01 sec)mysql&gt; select @a,@b;+------+------+| @a | @b |+------+------+| 100 | 200 |+------+------+1 row in set (0.00 sec) INOUT这种类型的参数既可以在存储过程中被读取，也可以被赋值后被调用者看到，因此参数就不允许是常量。 存储过程和函数的区别 存储函数在定义时需要显式用RETURNS语句标明返回的数据类型，而且在函数体中必须使用RETURN语句来显式指定返回的值，存储过程不需要。 存储函数的参数类型只能是IN，而存储过程支持IN、OUT、INOUT三种参数类型。 存储函数只能返回一个值，而存储过程可以通过设置多个OUT类型的参数来返回多个结果。 存储函数执行过程中产生的结果集并不会被显示到客户端，而存储过程执行过程中产生的结果集会被显示到客户端。 存储函数的调用直接使用在表达式中，而存储过程只能通过CALL语句来显式调用。 游标游标（Cursor）是处理数据的一种方法，为了查看或者处理结果集中的数据，游标提供了在结果集中一次一行或者多行前进或向后浏览数据的能力。初始状态下它标记查询结果集中的第一条记录,根据这个游标取出它对应记录的信息，随后再移动游标，让它指向别的记录。 创建游标1DECLARE 游标名称 CURSOR FOR 查询语句; 举个🌰： 12345mysql&gt; CREATE PROCEDURE cursor_demo() -&gt; BEGIN -&gt; DECLARE first_table_cursor CURSOR FOR select * from first_table; -&gt; END $Query OK, 0 rows affected (0.01 sec) 打开和关闭游标123OPEN 游标名称;CLOSE 游标名称; 打开游标意味着执行查询语句，让创建好的游标与该查询语句得到的结果集关联起来，关闭游标意味着会释放该游标占用的内存，所以一旦我们使用完了游标，就要把它关闭掉。 游标获取记录1FETCH 游标名 INTO 变量1, 变量2, ... 变量n 举个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354mysql&gt; CREATE PROCEDURE cursor_demo() -&gt; BEGIN -&gt; DECLARE c1 INT; -&gt; DECLARE c2 VARCHAR(100); -&gt; DECLARE record_count INT; -&gt; DECLARE i INT DEFAULT 0; -&gt; -- 声明游标 -&gt; DECLARE first_table_cursor CURSOR FOR select * from first_table; -&gt; -&gt; -- 统计表行数 -&gt; SELECT COUNT(*) FROM first_table INTO record_count; -&gt; -&gt; -- 使用游标遍历 -&gt; OPEN first_table_cursor; -&gt; -&gt; WHILE i &lt; record_count DO -&gt; FETCH first_table_cursor INTO c1 , c2; -&gt; SELECT c1,c2; -&gt; SET i = i + 1; -&gt; END WHILE; -&gt; CLOSE first_table_cursor; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; CALL cursor_demo();+------+------+| c1 | c2 |+------+------+| 1 | aaa |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 2 | NULL |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| NULL | ccc |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 4 | test |+------+------+1 row in set (0.01 sec)Query OK, 0 rows affected (0.01 sec) i表示当前游标对应的记录位置。每调用一次 FETCH 语句，游标就移动到下一条记录的位置。 遍历结束的执行策略其实在FETCH语句获取不到记录的时候会触发一个事件，从而我们可以得知所有的记录都被获取过了，然后我们就可以去主动的停止循环。MySQL中响应这个事件的语句如下： 1DECLARE CONTINUE HANDLER FOR NOT FOUND 语句; 举个🌰，再来改写一下cursor_demo存储过程： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263mysql&gt; delimiter $mysql&gt; CREATE PROCEDURE cursor_demo() -&gt; BEGIN -&gt; -- 声明变量 -&gt; DECLARE c1 INT; -&gt; DECLARE c2 VARCHAR(100); -&gt; DECLARE not_done INT DEFAULT 1; -&gt; -&gt; -- 声明游标 -&gt; DECLARE first_table_cursor CURSOR FOR select * from first_table; -&gt; -&gt; -- 在游标遍历完记录的时候将变量 not_done 的值设置为 0，并且继续执行后边的语句 -&gt; DECLARE CONTINUE HANDLER FOR NOT FOUND SET not_done = 0; -&gt; -&gt; -- 使用游标遍历 -&gt; OPEN first_table_cursor; -&gt; -&gt; WHILE not_done = 1 DO -&gt; -&gt; FETCH first_table_cursor INTO c1 , c2; -&gt; SELECT c1,c2; -&gt; END WHILE; -&gt; CLOSE first_table_cursor; -&gt; END $Query OK, 0 rows affected (0.00 sec)mysql&gt; CALL cursor_demo();+------+------+| c1 | c2 |+------+------+| 1 | aaa |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 2 | NULL |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| NULL | ccc |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 4 | test |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 4 | test |+------+------+1 row in set (0.01 sec)Query OK, 0 rows affected (0.01 sec) 我们发现结果集中最后一条记录输出两遍怎么办呢，我们可以使用EXIT来替代上边的CONTINUE：CONTINUE表示在FETCH语句获取不到记录的时候仍然会执行之后存储过程的语句，也就是会将最后一次关联的记录中的值放入指定的变量EXIT表示在FETCH语句获取不到记录的时候仍然不会执行之后存储过程的语句 触发器存储函数与存储过程都是需要我们手动调用的，如果想在执行某条语句之前或者之后自动去调用另外一些语句，就需要用到触发器。 创建触发器触发器的定义： 12345678CREATE TRIGGER 触发器名&#123;BEFORE|AFTER&#125; &#123;INSERT|DELETE|UPDATE&#125;ON 表名FOR EACH ROW BEGIN 触发器内容END MySQL中目前只支持对INSERT、DELETE、UPDATE这三种类型的语句设置触发器。 因为触发器会对某个语句影响的所有记录依次调用我们自定义的触发器内容，所以我们需要一种访问该记录中的内容的方式，MySQL提供了NEW和OLD两个单词来分别代表新记录和旧记录，它们在不同操作中的含义不同： 对于INSERT语句设置的触发器来说，NEW代表准备插入的记录，不能使用OLD。 对于DELETE语句设置的触发器来说，OLD代表删除前的记录，不能使用NEW。 对于UPDATE语句设置的触发器来说，NEW代表修改后的记录，OLD代表修改前的记录。 举个🌰： 12345678910111213141516171819202122232425262728293031323334353637383940mysql&gt; CREATE TRIGGER test_trigger -&gt; BEFORE INSERT ON first_table -&gt; FOR EACH ROW -&gt; BEGIN -&gt; IF NEW.first_column &lt; 1 THEN -&gt; SET NEW.first_column = 1; -&gt; ELSEIF NEW.first_column &gt; 10 THEN -&gt; SET NEW.first_column = 10; -&gt; END IF; -&gt; END $Query OK, 0 rows affected (0.02 sec)mysql&gt; select * from first_table;+--------------+---------------+| first_column | second_column |+--------------+---------------+| 1 | aaa || 2 | NULL || NULL | ccc || 4 | test |+--------------+---------------+4 rows in set (0.00 sec)mysql&gt; INSERT INTO first_table(first_column,second_column) VALUES(5,'5'),(20,'20');Query OK, 2 rows affected (0.01 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from first_table;+--------------+---------------+| first_column | second_column |+--------------+---------------+| 1 | aaa || 2 | NULL || NULL | ccc || 4 | test || 5 | 5 || 10 | 20 |+--------------+---------------+6 rows in set (0.00 sec) (20,’20’)的插入结果变成了(10,’20’)说明触发器生效了。 查看触发器12345查看当前数据库中的所有触发器的语句：SHOW TRIGGERS;查看某个具体的触发器的定义：SHOW CREATE TRIGGER 触发器名; 删除触发器：1DROP TRIGGER 触发器名; 触发器使用注意事项 触发器内容中不能有输出结果集的语句。 一个表最多只能定义6个触发器分别是： BEFORE INSERT触发器 BEFORE DELETE触发器 BEFORE UPDATE触发器 AFTER INSERT触发器 AFTER DELETE触发器 AFTER UPDATE触发器 NEW中的值可以被更改，OLD中的值无法更改。 如果我们的BEFORE触发器内容执行过程中遇到了ERROR，那这个触发器对应的具体语句将无法执行；如果具体的操作语句执行过程中遇到了ERROR，那与它对应的AFTER触发器的内容将无法执行。 事件如果我们想指定某些语句在某个时间点或者每隔一个时间段执行一次的话,就需要创建一个事件。 创建事件1234567CREATE EVENT 事件名ON SCHEDULE&#123;AT 某个确定的时间点 | EVERY 期望的时间间隔 [STARTS datetime][END datetime]&#125;DOBEGIN 具体的语句END 事件支持两种类型的定时执行： 某个确定的时间点执行 1234567mysql&gt; CREATE EVENT insert_first_table -&gt; ON SCHEDULE -&gt; AT '2019-09-10 11:30:30' -&gt; DO -&gt; BEGIN -&gt; INSERT INTO first_table(first_column,second_column) VALUES(6,'6'); -&gt; END $ 除了直接填某个时间常量，也可以填写一些表达式： 1234567mysql&gt; CREATE EVENT insert_first_table -&gt; ON SCHEDULE -&gt; AT DATE_ADD(NOW(), INTERVAL 2 DAY) -&gt; DO -&gt; BEGIN -&gt; INSERT INTO first_table(first_column,second_column) VALUES(6,'6'); -&gt; END $ DATE_ADD(NOW(), INTERVAL 2 DAY)表示该事件将在当前时间的两天后执行。 每隔一段时间执行一次 1234567mysql&gt; CREATE EVENT insert_first_table -&gt; ON SCHEDULE -&gt; EVERY 1 HOUR -&gt; DO -&gt; BEGIN -&gt; INSERT INTO first_table(first_column,second_column) VALUES(6,'6'); -&gt; END $ 默认情况下，采用这种每隔一段时间执行一次的方式将从创建事件的事件开始，无限制的执行下去。我们也可以指定该事件开始执行时间和截止时间： 1234567mysql&gt; CREATE EVENT insert_first_table -&gt; ON SCHEDULE -&gt; EVERY 1 HOUR STARTS '2019-09-10 11:30:30' ENDS '2019-09-12 11:30:30' -&gt; DO -&gt; BEGIN -&gt; INSERT INTO first_table(first_column,second_column) VALUES(6,'6'); -&gt; END $ 在创建好事件之后我们就不用管了，到了指定时间，MySQL服务器会帮我们自动执行的。 查看事件12345查看当前数据库中的所有事件的语句：SHOW EVENTS;查看某个具体的事件的定义:SHOW CREATE EVENT 事件名; 删除事件1DROP EVENT 事件名; 事件使用注意事项默认情况下，MySQL服务器并不会帮助我们执行事件，除非我们在启动服务器的时候就指定了下边这个选项： 1event_scheduler = ON 如果在服务器已经启动的情况下，我们可以通过设置event_scheduler的系统变量来让MySQL服务器帮助我们执行事件，设置方式如下： 12mysql&gt; SET GLOBAL event_scheduler = ON;Query OK, 0 rows affected (0.00 sec) 错误解决在MySql中创建自定义函数报错信息如下： 1ERROR 1418 (HY000): This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you *might* want to use the less safe log_bin_trust_function_creators variable) 解决方法： 1mysql&gt; set global log_bin_trust_function_creators=1;]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-Mysql文件]]></title>
    <url>%2FMysql%2FMysql03%2F</url>
    <content type="text"><![CDATA[文件种类 参数文件：告诉MySQL实例启动时在哪里可以找到数据库文件，并且指定某些初始化参数，这些参数定义了某种内存结构的大小等设置，还会介绍各种参数的类型。 日志文件：用来记录MySQL实例对某种条件做出响应时写入的文件，如错误日志文件、二进制日志文件、慢查询日志文件、查询日志文件等。 socket文件：当用UNIX域套接字方式进行连接时需要的文件。 pid文件：MySQL实例的进程ID文件。 MySQL表结构文件：用来存放MySQL表结构定义文件。 存储引擎文件：因为MySQL表存储引擎的关系，每个存储引擎都会有自己的文件来保存各种数据。这些存储引擎真正存储了记录和索引等数据。本章主要介绍与InnoDB有关的存储引擎文件。” 参数文件参数分为两类： 动态参数：在 Mysql 实例运行中可以进行更改 静态参数：在整个实例生命周期内都不得更改 更改动态参数的语法如下： 123456789101112131415SET| [global | session] system_var_name=expr| [@@global. | @@session. | @@] system_var_name = expr# 改变当前会话，不会改变全局SET read_buffer_size = 524288# 改变全局会话参数，不会改变当前SET @@global.read_buffer_size = 1048576;# 查询当前会话参数SELECT @@session.read_buffer_size;# 查询全局会话参数SELECT @@global.read_buffer_size; ​global：全局的，session：当前会话。这种修改，并不最终修改配置文件my.cnf的参数值，所以重新启动后，参数还是按照配置文件中的加载。 日志文件MySQL中常见的日志文件有： 错误日志（error log）：对MySQL的启动、运行、关闭过程进行记录错误信息、警告信息。 慢查询日志（slow query log） 二进制日志（bin log） 查询日志（log） 慢查询日志在MySQL启动时设一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询日志文件中。 参数 作用 set global log_slow_queries = on; 开启慢查询命令，默认启动慢查询 set global long_query_time = 1; 设置慢查询时间超过1s即被认为慢查询，默认10s set global log_queries_not_using_indeces = on; 如果SQL语句没有使用索引，会记录到慢查询中 set global log_throttle_queries_not_using_indexs = on; 设置每分钟允许记录到slow log的且未使用索引的SQL语句次数，默认为0，表示没有限制。 套接字文件pid文件表结构定义文件innoDB存储引擎文件参考 MySQL技术内幕：InnoDB存储引擎(第2版) https://www.jianshu.com/p/c1ffd6956e6a https://www.cnblogs.com/BlueMountain-HaggenDazs/p/9297883.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-LSN]]></title>
    <url>%2FMysql%2FMysql-LSN%2F</url>
    <content type="text"><![CDATA[LSN(log sequence number)——日志序列号：递增产生，表示事务写入重做日志的字节总量，占用8个字节。 LSN存在什么地方？有什么含义？1234567---LOG---Log sequence number 15151135824 -- redo log buffer 的 lsn，存放在redo log buffer 中称： redo_mem_lsnLog flushed up to 15151135824 -- redo log file 的 lsn，存放在redo log 中称： redo_log_lsnPages flushed up to 15151135824 -- 最后一个刷到磁盘上的页的最新的 lsn Last checkpoint at 15151135815 -- 共享表空间上的日志记录点，最后一次检查点，及崩溃恢复时指定的起点 , checkpoint 所在的 lsn, 存放在redo log第一个文件的头部，称： cp_lsn LSN 有什么用？主要用于MySQL重启恢复 恢复的算法假设： redo_log_lsn = 15000 , cp_lsn=10000 , 这时候MySQL crash了，重启后的恢复流程如下： cp_lsn = 10000 之前的redo 日志，不需要恢复： 因为checkpoint之前的日志已经可以确保刷新完毕 那么 10000 &lt;= redo_log_LSN &lt;= 15000 的日志需要结合page_lsn判断，哪些需要重做，哪些不需要重做。 redo_log_LSN 日志里面记录的page 操作，如果redo_log_LSN &lt;= page_lsn , 这些日志不需要重做，因为page已经是最新的 redo_log_LSN 日志里面记录的page 操作, 如果redo_log_LSN &gt;= page_lsn , 这些日志是需要应用到page 里面去的，这一系列操作我们称为恢复.]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-Redo与Undo]]></title>
    <url>%2FMysql%2FMysql-RedoAndUndo%2F</url>
    <content type="text"><![CDATA[redo logredo 概念重做日志(redo log)：在InnoDB存储引擎中，大部分情况下 Redo 是物理日志，记录的是数据页的物理变化。 redo 结构Redo log可以简单分为以下两个部分： 重做日志缓冲 (redo log buffer),是易失的，在内存中 日志会先写到redo log buffer ，根据制定条件刷新到redo log file 由log block组成 每个log block 512字节，所以不需要 double write，因为每次刷新都是原子的 重做日志文件 (redo log file)，是持久的，保存在磁盘中 redo log的物理文件，一般有2个,大小可配置 redo 写入时机 在数据页修改完成之后，在脏页刷出磁盘之前，写入redo日志。注意的是先修改数据，后写日志 redo日志比数据页先写回磁盘 聚集索引、非聚集索引、undo页面的修改，均需要记录Redo日志。 redo 的整体流程 redo如何保证事务的持久性？InnoDB 通过 Force Log at Commit 机制实现事务的持久性，即当事务提交时，先将 redo log buffer 写入到 redo log file 进行持久化，待事务的commit操作完成时才算完成。这种做法也被称为 Write-Ahead Log(预先日志持久化)，在持久化一个数据页之前，先将内存中相应的日志页持久化。 为了保证每次日志都写入redo log file，在每次将redo buffer写入redo log file之后，默认情况下，InnoDB存储引擎都需要调用一次 fsync操作,因为重做日志打开并没有 O_DIRECT选项，所以重做日志先写入到文件系统缓存。为了确保重做日志写入到磁盘，必须进行一次 fsync操作。fsync操作 将数据提交到硬盘中，强制硬盘同步，将一直阻塞到写入硬盘完成后返回，大量进行fsync操作就有性能瓶颈，因此磁盘的性能也影响了事务提交的性能，也就是数据库的性能。(O_DIRECT选项是在Linux系统中的选项，使用该选项后，对文件进行直接IO操作，不经过文件系统缓存，直接写入磁盘) undo logundo 概念undo log主要记录的是数据的逻辑变化，为了在发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚。 undo 结构在InnoDB存储引擎中，undo存储在回滚段(Rollback Segment)中,每个回滚段记录了1024个undo log segment，而在每个undo log segment段中进行undo 页的申请，在5.6以前，Rollback Segment是在共享表空间里的，5.6.3之后，可通过 innodb_undo_tablespace设置undo存储的位置。 undo 写入时机 DML操作修改聚集索引前，记录undo日志 非聚集索引记录的修改，不记录undo日志 undo 的整体流程undo log 采用顺序IO写入磁盘共享表空间。 undo 类型 insert undo log：在insert 操作中产生的undo log，因为insert操作的记录，只对事务本身可见，对其他事务不可见。故该undo log可以在事务提交后直接删除，不需要进行purge操作。 update undo log：在delete 和update操作产生的undo log，该undo log可能需要提供MVCC机制，因此不能再事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除。 ## DML的相关物理实现算法 * 主键索引 12341. 对于delete --需要undo绑定该记录才能进行回滚，所以只能打上标记，delete mark 2. 对于update --原记录可以物理删除，因为可以在新插入进来的地方进行undo绑定 * 如果不能原地更新： delete(注意：这里是直接delete,而不是delete mark) + insert * 如果可以原地更新，那么直接update就好 非聚集索12341. 对于delete --不能直接被物理删除，因为二级索引没有undo，只能通过打标记，然后回滚。否则如果被物理删除，则无法回滚 delete mark 2. 对于update --不能直接被物理删除，因为二级索引没有undo，只能通过打标记，然后回滚。否则如果被物理删除，则无法回滚 delete mark + insert redo &amp; undoundo log 是否是 redo log 的逆过程？undo log是逻辑日志，对事务回滚时，只是将数据库逻辑地恢复到原来的样子。redo log是物理日志，记录的是数据页的物理变化，显然undo log不是redo log的逆过程。 事务实现过程事务B要将字段A的值由原来的1修改为3，要将B的值由原来的2修改为4，redo日志记录的是： 1234567891011假设有A、B两个数据，值分别为1,2.1. 事务B开始2. 记录A=1到undo log3. 修改A=34. 记录A=3到 redo log5. 记录B=2到 undo log6. 修改B=47. 记录B=4到redo log8. 将redo log写入磁盘9. 事务提交，将数据写入磁盘10.事物B结束 在insert/update/delete操作中，redo和undo分别记录的内容都不一样，量也不一样。在InnoDB内存中，一般的顺序如下： 写undo的redo 写undo 修改数据页 写Redo 如果上面事务B回滚（当做新的事务C），则redo记录的是： 1234567891011121314151. 事务C开始2. 记录A=1到undo log3. 修改A=34. 记录A=3到 redo log5. 记录B=2到 undo log6. 修改B=47. 记录B=4到redo log &lt;!--回滚--&gt;8. 修改B=29. 记录B=2到redo log10.修改A=111.记录A=1到redo log12.将redo log写入磁盘13.事务提交，将数据写入磁盘14.事物C结束 恢复策略：恢复时，先根据redo重做所有事务（包括未提交和回滚了的），再根据undo回滚未提交的事务。当系统发生宕机时，如果一个事务的 redo log 已经全部刷入磁盘，那么该事务一定可以恢复；如果一个事务的 redo log 没有全部刷入磁盘，那么就通过 undo log 将这个事务恢复到执行之前。 如上，如果事务B异常未提交事务就宕机，恢复时，先根据redo日志将数据恢复为A=3&amp;B=4，然后根据undo记录的A=1&amp;B=2将数据恢复如初。 # 参考： * https://keithlan.github.io/2017/06/12/innodb_locks_redo/ * https://juejin.im/post/5c3c5c0451882525487c498d * https://t.hao0.me/mysql/2016/11/05/mysql-innodb-05-tablespaces.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-InnoDB存储引擎]]></title>
    <url>%2FMysql%2FMysql02%2F</url>
    <content type="text"><![CDATA[概述 InnoDB存储引擎最早由Innobase Oy公司开发，被包括在MySQL数据库所有的二进制发行版本中， 从MySQL 5.5版本开始是默认的表存储引擎（之前的版本InnoDB存储引擎仅在Windows下为默认的存储引擎） 第一个完整支持ACID事务的MySQL存储引擎（BDB是第一个支持事务的MySQL存储引擎，现在已经停止开发） 特点：行锁设计、支持 MVCC、支持外键、提供一致性非锁定读、有效利用内存和 CPU 体系架构InnoDB存储引擎是由内存池、后台线程、磁盘存储三大部分组成。 线程InnoDB 使用的是多线程模型, 其后台有多个不同的线程负责处理不同的任务 Master ThreadMaster Thread是最核心的一个后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性。包括脏页刷新、合并插入缓冲、UNDO页的回收等。 IO Thread在 InnoDB 存储引擎中大量使用了异步IO(Async IO)来处理写IO请求, IO Thread的工作主要是负责这些 IO 请求的回调。 InnoDB 版本 线程 1.0之前 4 个 io thread：write，read，insert buffer，log IO Thread. 在Linux下，IO Thread的数量不能进行调整 在Windows下可以通过参数 innodb_file_io_threads 来增大IO Thread 1.0之后 read 和 write IO thread 分别增大到了 4 个 分别使用 innodb_read_io_threads 和 innodb_write_io_threads 设置线程数 Purge Thread事务提交后，其所使用的undo log可能不再需要，因此需要Purge Thread来回收已经分配并使用的UNDO页。 InnoDB 版本 作用 1.1之前 purge 操作在 master thread 内完成 1.1之后 purge 可以独立到单独的线程,减轻 master thread 工作,提高 cpu 利用率和提高性能 MySQL数据库的配置文件[mysqld]中添加如下命令来启用独立的Purge Thread： innodb_purge_threads=1 1.1版本中，即使将 innodb_purge_threads 设为大于1，InnoDB存储引擎启动时也会将其设为1 1.2之后 支持多个Purge Thread, 这样做可以加快UNDO页的回收，也能更进一步利用磁盘的随机读取性能 Page Cleaner ThreadPage Cleaner Thread的作用是取代Master Thread中脏页刷新的操作，减轻原Master Thread的工作及对于用户查询线程的阻塞，进一步提高性能。 内存innoDB内存主要由缓冲池(innodb buffer pool)、重做日志缓冲(redo log buffer)、额外内存池组成(innodb additional men pool size)组成 缓冲池缓冲池是主存储器中的一个区域，用于在访问时缓存表和索引数据。缓冲池允许直接从内存处理常用数据，从而加快处理速度。在专用服务器上，通常会将最多80％的物理内存分配给缓冲池。读取流程： 更新流程： 因此缓冲池的大小影响数据库的整体性能。 由于32位操作系统的限制，在该系统下最多将该值设置为3G。用户可以打开操作系统的PAE选项来获得32位操作系统下最大64GB内存的支持。为了让数据库使用更多的内存,建议数据库系统都采用 64 位操作系统。 参数 版本 作用 innodb_buffer_pool_instances 从InnoDB 1.0.x开始 配置多个缓冲池实例，默认为1 缓冲池中缓存的数据页类型 索引页(index page)：缓存数据表索引 数据页(data page)：缓存数据页，占缓冲池的绝大部分 undo页(undo Log Page)：undo页是保存事务，为回滚做准备的。 插入缓冲（insert buffer）：插入数据时要先插入到缓存池中。 自适应哈希索引（adaptive hash index）： 除了B+ Tree索引外，在缓冲池还会维护一个哈希索引，以便在缓冲池中快速找到数据页。 InnoDB存储的锁信息（lock info） 数据字典信息（data dictionary） 在MySQL中，数据字典信息内容就包括表结构、数据库名或表名、字段的数据类型、视图、索引、表字段信息、存储过程、触发器等内容。 InnoDB有自己的表缓存，可以称为表定义缓存或者数据字典。当InnoDB打开一张表，就增加一个对应的对象到数据字典。 缓冲池管理方式 Free list当数据库刚启动时，LRU列表是空的，这时页都存放在Free list中。当需要从缓冲池中分页时，从Free list中查找是否有可用的空闲页，若有则将该页从Free列表中删除，放入到LRU列表中,维持页数守恒。 LRU list LRU算法：最频繁使用页在LRU列表的前端，最少使用的页在尾端。首先释放LRU列表中的尾端的页。缓冲池中页的大小默认为16KB。 InnoDB优化的LRU算法(midpoint insertion strategy)：将新读取到的页不放在首部，而是中间部位 midpoint 位置。目标是确保频繁访问”热”页面保留在缓冲池中。 参数 作用 innodb_old_blocks_pct 控制LRU列表中 old list 的百分比。 默认值为 37，对应于原始固定比率3/8。 值范围是 5（缓冲池中的新页面很快就会老化）到 95。 innodb_old_blocks_time 指定第一次访问页面之后的时间窗口（ms） 在此期间可以访问该页面而不移动到LRU列表的前端 默认值为 1000 ms 默认情况下，算法操作如下： 在默认配置下， midpoint位置在LRU list 的5/8处。 midpoint是new sublist的尾部与old sublist的头部相交的边界。 当 InnoDB 将页面读入缓冲池时，将页插入midpoint位置(old sublist的头部)。 访问old sublist中的页 &amp;&amp; 该页在old sublist中的停留时间超过innodb_old_blocks_time设置的时间，使其变young,将其移动到缓冲池的头部(new sublist的头部)。 当页从LRU列表的old部分加入到new部分时，称此时发生的操作为page made young，而因为innodb_old_blocks_time的设置而导致页没有从old部分移动到new部分的操作称为page not made young 在数据库操作中，被访问的页将移到new sublist的表头，这样一来，在new sublist中的未被访问的节点将逐渐往表尾移动，当移动过中点，将变为old list的节点。当表满时，old list末尾的页将会被移除。 为什么不采用朴素的LRU？因为某些SQL操作会访问很多页，甚至全部页，但仅仅在该次查询操作，并不是活跃的热点数据。可能会使缓冲池中的页被刷新出，从而影响缓冲池的效率。 Flush list在LRU类表的页被修改后，称为脏页（Dirty Page），即缓存和硬盘的页数据不一致。数据库会通过CHECKPOINT机制将脏页刷新回磁盘，Flush list中的页即为脏页列表。 重做日志缓冲 什么是redo log？当数据库对数据做修改的时候，需要把数据页从磁盘读到buffer pool中，然后在buffer pool中进行修改，那么这个时候buffer pool中的数据页就与磁盘上的数据页内容不一致，称buffer pool的数据页为dirty page 脏数据。如果发生非正常的DB服务重启，那么这些数据并没有同步到磁盘文件中（注意，同步到磁盘文件是个随机IO），会发生数据丢失。如果这个时候，能够有一个文件，当缓冲池中的data page变更结束后，把相应修改记录记录到这个文件（注意，记录日志是顺序IO），那么当DB服务发生crash的情况，恢复DB的时候，也可以根据这个文件的记录内容，重新应用到磁盘文件，数据保持一致。这个文件就是redo log ，用于记录 数据修改后的记录，顺序记录。什么是undo log？undo日志用于存放数据修改被修改前的值。假设修改表中 id=1 的行数据，把Name=’B’ 修改为Name = ‘B2’ ，那么undo日志就会用来存放Name=’B’的记录，如果这个修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。 重做日志缓冲不需要设置很大，通常情况下8M能满足大部分的应用场景。重做日志支持以下三种情况触发刷新： Master Thread每一秒将重做日志缓冲刷新到重做日志文件 每次事务提交时将重做日志缓冲刷新到重做日志文件 当重做日志缓冲池剩余空间小于1/2时，重做日志缓冲刷新到重做日志文件 额外的内存池 在InnoDB存储引擎中，对内存的管理是通过一种称为内存堆的方式进行的。在对一些数据结构本身的内存进行分配时，需要从额外的内存池中进行申请，当该区域的内存不够时，会从缓冲池中进行申请。 Checkpoint技术什么是Checkpoint？是一个数据库事件(event)，这个事件激活以后会触发数据库写进程(DBWR)将脏数据块写到磁盘中。 为什么需要Checkpoint技术？innoDB在事务提交时，先写重做日志，再修改内存数据这样，就产生了脏页。既然有重做日志保证数据持久性，查询时也可以从缓冲池页中取数据，那为什么还要刷新脏页到磁盘呢？如果重做日志可以无限增大，同时缓冲池足够大，能够缓存所有数据，那么是不需要将缓冲池中的脏页刷新到磁盘。但是，会有以下几个问题：1) 服务器内存有限，缓冲池不够用，无法缓存全部数据2) 重做日志无限增大成本要求太高3) 宕机时如果重做全部日志恢复时间过长 Checkpoint 解决了什么问题？1) 缩短短数据库的恢复时间2) 缓冲池不够时，将脏页刷新到磁盘3) 重做日志不可用时，刷新脏页 对于InnoDB存储引擎而言，其是通过LSN（Log Sequence Number）来标记版本的。每个页有LSN，重做日志中也有LSN，Checkpoint也有LSN。 innodb 内部有两种 checkpoint： sharp checkpoint：数据库关闭的时候将所有的脏页刷回到磁盘，默认方式，参数 innodb_fast_shudown=1 fuzzy checkpoint：只刷新部分脏页 master thread checkpoint：master thread 异步的以每秒或者每 10 秒的速度从缓冲池的脏页列表中刷新一定比列的也回磁盘 周期性，异步，读取flush list，找到脏页，写入磁盘 flush_lru_list checkpoint：InnoDB要保证LRU列表中需要有差不多100个空闲页可供使用。如果没有这么多，就会将 lru list 尾部的页移除。如果这些页有脏页，就需要进行 checkpoint。 innodb 1.1.x版本之前，检查在用户查询线程中,会阻塞用户查询操作。 innodb 1.2.x版本之后，检查放到了单独的 page cleaner 线程中,可通过 innodb_lru_scan_depth 控制lru列表中可用页的数量，默认是1024。 async/sync flush checkpoint：重做日志文件不可用时，强制将一些页刷新到磁盘。达到重做日志文件的大小阈值。 checkpoint age = redo_log_lsn - cp_lsn 低水位=75% * total_redo_log_file_size 高水位=90% * total_redo_log_file_size checkpoint age &lt; 低水位 不需要刷新 低水位 &lt;= checkpoint age &lt;= 高水位 会强制进行 checkpoint ，触发async flush， 根据flush_list的顺序，刷新足够多的脏页，直到 checkpoint age &lt; 低水位 checkpoint age &gt; 高水位 会强制进行 checkpoint ，触发sync flush 根据flush_list的顺序，刷新脏页, 直到 checkpoint age &lt; 低水位 dirty page too much checkpoint：当缓冲池中脏页的数量占据一定百分比时，强制进行Checkpoint，用来保证缓冲池中有足够的页，通过 innodb_max_dirty_pages_pct 参数控制。 -------- 第二部分 -------- Master thread 工作方式InnoDB 1.0.x 版本之前的 Master threadMaster thread 内部有多个循环 loop 组成： 主循环 loop 后台循环 backgroup loop 刷新循环 flush loop 暂停循环 suspend loop 伪代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364void master_thread()&#123; goto loop; //主循环 loop ： for(int i = 0; i &lt; 10; ++i)&#123; thread_sleep(1); //1. 日志缓冲刷新到磁盘，即使事务还没有提交 do log buffer flush to disk; //2. 根据前一秒IO操作小于5，合并插入缓冲 if(last_one_second_ios &lt; 5) do merge at most 5 insert buffer; //3. 脏页的比例超过了阈值，刷新 100 个脏页到磁盘 if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) do buffer pool flush 100 dirty page; //4. 没有用户活动（数据库空闲时）或者数据库关闭（shutdown），切换到 backgroup loop if(no user activity) goto backgroud loop; &#125; //1. 前10秒IO操作小于200，刷新 100 个脏页到磁盘 if(last_ten_second_ios &lt; 200) do buffer pool flush 100 dirty page; //2. 合并至多 5 个插入缓冲 do merge at most 5 insert buffer; //3. 将重做日志刷新到磁盘 do log buffer flush to disk; //4. 删除无用的 undo 页（每次最多尝试回收 20 个 undo 页） do full purge; //5. 脏页比例超过 70% 刷新100 个脏页到磁盘，否则刷新 10 个脏页 if ( buf_get_modified_ratio_pct ＞ 70 % ) do buffer pool flush 100 dirty page else buffer pool flush 10 dirty page goto loop //后台循环 background loop : //1. 删除无用的 undo 页 do full purge //2. 合并 20 个插入缓冲 do merge 20 insert buffer //3.如果有任务，跳转到主循环，否则跳转到刷新循环 if not idle goto loop else goto flush loop //刷新循环 flush loop : //不断刷新100个脏页，直到脏页比例没有超过阈值 do buffer pool flush 100 dirty page if ( buf_get_modified_ratio_pct ＞ innodb_max_dirty_pages_pct ) goto flush loop //没有任务，跳转到暂停循环 goto suspend loop //暂停循环 suspend loop : //将主线程挂起，等待事件发生 suspend_thread() waiting event goto loop;&#125; InnoDB 1.2.x 版本之前的 Master thread 提高刷新脏页数量和合并插入数量，改善磁盘 IO 处理能力,刷新数量不再硬编码，而是使用百分比控制。 在合并插入缓冲的时候，合并插入缓冲的数量为 innodb_io_capacity 的 5% 在从缓冲区刷新脏页的时候，刷新脏页的数量为 innodb_io_capacity 增加了自适应刷新脏页功能。 1.0.x之前版本：脏页在缓冲池占比小于innodb_max_dirty_pages_pct，不刷新脏页，大于则刷新100个脏页 1.0.x版本开始：引入innodb_adaptive_flushing参数，通过函数buf_flush_get_desired_flush_rate判断产生重做日志的速度来决定最适合的刷新脏页数量。 full purge回收的Undo页的数量也不再硬编码，使用参数innodb_purge_batch_size控制。 参数 InnoDB 版本 作用 innodb_io_capacity 1.0.x开始 表示磁盘IO的吞吐量,默认值是200 innodb_max_dirty_pages_pct 1.0.x之前 脏页在缓冲池中所占比率，默认值是90 1.0.x开始 默认值是75加快刷新脏页的频率，保证了磁盘IO的负载。 innodb_adaptive_flushing 1.0.x开始 是否自适应刷新脏页，默认为 ON innodb_purge_batch_size 1.0.x开始 清除 undo 页时,表示一次删除多少页,默认是 20 Master Thread的伪代码变为了下面的形式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465void master_thread()&#123; goto loop; //主循环 loop ： for(int i = 0; i &lt; 10; ++i)&#123; thread_sleep(1); //1. 日志缓冲刷新到磁盘，即使事务还没有提交 do log buffer flush to disk; //2. 根据前一秒IO操作小于5%innodb_io_capacity，合并插入缓冲 if(last_one_second_ios &lt; 5%innodb_io_capacity) do merge 5%innodb_io_capacity insert buffer; //3. 脏页的比例超过了阈值，刷新 100%innodb_io_capacity 个脏页到磁盘 if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) do buffer pool flush 100%innodb_io_capacity dirty page; //4. 没有用户活动（数据库空闲时）或者数据库关闭（shutdown），切换到 backgroup loop if(no user activity) goto backgroud loop; &#125; //1. 前10秒IO操作小于innodb_io_capacity，刷新 innodb_io_capacity 个脏页到磁盘 if(last_ten_second_ios &lt; innodb_io_capacity) do buffer pool flush 100%innodb_io_capacity dirty page; //2. 合并至多 5%innodb_io_capacity 个插入缓冲 do merge at most 5%innodb_io_capacity insert buffer; //3. 将重做日志刷新到磁盘 do log buffer flush to disk; //4. 删除无用的 undo 页（每次最多尝试回收 5%innodb_io_capacity 个 undo 页） do full purge; //5. 脏页比例超过 70% 刷新 100%innodb_io_capacity 个脏页到磁盘， // 否则刷新 10%innodb_io_capacity 个脏页 if ( buf_get_modified_ratio_pct ＞ 70 % ) do buffer pool flush 100%innodb_io_capacity dirty page else buffer pool flush 10%innodb_io_capacity dirty page goto loop //后台循环 background loop : //1. 删除无用的 undo 页 do full purge //2. 合并 100%innodb_io_capacity 个插入缓冲 do merge 100%innodb_io_capacity insert buffer //3.如果有任务，跳转到主循环，否则跳转到刷新循环 if not idle goto loop else goto flush loop //刷新循环 flush loop : //不断刷新 100%innodb_io_capacity 个脏页，直到脏页比例没有超过阈值 do buffer pool flush 100%innodb_io_capacity dirty page if ( buf_get_modified_ratio_pct ＞ innodb_max_dirty_pages_pct ) goto flush loop //没有任务，跳转到暂停循环 goto suspend loop //暂停循环 suspend loop : //将主线程挂起，等待事件发生 suspend_thread() waiting event goto loop;&#125; InnoDB 1.2.x 版本的 Master threadInnoDB 1.2.x 版本中再次对 Master Thread 进行了优化，伪代码如下： 123456if InnoDB is idle//之前版本中每10秒的操作srv_master_do_idle_tasks();else//之前版本中每秒的操作srv_master_do_active_tasks(); 对于刷新脏页的操作，从Master Thread线程分离到一个单独的Page Cleaner Thread，从而减轻了Master Thread的工作，同时进一步提高了系统的并发性。 InnoDB 关键特性关键特性包括： 插入缓冲 insert buffer 两次写 double write 自适应哈希索引 adaptive hash index 异步 io async io 刷新邻接页 flush neighbor page 插入缓冲聚集索引（一级索引）表在存储的时候按照主键排序进行存储，不需要磁盘的随机读取，插入效率高。非叶子节点存放的是键值，叶子节点存放的是行数据，称之为数据页。 辅助索引（二级索引）除了聚集索引之外的索引都可以称之为辅助索引，叶子节点中存放的是主键的键值。叶子节点的插入不再有序，这时就需要离散访问非聚集索引页，插入性能变低。一张表可以存在多个辅助索引，但是只能有一个聚集索引，通过辅助索引来查找对应的航记录的话，需要进行两步，第一步通过辅助索引来确定对应的主键，第二步通过相应的主键值在聚集索引中查询到对应的行记录，也就是进行两次B+树搜索。 索引数据页的更新（针对二级索引）表数据更新的同时也会更新对应的表的索引数据，所以：对表进行insert delete update时，很可能会产生大量的物理读(物理读索引数据页) 1. insert bufferInsert Buffer的使用流程： 插入缓冲的启用需要满足以下两个条件：1）索引是辅助索引（secondary index）2）索引不是唯一的：整个索引数据被切分为2部分，无法保证唯一性。 insert buffer结构insert buffer的数据结构是B+树，全局只有一颗B+树。B+树的非叶子节点是Search key，构造结构为(space,marker,offset)。 space：待插入记录所在表的表空间id。每个表都有唯一的表空间id，通过表空间id可以查出是哪张表。 marker：兼容之前的版本。 offset：在表空间中页的偏移量。 当一个辅助索引要插入到(space,offset)中时，如果该页不在缓冲池中，则按上述规则构造一个search key，将该记录插入到insert buffer中。但是如果该页一直在insert buffer中，不断有记录插入到同一个索引页中，那么该索引页的空间就会逐渐缩小，要出现B+树节点的分裂情况，这时就不能进行insert buffer了。所以，我们需要一个机制来管理每个页面的剩余空闲空间，这就是Insert buffer bitmap。每隔page_size个页面，就是一个Insert buffer bitmap page。例如：若page_size = 16384(16k)，那么page_no为0，16384，32768，…的page，就是Insert buffer bitmap page，Bitmap page的功能，就是管理其后连续的page_size – 1个page的空间使用率。每个辅助索引页在Insert buffer bitmap中占用4bit。 merge insert buffer 发生条件 辅助索引页被读取到buffer pool中：正常的select查询操作，索引页被调入内存，该索引页对应在insert buffer中的索引更改记录就会发生merge操作。 Insert buffer bitmap page追踪到该索引页无可用空间时。 Master Thread。 insert buffer 刷新到磁盘条件 有一个后台线程，会认为数据库空闲时； 数据库缓冲池不够用时； 数据库正常关闭时； redo log写满时：几乎不会出现redo log写满，此时整个数据库处于无法写入的不可用状态 插入缓冲主要带来如下两个坏处1）可能导致数据库宕机后实例恢复时间变长。如果应用程序执行大量的插入和更新操作，且涉及非唯一的聚集索引，一旦出现宕机，这时就有大量内存中的插入缓冲区数据没有合并至索引页中，导致实例恢复时间会很长。2）在写密集的情况下，插入缓冲会占用过多的缓冲池内存，默认情况下最大可以占用1/2，这在实际应用中会带来一定的问题。 2. change bufferInnoDB从1.0.x版本开始引入了Change Buffer，可以将其视为Insert Buffer的升级。从这个版本开始，InnoDB可以对DML操作——Insert、Delete、Update(delete+insert)都进行缓冲，它们分别是：Insert Buffer, Delete Buffer,Purge Buffer。对一个记录进行 update 操作有两个过程 将记录标记为删除：delete buffer 将记录真正删除：pruge buffer 参数 InnoDB 版本 作用 innodb_change_buffering 1.0.x开始 用来开启各种Buffer选项，默认值是all inserts deletes purges changes：开启 inserts 和 deletes all：都开启 none：都不开启 innodb_change_buffer_max_size 1.2.x开始 用来控制change buffer最大使用内存数量默认值为25,表示最多使用1/4的缓存池空间该参数最大有效值是50 两次写提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。 脏页刷新到磁盘风险IO的最小单位： 数据库IO的最小单位是16K（MySQL默认，oracle是8K） 文件系统IO的最小单位是4K（也有1K的） 磁盘IO的最小单位是512字节 因此，存在IO写入导致page损坏的风险： 提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。 Double write解决了什么问题一个数据页的大小是16K，假设在把内存中的脏页写到数据库的时候，写了8K突然宕机了，也就是说前8K数据是新的，后8K是旧的，那么磁盘数据库这个数据页就是不完整的，是一个坏掉的数据页，这种情况被称为部分写失效 那么可不可以通过 redo log 来进行恢复呢？redo记录的是对页的修改，只能恢复校验完整（还没写）的页，不能修复坏掉的数据页，所以这个数据就丢失了，可能会造成数据不一致，所以需要double write。 为什么 redo log 不需要 doublewrite 的支持？因为 redo log 写入的单位就是 512 字节，也就是磁盘 IO 的最小单位，所以无所谓数据损坏。 两次写工作流程doublewrite由两部分组成，一部分为内存中的doublewrite buffer，其大小为2MB，另一部分是磁盘上共享表空间(ibdata x)中连续的128个页，即2个区(extent)，大小也是2M。 当一系列机制触发数据缓冲池中的脏页刷新时，并不直接写入磁盘数据文件中，而是先拷贝至内存中的doublewrite buffer中； 接着从两次写缓冲区分两次写入磁盘共享表空间中(连续存储，顺序写，性能很高)，每次写1MB； 待第二步完成后，再将doublewrite buffer中的脏页数据写入实际的各个表空间文件(离散写)；(脏页数据固化后，即进行标记对应doublewrite数据可覆盖) 现在我们来分析一下为什么 double write 可以生效。当宕机发生时，有那么几种情况： 磁盘还未写，此时可以通过 redo log 恢复； 磁盘正在进行从内存到共享表空间的写，此时数据文件中的页还没开始被写入，因此也同样可以通过 redo log 恢复； 磁盘正在写数据文件，此时共享表空间已经写完，可以从共享表空间拷贝页的副本到数据文件实现恢复。 自适应哈希索引哈希：一次就可以定位数据 B+树：取决于树的高度，生产环境一般是 3-4 层，所以需要查询 3-4 次 自适应哈希索引 AHI（adaptive hash index）建立条件：观察到一个访问模式访问频繁，就会建立哈希索引 通过该模式访问了 100 次（模式：where x = ?） 页通过该模式访问了 N 次，其中 N = 页的记录总数⁄16 InnoDB 存储引擎官方文档显示，启用 AHI 后,读取和写入速度可以提高 2 倍，辅助索引的连接操作性能可以提高 5 倍。 异步IO为了提高磁盘的操作性能，当前的数据库系统都采用异步IO的方式处理磁盘操作。用户可以在发出一个IO请求胡立即再发出另一个IO请求，当全部IO请求发送完毕后，等待所有IO操作完成，这就是AIO。AIO的另一个优势是可以进行IO Merge操作，也就是将多个IO合并为1个IO, 这样可以提高IOPS(Input/Output Per Second)的性能。 例如：用户访问页的（space, page_no)为(8,6) (8,7) (8,8)，每个页的大小为16KB，同步IO需要3次IO操作。可以优化为从(8,6)开始读取48KB。 刷新临接页当刷新一个脏页时，InnoDB会检查该页所在extent的所有页，如果是脏页，一起刷新。 参数 版本 作用 innodb_flush_neighbors 1.2.x开始 控制是否启用该特性 参考： http://oohcode.com/2015/10/14/InnoDB-Key-Features/ https://chyroc.cn/posts/innodb-storage-engine-reading-1/ https://www.cnblogs.com/zhoujinyi/archive/2013/04/11/2988923.html http://huzb.me/2019/01/14/%E6%8F%92%E5%85%A5%E7%BC%93%E5%86%B2%E3%80%81%E4%B8%A4%E6%AC%A1%E5%86%99%E5%92%8C%E8%87%AA%E9%80%82%E5%BA%94%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/ https://blog.csdn.net/tanliqing2010/article/details/81509539 https://www.cnblogs.com/geaozhang/p/7341333.html https://draveness.me/mysql-innodb http://richfisher.me/blog/2017/12/18/innodb-notes/ https://www.docs4dev.com/docs/zh/mysql/5.7/reference/innodb-architecture.html#innodb%E6%9E%B6%E6%9E%84 MySQL技术内幕：InnoDB存储引擎(第2版)]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-MySql体系结构和存储引擎]]></title>
    <url>%2FMysql%2FMysql01%2F</url>
    <content type="text"><![CDATA[数据库 数据库是文件的集合，是依照某种数据模型组织起来并存放于二级存储器中的数据集合。在MySQL数据库中，数据库文件可以是frm、MYD、MYI、ibd结尾的文件。 数据库实例 数据库实例是程序，是位于用户与操作系统之间的一层数据管理软件，用户对数据库数据的任何操作，包括数据库定义、数据查询、数据维护、数据库运行控制等都是在数据库实例下进行的，应用程序只有通过数据库实例才能和数据库打交道。 MySql体系结构 从图中可以发现，MySQL由：连接池组件、管理服务和工具组件、SQL接口组件、查询分析器组件、优化器组件、缓冲（Cache）组件、插件式存储引擎和物理文件组成。MySQL数据库区别于其他数据库的最重要的一个特点就是其插件式的表存储引擎。 MySql存储引擎MySql数据库常用存储引擎：InnoDB、MyISAM、NDB、Memory(HEAP)、Archive、BDB(BerkeleyDB)、Federated、Maria等。 特性 InnoDB MyISAM NDB Memory Archive BDB 存储限制 64TB No Yes Yes No No 事务 Yes Yes 锁粒度 Row Table Row Table Row Page MVCC Yes Yes Yes B树索引 Yes Yes Yes Yes Yes 哈希索引 Yes Yes Yes 全文索引 5.6支持英文 Yes 集群索引 Yes 数据缓存 Yes Yes Yes 索引缓存 Yes Yes Yes Yes 数据压缩 Yes Yes 加密传输 Yes Yes Yes Yes Yes Yes 批量插入 相对低 高 高 高 非常高 高 内存消耗 高 低 高 中 低 低 存储空间消耗 高 低 低 N/A 非常低 低 外键支持 Yes 复制支持 Yes Yes Yes Yes Yes Yes 查询缓存 Yes Yes Yes Yes Yes Yes 备份恢复 Yes Yes Yes Yes Yes Yes 数据字典更新 Yes Yes Yes Yes Yes Yes 备份/时间点恢复 Yes Yes Yes Yes Yes Yes 集群支持 Yes]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[泰国旅游攻略]]></title>
    <url>%2F%E6%97%85%E6%B8%B8%2FThailand-Raiders%2F</url>
    <content type="text"><![CDATA[准备工作一、签证 所需资料 护照原件（无皮损，且有180天以上的有效期） 往返泰国的机票 携带20,000以上泰铢的现金或者等值货币（约3700-4000元人民币，汇率会有所波动，建议携带足够现金）过关时抽查，如没有携带足够的现金，有可能会被拒签或遣返。 1张2寸白底彩色照片（如没有带照片。可现场拍照，额外拍照费用100泰铢） 费用 1000泰铢手续费（目前落地签免费） 如没带照片，需现场拍照，加收100泰铢拍照费 只收泰铢现多，旁有兑换点，但汇率比较低，建议办理落地签，提前在国内换好所需泰铢费用。(中国银行可预约兑换泰铢，汇率约为1：5，实时汇率请咨询银行） 落地签办理流程 在机场按照指示牌找到“Visa on Arrival” （落地签）柜台 填写落地签申请表（柜台上就发放有表格，以及填写示例，最好提前打印下来填好） 递交资料（包括申请表、照片、护照、往返机票等），工作人员审核后给排队号 等叫号，上交资料和手续费，工作人员在你的护照上盖上落地签的章，病提供泰文收据等 走到“For Visa-on-Arrival Only”的柜台，入境官会再检查你的护照等资料，然后把出入境卡的入境部分撕掉，将出境卡盖章之后订在护照上，你就可以跨过边检柜台，正式进入泰国境内了 落地签的有效期是15天，即你可以在泰国逗留不超过15天的时间 9.10北京出发北京南站-&gt;天津站： C2077 20：50-21：20 二等座 54.5￥ 天津天津站-&gt;天津滨海国际机场T1航站楼 地铁二号线 45min 约3￥ 约10点到达机场 天津滨海国际机场T1航站楼 -&gt;廊曼国际机场T1航站楼SL963 23:50-03:35 +1 经济舱 9.11泰国曼谷曼谷酒店位置：455/4 Maha Chai Road, Bangkok, Krung Thep Maha Nakhon 10200, Thailand 廊曼国际机场T1航站楼 -&gt;曼谷酒店 455/4 Maha Chai Road淘宝下单接送机，或者携程下单（建议携程，航班延误可以2小时免费等）。到酒店睡觉啦啦啦啦 9.11 10:00浏览景点安排 大皇宫 3h 500泰铢 111.5￥ 长袖，长裙不能露膝盖卧佛寺 0.5h 200泰铢 44￥坐船 3泰铢郑王庙 0.5h 30泰铢 7￥考山路夜市吃吃吃 2h 9.12 泰国曼谷退房（留小费） 酒店-&gt;汽车站begin1、步行到船站 船票单次50🐷2、坐船3、下船后步行到BST车站4、坐上BST，约8站到达Ekkamai ，2号出站口出站下楼梯再向后方走100m就到了end 汽车站买票：窗口正上方写着Pattaya就可以买票了约108🐷一位上下浮动 厕所3🐷一次曼谷到芭提雅媒体7：00-20：00，每半小时一班车 车程2h 9.12 芭提雅酒店位置：88/999,Moo.10 Building B Room No.999, Pattayasaisong Rd, Muang Pattaya, Amphoe Bang Lamung, Chang Wat Chon Buri 20150 双条车路线图上车不要说话，说话会被误认为包车，10🐷/人 游玩路线芭提雅车站在North Pattaya Road路 Bus Terminal距酒店2.6公里，可以考虑步行 东芭乐园 9：00-18：00 800泰铢 178￥ 双条车 四方水上市场 9：00-20：00 蒂芬妮人妖秀 每天18：00、19：30、21：00共三场 普通800🐷，vip1000🐷、金vip1200🐷 178￥ 双条车每人10🐷 或步行 芭提雅海滩走一波 吃小吃 9.13芭提雅格兰岛一日游：网上跟团即可海上滑翔、海底漫步、摩托艇、香蕉船、浮潜。 9.14芭提雅购物中心逛一逛 到Bus Terminal汽车站买票回曼谷：窗口正上方写着Ekkamai，最晚班次晚上11点 9.14曼谷曼谷酒店位置：Ratchaprarop Road Ratchaprarop Road indraCondo/177/209 soi saengkran/ratchprarop road, Khet Ratchathewi, Krung Thep Maha Nakhon 10400, Thailand, Khet Ratchathewi, Krung Thep Maha Nakhon 10400, Thailand 9.15天津预约接机（提前两天预定） 泰国必买BIG-C零食711 必买药品 必买化妆品欧莱雅唇膏、小黑瓶ONLY 小白瓶N7冰激凌面膜 必逛华歌尔内衣Wacoal（曼谷尚泰商圈和暹罗挨着central world 三/四楼）LeeLeVI’S（暹罗广场，国内五折）everyandboy泰国本土美妆店日本松本清（central world 三楼） 泰国机场退税取货]]></content>
      <categories>
        <category>旅游</category>
      </categories>
      <tags>
        <tag>旅游攻略</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设置git忽略.idea文件]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%2FGIt-Ignore%2F</url>
    <content type="text"><![CDATA[1.将.idea目录加入ignore： 1$ echo '.idea' &gt;&gt; .gitignore 2.从git中删除idea： 1$ git rm -r --cached .idea 3.将.gitignore文件加入git： 1$ git add .gitignore 4.提交.gitignore文件，将.idea从代码仓库中忽略： 1$ git commit -m '忽略.idea文件夹' 5、Push到Git服务器： 1$ git push]]></content>
      <categories>
        <category>项目搭建</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++右值引用]]></title>
    <url>%2FC%2FC%2B%2B-02%2F</url>
    <content type="text"><![CDATA[左值与右值的区分在C++中，所有的值均被分为左值与右值之一。左值是指表达式结束后依然存在的持久化对象，右值是指表达式结束时就不再存在的临时对象。所有的具名变量或者对象都是左值，而右值不具名。有一个很简单的方法区别左值与右值，观察是否可以对表达式进行取地址，如果能，则为左值，否则为右值。 12345int func() &#123;return func;&#125;int a = 1 + 2;int b = func();int c = a + b;int d = c; 如上图所示，第2行的 a显然是左值，而(1 + 2)产生的临时变量值则为右值。同理第3行的b为左值，而func()的返回值同样是一个临时变量，为右值。第4行的c为左值，而(a+b)作为临时变量则为右值，而第5行中，d与c都为左值。套用上述的区别方法，a,b,c,d变量均可以进行取地址操作，而(1+2), func()返回值, (a+b)我们都无法取其地址。如书中记载，在C++11中，右值分为纯右值与将亡值。纯右值指的就是我们上述所描述的各类临时变量。而将亡值则是c++11新增的和右值引用相关的表达式，这样的表达式通常时将要移动的对象、T&amp;&amp;函数返回值、std::move()函数的返回值等，将亡值与纯右值均为右值，使用没有任何区别。 左值引用与右值引用在C++11前引用已经非常常见了，就是给变量取了一个别名，如下所示。 123int b = 1;int &amp;a = b;int &amp;c = 2; //编译错误! 需注意，在定义左值引用的同时，我们不能将右值绑定至左值引用上，如上面第3行代码所示。而C++的右值引用同左值引用相似，则使用 &amp;&amp; 来表示。 12345int func() &#123;return 1;&#125;int &amp;&amp; a = 1;int &amp;&amp; b = (1 + 2) * 3;int &amp;&amp; c = func();int &amp;&amp; d = a; //编译错误! 同左值引用相同，在定义右值引用的同时，我们不能将左值绑定在右值引用上，如上面第5行所示。也就是说，右值引用只能绑定右值，左值引用只能绑定左值。若希望将一个左值绑定到右值引用，则可以使用移动语义std::move()将左值转换为右值，例如: 12int a = 1;int &amp;&amp;b = std::move(a); //编译通过 但是注意，使用移动移动语义转移对象a后，a不可再次被使用!除了普通引用，还有一类我们经常使用到的引用，常量左值引用。大家可以显而易见的发现，常量左值引用是个“万能”的引用类型。它可以接受非常量左值、常量左值、右值对其进行初始化。但是在它的存活时期中，它只能是可读的。我们常在函数参数中使用到它，如下所示。 123456789101112void func(const string &amp;str) &#123; cout &lt;&lt; str &lt;&lt; endl;&#125;int main(int argc, char *argv[]) &#123; string s1("1234"); const string s2("1234"); func(s1); //非常量左值 func(s2); //常量左值 func(string("1234")); //右值 return 0;&#125; 在如上介绍中很容易可以得出一下结论(转载图片)：]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++强制类型转换]]></title>
    <url>%2FC%2FC%2B%2B-01%2F</url>
    <content type="text"><![CDATA[C语言类型转换c语言类型转换有如下两种(旧式转型) 12(T)expression //将 expression 转型为TT(expression) //将 expression 转型为T C++新式类型转换C++类型转换有如下四种 1234const_cast&lt;T&gt;(expression)dynamic_cast&lt;T&gt;(expression)reinterpret_cast&lt;T&gt;(expression)static_cast&lt;T&gt;(expression) const_cast一般用于移除对象的const与volatile。如下图所示，b可以修改a的值。但是注意，编译器会进行优化，将数字常量1替代a常量。所以cout &lt;&lt; a &lt;&lt; endl;输出为1。 123456789#include &lt;iostream&gt;using namespace std;int main(int argc, char *argv[]) &#123; const int a = 1; int* b = const_cast&lt;int*&gt; (&amp;a); *b = 2; cout &lt;&lt; a &lt;&lt; " " &lt;&lt; *(&amp;a) &lt;&lt; " " &lt;&lt; *b &lt;&lt; endl;&#125;// 输出：1 2 2 dynamic_castdynamic_cast 主要作用是将指向派生类对象的基类指针或引用，安全的转换为指向派生类对象的派生类指针或引用，并使用转换后的指针调用派生类独有的函数(非虚函数)。如果转换指针转换失败，则将返回空指针；如果转换引用失败，则将会抛出一个名为std::bad_cast的异常。在如下3种情况中转换可以成功 expression的类型与待转换类型相同。则转换必定成功。 expression的类型为待转换类型的公有派生类。(指针向上转换) expression的类型为待转换类型的公有基类时，必须满足以下两个要求，才会转换成功，否则转换失败。(指针向下转换) 当expression为指向派生类的指针或引用派生类对象的基类引用。 基类中必须包含虚函数，也就是必须具备多态性。 假设有如下两个类 123456789101112131415161718192021222324252627#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;class Base &#123;public: Base() &#123;&#125; Base(string s) : str(s) &#123;&#125; virtual void Print() &#123;cout &lt;&lt; str &lt;&lt; endl;&#125;private: string str;&#125;;class Derived : public Base &#123;public: Derived() &#123;&#125; Derived(string s, int i) : Base(s), ival(i) &#123;&#125; void Print() &#123; Base::Print(); cout &lt;&lt; ival &lt;&lt; endl; &#125; void PrintIval() &#123; cout &lt;&lt; ival &lt;&lt; endl; &#125;private: int ival;&#125;; example: 1234567891011int main(int argc, char *argv[])&#123; //基类指针指向派生类对象，基类中包含虚函数，符合向下转换规则。 Base* b = new Derived("test", 1); //使用基类指针无法调用派生类独有的函数，编译无法通过 b-&gt;PrintIval(); //类型转换至派生类指针就可以调用到派生类独有的函数 Derived* d = dynamic_cast&lt;Derived*&gt; (b); d-&gt;PrintIval(); return 0;&#125; reinterpret_castreinterpret_cast 主要的作用为允许任意长度相同的对象之间进行转换，而转换的安全性，则全部由程序员所保证，它只关注对象之间长度是否相同，长度不相同则无法通过编译。注意，reinterpret_cast无法去掉源对象的const、volatile属性。 1234567891011int main(int argc, char *argv[])&#123; char a = 1; //将a的指针转化为长整型数 long b = reinterpret_cast&lt;long&gt;(&amp;a); //将空指针转化为长整型数 long d = reinterpret_cast&lt;long&gt;(NULL); //编译报错 int 4字节，而指针8字节 int e = reinterpret_cast&lt;int&gt;(&amp;a); return 0;&#125; 需注意在32位机器上，指针为4字节，而在64位机器上，指针为8字节。 static_caststatic_cast 类似C语言强制类型转换，它可以完成如下一些转换 编译器隐式执行的类型转换，如int与float、double与char、enum与int之间的转换等。(精度大-&gt;精度小使用位截断处理) 将任意类型表达式转换为void类型，或从void*指针中找回其中的值。 基类与派生类指针或引用类型之间的转换，注意，由派生类转换至基类时(向上转换)是安全的，由基类至派生类转换时(向下转换)是非安全的。example(使用上述代码定义的类):1234567891011121314151617181920int main(int argc, char *argv[])&#123; char a = 'a'; //将 char -&gt; int int i = static_cast&lt;int&gt;(a); Base* b = new Base("123"); //将 Base 转换为 void* void* v = static_cast&lt;void*&gt;(b); //将 *void 转换为 Base b = static_cast&lt;Base*&gt;(v); Derived* d = new Derived("123", 123); //向上转换 将Derived* 转换为 Base* Base* bb = static_cast&lt;Base*&gt;(d); //向下转换 将Base* 转换为 Derived* Derived* dd = static_cast&lt;Derived*&gt;(b); //未定义的行为!非常危险!父类对象并不包含该函数，编译不会报错! dd-&gt;PrintIval();&#125; 一下内容取自effective C++ 如果可以，尽量避免转型，特别是在注重效率的代码中避免 dynamic_cast。如果有个设计需要转型的动作，试着发展无需转型的替代设计。如果转型是必要的，试着将它隐藏至某个函数背后，客户随后可以调用该函数，而不需将转型放进他们的代码内。宁可使用C++-style(新式转型)，不要使用旧式转型。前者很容易辨识出来，而且有其不同的职责。]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka-生产者]]></title>
    <url>%2FKafka%2FKafka-Producer%2F</url>
    <content type="text"><![CDATA[Kafka生产者kafka数据生产流程如图： 创建一个 ProducerRecord 对象,包括目标主题和要发送的内容 将对象序列化成字节数组 数据被传给分区： 是否指定了partition -&gt; 直接到指定分区 是否指定了key -&gt; 分区器使用该 key 进行 hash 操作，然后对 topic 对应的分区数量进行取模操作并返回一个分区。 没有指定key -&gt; 则通过先产生随机数，之后在该数上自增的方式产生一个数，并转为正数之后进行取余操作。 添加到批次，并发送 服务器收到消息后返回响应 成功 -&gt; 返回 RecordMetaData对象 失败 -&gt; 返回错误信息]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka-MacOs安装]]></title>
    <url>%2FKafka%2FKafka-Install%2F</url>
    <content type="text"><![CDATA[MacOS Docker 安装安装和镜像加速参考docker安装教程 Docker 下载Zookeeper 和 kafka 镜像123~ » docker pull zookeeper:latest~ » docker pull wurstmeister/kafka:latest~ » docker pull sheepkiller/kafka-manager 启动容器1、创建网络：由于要涉及到zookeeper和kafka之间的通信，所以我们运用docker内部容器通信机制先新建一个网络。 123~ » docker network create appd481270a05236007178e6ed0ce4b775c9d2aebb6c13bc050bb852bc46ca0b874 运行 docker network ls查看新建的网络 1234567~ » docker network ls NETWORK ID NAME DRIVER SCOPEd481270a0523 app bridge local0ab6b1467267 bridge bridge localcd08298f526b host host local86a734066770 none null local 运行docker network inspect app查看网络详细信息 12345678910111213141516171819202122232425262728293031~ » docker network inspect app [ &#123; "Name": "app", "Id": "d481270a05236007178e6ed0ce4b775c9d2aebb6c13bc050bb852bc46ca0b874", "Created": "2019-07-19T06:57:10.768655482Z", "Scope": "local", "Driver": "bridge", "EnableIPv6": false, "IPAM": &#123; "Driver": "default", "Options": &#123;&#125;, "Config": [ &#123; "Subnet": "172.18.0.0/16", "Gateway": "172.18.0.1" &#125; ] &#125;, "Internal": false, "Attachable": false, "Ingress": false, "ConfigFrom": &#123; "Network": "" &#125;, "ConfigOnly": false, "Containers": &#123;&#125;, "Options": &#123;&#125;, "Labels": &#123;&#125; &#125;] 可以看到其连接的containers为空，说明还没有容器连接进来2、创建Zookeeper容器 1~ » docker run --net=app --name zookeeper -p 2181 -t zookeeper 遇到了如下问题 12docker: Error response from daemon: Conflict. The container name "/zookeeper" is already in use by container "26ffbd391e8c6e5e90b8f593e354f80768f179741e1de35640efacc6303fdad0". You have to remove (or rename) that container to be able to reuse that name.See 'docker run --help'. docker ps -l 查看发现已经创建的zookeeper 可以使用docker rm 删除 12345~ » docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES26ffbd391e8c zookeeper "/docker-entrypoint.…" 8 minutes ago Created zookeeper~ » docker rm 26ffbd391e8c 重新执行创建命令 run，创建新容器，并为容器配置一些参数。 -t，在容器内部创建一个tty或者伪终端。 -i，允许主机终端按照容器内部的标准与其交互。 -d，后台运行容器并打印容器名称。 –name，容器名称。 -p，端口映射，参数格式为：主机物理端口:容器内部端口。 最后跟上的就是我们已经下载的镜像 3、创建Kafka容器 1234567~ » docker run --net=app --name kafka -p 9092 \--env HOST_IP=127.0.0.1 \--env KAFKA_ADVERTISED_HOST_NAME=localhost \--env KAFKA_ADVERTISED_PORT=9092 \--env KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \--link zookeeper \wurstmeister/kafka:latest -e，配置容器环境变量。 –link，链接到另一个容器，参数格式为：目标容器名称:在本容器内的别名。 这里的环境变量设置，其实是就是对即将创建的Kafka配置文件server.properties进行初始化。 4、创建kafka-manager 12345~ » docker run --net=app \--name kafka-manager \-p 9000:9000 \-e ZK_HOSTS=zookeeper:2181 \sheepkiller/kafka-manager 访问ip:9000即可 5、测试Kafka进入kafka容器 1~ » docker exec -it kafka /bin/bash 发送消息 12345bash-4.4# kafka-console-producer.sh --broker-list localhost:9092 --topic test&gt;hello&gt;AAAA&gt;BBBB&gt;hey 读取消息(需要打开另一个终端) 1234567bash-4.4# kafka-console-consumer.sh \&gt; --bootstrap-server localhost:9092 \&gt; --topic test --from-beginninghelloAAAABBBBhey 测试成功！(＾－＾)V 参考https://cloud.tencent.com/developer/news/371290]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot + MyBatis 多模块项目搭建]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%2FProject-Module%2F</url>
    <content type="text"><![CDATA[准备开发工具及系统环境 IDE：IntelliJ IDEA 2019.1 系统环境：mac OSX 项目目录结构 biz层：业务逻辑层 dao层：数据持久层，使用MB插件生成相关代码及xml common层：提供工程层面的基础工具类。 web层：请求处理层 搭建步骤 搭建父工程1、 IDEA 工具栏选择菜单 File -&gt; New -&gt; Project…2、选择Spring Initializr，Initializr默认选择Default，点击Next3、填写项目资料,点击Next4、直接点击Next5、填写name，点击Finish6、项目结构如下7、删除多余目录，只留如下结构 创建子模块8、选择项目根目录,右键-&gt;New -&gt; Module9、选择Maven，点击Next10、填写ArifactId，点击Next11、点击Finish12、同理添加其他子模块，最终项目目录结构如下图 模块间依赖关系各个子模块的依赖关系： biz层：依赖dao层，common层 dao层：不依赖 common层：不依赖 web层：依赖biz层，common层。 13、父pom文件中声明所有子模块依赖 123456789101112131415161718192021222324&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;biz&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;dao&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;web&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 14、biz层pom文件中添加dao层，common层依赖 12345678910&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;dao&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 15、web层pom文件中添加biz层，common层依赖 12345678910&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;biz&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 运行项目16、在web层pom文件中添加spring-boot-starter-web 12345&lt;!-- spring-boot --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 17、在web层创建com.example.test.demo.web包并添加入口类AppServiceApplication.java，目录结构如下入口类代码如下： 123456789101112package com.example.test.demo.web;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class AppServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(AppServiceApplication.class, args); &#125;&#125; 18、在com.example.test.demo.web包下创建controller目录添加test方法测试接口是否可以正常访问 123456789101112131415package com.example.test.demo.web.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping("demo")public class DemoController &#123; @RequestMapping("test") public String test() &#123; return "Hello World!"; &#125;&#125; 19、运行AppServiceApplication中的main方法启动项目，默认端口为8080，访问http://localhost:8080/demo/test得到如下效果 20、在biz层创建com.example.test.demo.biz包并创建DemoService接口类代码如下： 12345package com.example.test.demo.biz;public interface DemoService &#123; String test();&#125; 21、在com.example.test.demo.biz包下创建impl目录并添加DemoServiceImpl类，代码如下： 123456789101112131415package com.example.test.demo.biz.impl;import com.example.test.demo.biz.DemoService;import org.springframework.stereotype.Service;@Servicepublic class DemoServiceImpl implements DemoService &#123; @Override public String test() &#123; return "biz test"; &#125;&#125; 22、DemoController类通过@Autowired注解注入DemoService，修改DemoController的test方法，代码如下： 1234567891011121314151617181920package com.example.test.demo.web.controller;import com.example.test.demo.biz.DemoService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping("demo")public class DemoController &#123; @Autowired private DemoService demoService; @RequestMapping("test") public String test() &#123; return demoService.test(); &#125;&#125; 23、在入口类AppServiceApplication上添加@ComponentScan注解 123456789101112131415package com.example.test.demo.web;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.ComponentScan;@SpringBootApplication@ComponentScan(basePackages = &#123; "com.example.test.demo.*"&#125;)public class AppServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(AppServiceApplication.class, args); &#125;&#125; 24、更改完之后运行main方法，访问http://localhost:8080/demo/test得到如下效果 25、其他层同理验证。 集成Mybatis26、父pom文件中声明mybatis-spring-boot-starter、mysql-connector-java等依赖。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;biz&lt;/module&gt; &lt;module&gt;dao&lt;/module&gt; &lt;module&gt;common&lt;/module&gt; &lt;module&gt;web&lt;/module&gt; &lt;/modules&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;mysql-connector.version&gt;8.0.11&lt;/mysql-connector.version&gt; &lt;mybatis.version&gt;1.3.2&lt;/mybatis.version&gt; &lt;mybatis.generator.version&gt;1.3.2&lt;/mybatis.generator.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!--mybatis--&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.generator.version&#125;&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql-connector.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.generator.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;biz&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;dao&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;web&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mybatis--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql-connector.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 27、在dao层中的pom文件中添加以下依赖 1234567891011121314151617&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;configurationFile&gt;$&#123;basedir&#125;/src/main/resources/mybatis-generator.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 28、在web/src/main/resources下添加application.properties com.mysql.jdbc.Driver 是 mysql-connector-java 5中的 com.mysql.cj.jdbc.Driver 是 mysql-connector-java 6中的 1234567spring.datasource.driverClassName = com.mysql.cj.jdbc.Driverspring.datasource.url = jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF8&amp;connectTimeout=1000&amp;socketTimeout=3000spring.datasource.username = rootspring.datasource.password = qwertyuimybatis.type-aliases-package = com.example.test.demo.dao.pomybatis.mapper-locations = classpath:mapper/*.xml 29、在web/src/main/resources下添加mybatis-generator.xml配置内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;generatorConfiguration&gt; &lt;!-- 引入SpringBoot配置文件 --&gt; &lt;properties resource="application.properties"/&gt; &lt;context id="Mysql" targetRuntime="MyBatis3" defaultModelType="flat"&gt; &lt;!-- 生成的pojo，将implements Serializable--&gt; &lt;plugin type="org.mybatis.generator.plugins.SerializablePlugin"/&gt; &lt;commentGenerator&gt; &lt;!--*是否去除自动生成的注释包含时间戳 true：是 ： false:否--&gt; &lt;property name="suppressDate" value="true" /&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name="suppressAllComments" value="true" /&gt; &lt;/commentGenerator&gt; &lt;jdbcConnection driverClass="$&#123;spring.datasource.driverClassName&#125;" connectionURL="$&#123;spring.datasource.url&#125;" userId="$&#123;spring.datasource.username&#125;" password="$&#123;spring.datasource.password&#125;"&gt; &lt;/jdbcConnection&gt; &lt;!-- 生成model模型，对应的包路径，以及文件存放路径(targetProject)，targetProject可以指定具体的路径,如./src/main/java， 也可以使用“MAVEN”来自动生成，这样生成的代码会在target/generatord-source目录下 --&gt; &lt;javaModelGenerator targetPackage="com.example.test.demo.dao.po" targetProject="../dao/src/main/java" &gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name="trimStrings" value="true"/&gt; &lt;/javaModelGenerator&gt; &lt;!--对应的mapper.xml文件 --&gt; &lt;sqlMapGenerator targetPackage="mapper" targetProject="../dao/src/main/resources"&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 对应的Mapper接口类文件 --&gt; &lt;javaClientGenerator type="XMLMAPPER" targetPackage="com.example.test.demo.dao.mapper" targetProject="../dao/src/main/java"&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;/javaClientGenerator&gt; &lt;!-- 表名及对应po类名称--&gt; &lt;table tableName="user_info" domainObjectName="UserInfoPO" enableCountByExample="true" enableUpdateByExample="true" enableDeleteByExample="true" enableSelectByExample="true" selectByExampleQueryId="false"&gt; &lt;property name="useActualColumnNames" value="false"/&gt; &lt;generatedKey column="id" identity="true" sqlStatement="MySql"/&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 30、根据表自动生成对应的Mapper以及po类,步骤如下。得到目录如下 30、biz层下DemoServiceImpl通过@Autowired注解注入UserMapper，修改DemoService的test方法如下： 12345678910111213141516171819202122package com.example.test.demo.biz.impl;import com.example.test.demo.biz.DemoService;import com.example.test.demo.dao.mapper.UserInfoPOMapper;import com.example.test.demo.dao.po.UserInfoPO;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class DemoServiceImpl implements DemoService &#123; @Autowired private UserInfoPOMapper userInfoPOMapper; @Override public String test() &#123; UserInfoPO po = userInfoPOMapper.selectByPrimaryKey(1L); return "UserInfo name is:" + po.getName(); &#125;&#125; 31、在入口类AppServiceApplication上中添加注解 1@MapperScan("com.example.test.demo.dao.mapper") 32、运行main方法启动项目 遇到的坑mybatis 自动生成时遇到 Client does not support authentication protocol requested by server; consider upgrading MySQL client 解决方法登录mysql： 12ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'qwertyui';SELECT plugin FROM mysql.user WHERE User = 'root';]]></content>
      <categories>
        <category>项目搭建</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
        <tag>教程</tag>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka背景及架构介绍]]></title>
    <url>%2FKafka%2FKafka-Background%2F</url>
    <content type="text"><![CDATA[Kafka创建背景kafka最初是LinkedIn的一个内部基础设施系统。最初开发的起因是，LinkedIn虽然有了数据库和其他系统可以用来存储数据，但是缺乏一个可以帮助处理持续数据流的组件。它的设计目的是提供一个高性能的消息系统，可以处理多种类型数据，并能够实时提供纯洁且结构化的用户活动数据和系统度量指标。 Kafka简介Kafka是由LinkedIn开发，使用Scala编写的一个分布式的消息系统，具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。一般在架构设计中起到解耦、削峰、异步处理的作用。 kafka对外使用topic的概念，生产者往topic里写消息，消费者从读消息。为了做到水平扩展，一个topic实际是由多个partition组成的，遇到瓶颈时，可以通过增加partition的数量来进行横向扩容。单个parition内是保证消息有序。 每新写一条消息，kafka就是在对应的文件append写，所以性能非常高。 Kafka架构一、名词解释 Broker：消息中间件处理节点（服务器），一个节点就是一个broker，一个Kafka集群由一个或多个broker组成 Topic：Kafka对消息进行归类，发送到集群的每一条消息都要指定一个topic Partition：物理上的概念，每个topic包含一个或多个partition，一个partition对应一个文件夹，这个文件夹下存储partition的数据和索引文件，每个partition内部是有序的 Producer：生产者，负责发布消息到broker Consumer：消费者，从broker读取消息 ConsumerGroup：每个consumer属于一个特定的consumer group，可为每个consumer指定group name，若不指定，则属于默认的group，一条消息可以发送到不同的consumer group，但一个consumer group中只能有一个consumer能消费这条消息 replica：partition 的副本，保障 partition 的高可用。 leader：replica 中的一个角色， producer 和 consumer 只跟 leader 交互。 follower：replica 中的一个角色，从 leader 中复制数据。 controller：每个集群都有一个broker同时充当了集群控制器角色（自动从集群的活跃成员中选举出来），负责管理工作包括分配分区给broker，监控broker等等 zookeeper：Kafka 通过 zookeeper 来存储集群的 meta 信息。 二、架构图总体数据流Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉去指定Topic的消息，然后进行业务处理。 生产者①首先要构造一个 ProducerRecord 对象，该对象可以声明主题Topic、分区Partition、键 Key以及值 Value，主题和值是必须要声明的，分区和键可以不用指定。 ②调用send() 方法进行消息发送。 ③因为消息要到网络上进行传输，所以必须进行序列化，序列化器的作用就是把消息的 key 和 value对象序列化成字节数组。 ④接下来数据传到分区器，如果之间的 ProducerRecord 对象指定了分区，那么分区器将不再做任何事，直接把指定的分区返回；如果没有，那么分区器会根据 Key 来选择一个分区，选择好分区之后，生产者就知道该往哪个主题和分区发送记录了。 ⑤接着这条记录会被添加到一个记录批次里面，这个批次里所有的消息会被发送到相同的主题和分区。会有一个独立的线程来把这些记录批次发送到相应的 Broker 上。 ③Broker成功接收到消息，表示发送成功，返回消息的元数据（包括主题和分区信息以及记录在分区里的偏移量）。发送失败，可以选择重试或者直接抛出异常。 key的作用：可以为消息的附加消息，也可以用来决定消息该被写到哪个主题分区，拥有相同key的消息将会被写到同一分区 topic从上图可以看出，Topic中数据是顺序不可变序列，采用log追加方式写入，因而kafka中无因随机写入导致性能低下的问题。 Topic的数据可存储在多个partition中，即可存放在不同的服务器上。这可使Topic大小不限于一台server容量。同时，消息存在多个partition上，可以实现Topic上消息的并发访问。 partition 每个 Partition 中的消息都是有序的，生产的消息被不断追加到 Partition log 上，其中的每一个消息都被赋予了一个唯一的 offset 值。 因此数据不会因消费而丢失，所以只要consumer指定offset，一个消息可被不同的consumer多次消费。 kafka中只能保证partition中记录是有序的，而不保证topic中不同partition的顺序。 分区的原因： 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了; 可以提高并发，因为可以以 Partition 为单位读写了。 Replication同一个 partition 可能会有多个 replication(对应 server.properties 配置中的 default.replication.factor=N)。 没有 replication 的情况下，一旦 broker 宕机，其上所有 patition 的数据都不可被消费，同时 producer 也不能再将数据存于其上的 patition。 引入 replication 之 后，同一个 partition 可能会有多个 replication，而这时需要在这些 replication 之间选出一个 leader，producer 和 consumer 只与这个 leader 交互，其它 replication 作为 follower 从 leader 中复制数据。 消费者订阅topic是以一个消费组来订阅的，一个消费组里面可以有多个消费者。 同一个消费组中的两个消费者，不会同时消费一个partition。换句话来说，就是一个partition，只能被消费组里的一个消费者消费，但是可以同时被多个消费组消费。 因此，如果消费组内的消费者如果比partition多的话，那么就会有个别消费者一直空闲。 Kafka Zookeeper 节点 Kafka常见的应用场景1.消息队列比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。 消息系统一般吞吐量相对较低，但是需要更小的端到端延时，并尝尝依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统， 如ActiveMQ或RabbitMQ。 2.行为跟踪行为跟踪是kafka基于发布订阅模式的扩展应用，当我们跟踪用户浏览页面、搜索及其他行为时，以发布-订阅的模式实时记录到对应的topic里那么这些结果被订阅者拿到后，就可以做进一步的实时处理，或实时监控，或放到hadoop/离线数据仓库里处理。 3.元信息监控与行为跟踪相似，作为操作记录的监控模块来使用，即汇集记录一些操作信息，可以理解为运维性质的数据监控吧。 4.日志收集日志收集方面，其实开源产品有很多，包括Scribe、Apache Flume。很多人使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或分布式文件系统）进行处理。然而Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比其他系统Kafka具有更高的扩展性，高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟。 5.流处理保存收集上游的流数据，以提供到下游的Storm或其他流式计算框架进行处理。很多用户会将那些从原始topic来的数据进行阶段性处理，汇总，扩充或者以其他的方式转换到新的topic下再继续后面的处理。例如一个文章推荐的处理流程，可能是先从数据源中抓取文章的内容，然后将其丢入一个叫做“文章”的topic中；后续操作可能是需要对这个内容进行清理，比如回复正常数据或者删除重复数据，最后再将内容匹配的结果返还给用户。这就在一个独立的topic之外，产生了一系列的实时数据处理的流程。Strom和Samza是非常著名的实现这种类型数据转换的框架。 6.持久性日志（commit log）Kafka可以为一种外部的持久性日志的分布式系统提供服务。这种日志可以在节点之外进行持久性日志的记录，节点间备份数据，并为故障节点数据回复提供一种重新同步的机制。Kafka中提供了日志压缩功能，日志压缩之后整体的日志状态仍然保留，并且通过日志回溯可以实现持久性日志的功能。在这种用法中，Kafka类似于Apache BookKeeper项目。 7.事件源将状态转移作为按时间顺序排列的记录序列，这种序列可以按时间回溯整个事件的状态变更，kafka本身的持久性，代表着他可以存储大量的日志，并且这些可以根据这些日志进行汇总和回溯等等。 实际应用中，适用最多最广泛的自然是MQ的功能。 Kafka用作MQ时与常用MQ的对比RabbitMQ——Rabbit Message Queue的简写，但不能仅仅理解其为消息队列，消息代理更合适。 RabbitMQ是一个由Erlang 语言开发的AMQP（高级消息队列协议）的开源实现。 RabbitMQ作为一个消息代理，主要和消息打交道，负责接收并转发消息。 ZeroMQ——是一个基于消息队列的多线程网络库，其对套接字类型、连接处理、帧、甚至路由的底层细节进行抽象，提供跨越多种传输协议的套接字。是网络通信中新的一层，介于应用层和传输层之间（按照TCP/IP划分），其是一个可伸缩层，可并行运行，分散在分布式系统间。 RocketMQ——阿里开源的一款高性能、高吞吐量的分布式消息中间件。 ActiveMQ——是一种开源的，实现了JMS1.1规范的，面向消息(MOM)的中间件，为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。 特性 Kafka RabbitMQ ZeroMQ RocketMQ ActiveMQ 开发语言 Scala Erlang C Java Java 支持协议 自行设计的基于TCP层的协议 AMQP TCP、UDP 自行设计 OpenWire、STOMP、REST、MQTT、XMPP、AMQP、WS 消息存储 内存、磁盘、数据库。支持大量堆积。 内存、磁盘。支持少量堆积。 消息发送端的内存或者磁盘中。不支持持久化。 磁盘。支持大量堆积。 内存、磁盘、数据库。支持少量堆积。 消息事务 支持 支持 不支持 支持 支持 负载均衡 支持 支持但支持的不好 去中心化，不支持负载均衡。本身只是一个多线程网络库。 支持 支持，可以基于zookeeper实现 集群方式 天然的‘‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave。 支持简单集群，’复制’模式，对高级集群模式支持不好。 去中心化，不支持集群。 ‘Master-Slave’ 模式，开源版本需手动切换Slave变成Master 支持简单集群模式，比如’主-备’，对高级集群模式支持不好。 可用性 非常高（分布式） 高（主从） 高 非常高（分布式） 高（主从） 消息重复 支持at least once、at most once 支持at least once、at most once 只有重传机制，但是没有持久化，消息丢了重传也没有用。既不是at least once、也不是at most once、更不是exactly only once 支持at least once 支持at least once 吞吐量TPS 极大 比较大 极大 大（发送端不是批量发送） 比较大 时效性 ms以内 us级 ms级 ms级 订阅形式和消息分发 基于topic以及按照topic进行正则匹配的发布订阅模式。 提供了4种方式：direct, topic ,Headers和fanout。 点对点(p2p) 基于topic/messageTag以及按照消息类型、属性进行正则匹配的发布订阅模式 点对点(p2p)、广播（发布-订阅） 顺序消息 支持 不支持 不支持 支持 不支持 消息确认 支持 支持 支持 支持 支持 消息回溯 支持指定分区offset位置的回溯 不支持 不支持 支持指定时间点的回溯 不支持 消费失败重试 不支持，但可以通过指定分区offset位置实现。 不支持，但是可以利用消息确认机制实现。 不支持 支持 不支持 并发度 高 极高 高 高 高 资料文档 中。有kafka作者自己写的书，网上资料也有一些。 多。有一些不错的书，网上资料多。 少。没有专门写zeromq的书，网上的资料多是一些代码的实现和简单介绍。 少。没有专门写rocketmq的书，网上的资料良莠不齐，官方文档很简洁，但是对技术细节没有过多的描述。 多。没有专门写activemq的书，网上资料多。 常用MQ的优缺点Kafka优点 性能卓越，单机写入TPS约在百万条/秒，最大的优点，就是吞吐量高。 时效性：ms级 可用性：非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消费者采用Pull方式获取消息, 消息有序, 通过控制能够保证所有消息被消费且仅被消费一次; 有优秀的第三方Kafka Web管理界面Kafka-Manager； 在日志领域比较成熟，被多家公司和多个开源项目使用； 功能支持：功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用 缺点 Kafka单机超过64个队列/分区，Load会发生明显的飙高现象，队列越多，load越高，发送消息响应时间变长 使用短轮询方式，实时性取决于轮询间隔时间； 消费失败不支持重试； 支持消息顺序，但是一台代理宕机后，就会产生消息乱序； 社区更新较慢； RabbitMQ优点 由于erlang语言的特性，mq 性能较好，高并发； 吞吐量到万级，MQ功能比较完备 健壮、稳定、易用、跨平台、支持多种语言、文档齐全； 开源提供的管理界面非常棒，用起来很好用 社区活跃度高； 缺点 erlang开发，很难去看懂源码，基本职能依赖于开源社区的快速维护和修复bug，不利于做二次开发和维护。 RabbitMQ确实吞吐量会低一些，这是因为他做的实现机制比较重。 需要学习比较复杂的接口和协议，学习和维护成本较高。 ZeroMQ优点 吞吐量：百万级 扩展性强，其他MQ都已经是成形的产品，已经是一款应用程序了。而ZeroMQ说白了就是一组库函数。 缺点 原生不支持持久化，仅支持相当有限的本地缓存，如需要消息持久化需要自己进行扩展。 在高并发环境下不会出问题，但是有可能会导致本地的缓存区被塞满而导致消息丢失的情况。 RocketMQ优点 单机吞吐量：十万级 可用性：非常高，分布式架构 消息可靠性：经过参数优化配置，消息可以做到0丢失 功能支持：MQ功能较为完善，还是分布式的，扩展性好 支持10亿级别的消息堆积，不会因为堆积导致性能下降 源码是java，我们可以自己阅读源码，定制自己公司的MQ，可以掌控 缺点 支持的客户端语言不多，目前是java及c++，其中c++不成熟； 社区活跃度一般 MQ核心代码未遵循JMS规范，有些系统要迁移需要修改大量代码 RocketMQ优点 单机吞吐量：万级 topic数量都吞吐量的影响： 时效性：ms级 可用性：高，基于主从架构实现高可用性 消息可靠性：有较低的概率丢失数据 功能支持：MQ领域的功能极其完备 遵循JMS规范安装部署方便 缺点 在并发较多时，消费端只能接收一部分，会出现丢失消息情况，需重启消费端才能接收到那部分剩下的消息。 官方社区现在对ActiveMQ 5.x维护越来越少，较少在大规模吞吐的场景中使用。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-list双向链表【学习笔记】]]></title>
    <url>%2Fredis%2FRedis-adlist%2F</url>
    <content type="text"><![CDATA[list定义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* * 双端链表节点 */typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;&#125; listNode;/* * 双端链表迭代器 */typedef struct listIter &#123; // 当前迭代到的节点 listNode *next; // 迭代的方向 int direction;&#125; listIter;/* * 双端链表结构 */typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key); // 链表所包含的节点数量 unsigned long len;&#125; list; list常用函数listCreate-创建新链表12345678910111213141516171819202122/** * 创建一个新的链表 * @return 创建成功返回链表，失败返回 NULL * T = O(1) */list *listCreate(void)&#123; struct list *list; // 分配内存 if ((list = zmalloc(sizeof(*list))) == NULL) return NULL; // 初始化属性 list-&gt;head = list-&gt;tail = NULL; list-&gt;len = 0; list-&gt;dup = NULL; list-&gt;free = NULL; list-&gt;match = NULL; return list;&#125; listRelease-释放整个链表1234567891011121314151617181920212223242526272829/** * 释放整个链表，以及链表中所有节点 * @param list * T = O(N) */void listRelease(list *list)&#123; unsigned long len; listNode *current, *next; // 指向头指针 current = list-&gt;head; // 遍历整个链表 len = list-&gt;len; while(len--) &#123; next = current-&gt;next; // 如果有设置值释放函数，那么调用它 if (list-&gt;free) list-&gt;free(current-&gt;value); // 释放节点结构 zfree(current); current = next; &#125; // 释放链表结构 zfree(list);&#125; listAddNodeHead-添加新节点到链表头1234567891011121314151617181920212223242526272829303132333435/** * 将一个包含有给定值指针 value 的新节点添加到链表的表头 * @param list * @param value * @return 如果为新节点分配内存出错，那么不执行任何动作，仅返回 NULL，如果执行成功，返回传入的链表指针 * T = O(1) */list *listAddNodeHead(list *list, void *value)&#123; listNode *node; // 为节点分配内存 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值指针 node-&gt;value = value; // 添加节点到空链表 if (list-&gt;len == 0) &#123; list-&gt;head = list-&gt;tail = node; node-&gt;prev = node-&gt;next = NULL; // 添加节点到非空链表 &#125; else &#123; node-&gt;prev = NULL; node-&gt;next = list-&gt;head; list-&gt;head-&gt;prev = node; list-&gt;head = node; &#125; // 更新链表节点数 list-&gt;len++; return list;&#125; listAddNodeTail-添加新节点到链表尾1234567891011121314151617181920212223242526272829303132333435/** * 将一个包含有给定值指针 value 的新节点添加到链表的表尾 * @param list * @param value 新节点 * @return 如果为新节点分配内存出错，那么不执行任何动作，仅返回 NULL，如果执行成功，返回传入的链表指针 * T = O(1) */list *listAddNodeTail(list *list, void *value)&#123; listNode *node; // 为新节点分配内存 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值指针 node-&gt;value = value; // 目标链表为空 if (list-&gt;len == 0) &#123; list-&gt;head = list-&gt;tail = node; node-&gt;prev = node-&gt;next = NULL; // 目标链表非空 &#125; else &#123; node-&gt;prev = list-&gt;tail; node-&gt;next = NULL; list-&gt;tail-&gt;next = node; list-&gt;tail = node; &#125; // 更新链表节点数 list-&gt;len++; return list;&#125; listInsertNode-将新节点添加到老节点之前或之后1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 创建一个包含值 value 的新节点，并将它插入到 old_node 的之前或之后 * 如果 after 为 0 ，将新节点插入到 old_node 之前。 * 如果 after 为 1 ，将新节点插入到 old_node 之后。 * @param list 链表 * @param old_node 老节点 * @param value 值 * @param after * @return 如果为新节点分配内存出错，那么不执行任何动作，仅返回 NULL，如果执行成功，返回传入的链表指针 * T = O(1) */list *listInsertNode(list *list, listNode *old_node, void *value, int after) &#123; listNode *node; // 创建新节点 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值 node-&gt;value = value; // 将新节点添加到给定节点之后 if (after) &#123; node-&gt;prev = old_node; node-&gt;next = old_node-&gt;next; // 给定节点是原表尾节点 if (list-&gt;tail == old_node) &#123; list-&gt;tail = node; &#125; // 将新节点添加到给定节点之前 &#125; else &#123; node-&gt;next = old_node; node-&gt;prev = old_node-&gt;prev; // 给定节点是原表头节点 if (list-&gt;head == old_node) &#123; list-&gt;head = node; &#125; &#125; // 更新新节点的前置指针 if (node-&gt;prev != NULL) &#123; node-&gt;prev-&gt;next = node; &#125; // 更新新节点的后置指针 if (node-&gt;next != NULL) &#123; node-&gt;next-&gt;prev = node; &#125; // 更新链表节点数 list-&gt;len++; return list;&#125; listDelNode-删除指定节点123456789101112131415161718192021222324252627282930/** * 从链表 list 中删除给定节点 node * 对节点私有值(private value of the node)的释放工作由调用者进行。 * @param list * @param node * T = O(1) */void listDelNode(list *list, listNode *node)&#123; // 调整前置节点的指针 if (node-&gt;prev) node-&gt;prev-&gt;next = node-&gt;next; else list-&gt;head = node-&gt;next; // 调整后置节点的指针 if (node-&gt;next) node-&gt;next-&gt;prev = node-&gt;prev; else list-&gt;tail = node-&gt;prev; // 释放值 if (list-&gt;free) list-&gt;free(node-&gt;value); // 释放节点 zfree(node); // 链表数减一 list-&gt;len--;&#125; listGetIterator-生成链表的迭代器123456789101112131415161718192021222324252627/** * 为给定链表创建一个迭代器， * 之后每次对这个迭代器调用 listNext 都返回被迭代到的链表节点 * @param list 链表 * @param direction 迭代方向 * AL_START_HEAD ：从表头向表尾迭代 * AL_START_TAIL ：从表尾想表头迭代 * @return 迭代器 * T = O(1) */listIter *listGetIterator(list *list, int direction)&#123; // 为迭代器分配内存 listIter *iter; if ((iter = zmalloc(sizeof(*iter))) == NULL) return NULL; // 根据迭代方向，设置迭代器的起始节点 if (direction == AL_START_HEAD) iter-&gt;next = list-&gt;head; else iter-&gt;next = list-&gt;tail; // 记录迭代方向 iter-&gt;direction = direction; return iter;&#125; listNext-返回迭代器当前所指向的节点1234567891011121314151617181920212223/** * 返回迭代器当前所指向的节点。 * 删除当前节点是允许的，但不能修改链表里的其他节点。 * @param iter 迭代器 * @return 函数要么返回一个节点，要么返回 NULL * T = O(1) */listNode *listNext(listIter *iter)&#123; listNode *current = iter-&gt;next; if (current != NULL) &#123; // 根据方向选择下一个节点 if (iter-&gt;direction == AL_START_HEAD) // 保存下一个节点，防止当前节点被删除而造成指针丢失 iter-&gt;next = current-&gt;next; else // 保存下一个节点，防止当前节点被删除而造成指针丢失 iter-&gt;next = current-&gt;prev; &#125; return current;&#125; listDup-复制整个链表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 复制整个链表。 * 无论复制是成功还是失败，输入节点都不会修改。 * 如果链表有设置值复制函数 dup ，那么对值的复制将使用复制函数进行， * 否则，新节点将和旧节点共享同一个指针。 * @param orig * @return 复制成功返回输入链表的副本，如果因为内存不足而造成复制失败，返回 NULL 。 * T = O(N) */list *listDup(list *orig)&#123; list *copy; listIter *iter; listNode *node; // 创建新链表 if ((copy = listCreate()) == NULL) return NULL; // 设置节点值处理函数 copy-&gt;dup = orig-&gt;dup; copy-&gt;free = orig-&gt;free; copy-&gt;match = orig-&gt;match; // 迭代整个输入链表 iter = listGetIterator(orig, AL_START_HEAD); while((node = listNext(iter)) != NULL) &#123; void *value; // 复制节点值到新节点 if (copy-&gt;dup) &#123; value = copy-&gt;dup(node-&gt;value); if (value == NULL) &#123; listRelease(copy); listReleaseIterator(iter); return NULL; &#125; &#125; else value = node-&gt;value; // 将节点添加到链表 if (listAddNodeTail(copy, value) == NULL) &#123; listRelease(copy); listReleaseIterator(iter); return NULL; &#125; &#125; // 释放迭代器 listReleaseIterator(iter); // 返回副本 return copy;&#125; listSearchKey-查找值为key的节点123456789101112131415161718192021222324252627282930313233343536373839/** * 查找链表 list 中值和 key 匹配的节点。 * 对比操作由链表的 match 函数负责进行，如果没有设置 match 函数， * 那么直接通过对比值的指针来决定是否匹配。 * @param list 链表 * @param key 值 * @return 如果匹配成功，那么第一个匹配的节点会被返回。 * 如果没有匹配任何节点，那么返回 NULL 。 */listNode *listSearchKey(list *list, void *key)&#123; listIter *iter; listNode *node; // 迭代整个链表 iter = listGetIterator(list, AL_START_HEAD); while((node = listNext(iter)) != NULL) &#123; // 对比 if (list-&gt;match) &#123; if (list-&gt;match(node-&gt;value, key)) &#123; listReleaseIterator(iter); // 找到 return node; &#125; &#125; else &#123; if (key == node-&gt;value) &#123; listReleaseIterator(iter); // 找到 return node; &#125; &#125; &#125; listReleaseIterator(iter); // 未找到 return NULL;&#125; listIndex-返回链表在指定索引上的值1234567891011121314151617181920212223/** * 返回链表在给定索引上的值。 * @param list * @param index 索引以 0 为起始，也可以是负数， -1 表示链表最后一个节点，诸如此类。 * @return 如果索引超出范围（out of range），返回 NULL 。 * T = O(N) */listNode *listIndex(list *list, long index) &#123; listNode *n; // 如果索引为负数，从表尾开始查找 if (index &lt; 0) &#123; index = (-index)-1; n = list-&gt;tail; while(index-- &amp;&amp; n) n = n-&gt;prev; // 如果索引为正数，从表头开始查找 &#125; else &#123; n = list-&gt;head; while(index-- &amp;&amp; n) n = n-&gt;next; &#125; return n;&#125; listRewind-设置正向迭代器12345678910/** * 将迭代器的方向设置为 AL_START_HEAD ，并将迭代指针重新指向表头节点。 * @param list 链表 * @param li 迭代器 * T = O(1) */void listRewind(list *list, listIter *li) &#123; li-&gt;next = list-&gt;head; li-&gt;direction = AL_START_HEAD;&#125; listRewindTail-设置反向迭代器12345678910/** * 将迭代器的方向设置为 AL_START_TAIL, 并将迭代指针重新指向表尾节点。 * @param list 链表 * @param li 迭代器 * T = O(1) */void listRewindTail(list *list, listIter *li) &#123; li-&gt;next = list-&gt;tail; li-&gt;direction = AL_START_TAIL;&#125; listRotate-将链表尾移动到表头1234567891011121314151617181920/ * 取出链表的表尾节点，并将它移动到表头，成为新的表头节点。 * @param list * T = O(1) */void listRotate(list *list) &#123; listNode *tail = list-&gt;tail; if (listLength(list) &lt;= 1) return; // 取出表尾节点 list-&gt;tail = tail-&gt;prev; list-&gt;tail-&gt;next = NULL; // 插入到表头 list-&gt;head-&gt;prev = tail; tail-&gt;prev = NULL; tail-&gt;next = list-&gt;head; list-&gt;head = tail;&#125;]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-sds动态字符串【学习笔记】]]></title>
    <url>%2Fredis%2FRedis-sds%2F</url>
    <content type="text"><![CDATA[SDS定义12345678910111213141516171819/* * 类型别名，用于指向 sdshdr 的 buf 属性 */typedef char *sds;/* * 保存字符串对象的结构 */struct sdshdr &#123; // buf 中已占用空间的长度 int len; // buf 中剩余可用空间的长度 int free; // 数据空间 char buf[];&#125;; SDS常用函数sdslen-sds长度123456789/* * 返回 sds 实际保存的字符串的长度 * * T = O(1) */static inline size_t sdslen(const sds s) &#123; struct sdshdr *sh = (void*)(s-(sizeof(struct sdshdr))); return sh-&gt;len;&#125; s 实际上存的是buf首个char数据的地址，也就是向前移动8个字节，就能到sdshdr的len的首地址char buf[]这个数组没有大小，是所谓的柔性数组，是不占据内存大小的，所以sizeof(struct sdshdr)为8。具体结构如下图 sdsavail-sds可用free空间长度123456789/* * 返回 sds 可用空间的长度 * * T = O(1) */static inline size_t sdsavail(const sds s) &#123; struct sdshdr *sh = (void*)(s-(sizeof(struct sdshdr))); return sh-&gt;free;&#125; sdsnewlen-根据字符串长度创建sds1234567891011121314151617181920212223242526272829303132333435363738/** * 根据给定的初始化字符串 init 和字符串长度 initlen,创建一个新的 sds * @param init 初始化字符串指针 * @param initlen 初始化字符串的长度 * @return 创建成功返回 sdshdr 相对应的 sds,创建失败返回 NULL * T = O(N) */sds sdsnewlen(const void *init, size_t initlen) &#123; struct sdshdr *sh; // 根据是否有初始化内容，选择适当的内存分配方式 // T = O(N) if (init) &#123; // zmalloc 不初始化所分配的内存 sh = zmalloc(sizeof(struct sdshdr)+initlen+1); &#125; else &#123; // zcalloc 将分配的内存全部初始化为 0 sh = zcalloc(sizeof(struct sdshdr)+initlen+1); &#125; // 内存分配失败，返回 if (sh == NULL) return NULL; // 设置初始化长度 sh-&gt;len = initlen; // 新 sds 不预留任何空间 sh-&gt;free = 0; // 如果有指定初始化内容，将它们复制到 sdshdr 的 buf 中 // T = O(N) if (initlen &amp;&amp; init) memcpy(sh-&gt;buf, init, initlen); // 以 \0 结尾 sh-&gt;buf[initlen] = '\0'; // 返回 buf 部分，而不是整个 sdshdr return (char*)sh-&gt;buf;&#125; sdsnew-创建sds12345678910/** * 根据给定字符串 init ，创建一个包含同样字符串的 sds * @param init 如果输入为 NULL ，那么创建一个空白 sds * @return 创建成功返回 sdshdr 相对应的 sds，创建失败返回 NULL * T = O(N) */sds sdsnew(const char *init) &#123; size_t initlen = (init == NULL) ? 0 : strlen(init); return sdsnewlen(init, initlen);&#125; sdsempty-创建空sds12345678/** * 创建并返回一个只保存了空字符串 "" 的 sds * @return 创建成功返回 sdshdr 相对应的 sds,创建失败返回 NULL * T = O(1) */sds sdsempty(void) &#123; return sdsnewlen("",0);&#125; sdsdup-复制sds创建副本123456789/** * 复制给定 sds 创建副本 * @param s sds * @return 创建成功返回输入 sds 的副本 * T = O(N) */sds sdsdup(const sds s) &#123; return sdsnewlen(s, sdslen(s));&#125; sdsfree-释放sds123456789/** * 释放给定的 sds * @param s * T = O(N) */void sdsfree(sds s) &#123; if (s == NULL) return; zfree(s-sizeof(struct sdshdr));&#125; sdsgrowzero-扩充sds未使用空间补0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * 将 sds 扩充至指定长度，未使用的空间以 0 字节填充。 * @param s * @param len 指定长度 * @return 扩充成功返回新 sds ，失败返回 NULL * T = O(N) */sds sdsgrowzero(sds s, size_t len) &#123; struct sdshdr *sh = (void*)(s-(sizeof(struct sdshdr))); size_t totlen, curlen = sh-&gt;len; // 如果 len 比字符串的现有长度小， // 那么直接返回，不做动作 if (len &lt;= curlen) return s; // 扩展 sds // T = O(N) s = sdsMakeRoomFor(s,len-curlen); // 如果内存不足，直接返回 if (s == NULL) return NULL; // 将新分配的空间用 0 填充，防止出现垃圾内容 // T = O(N) sh = (void*)(s-(sizeof(struct sdshdr))); memset(s+curlen,0,(len-curlen+1)); // 更新属性 totlen = sh-&gt;len+sh-&gt;free; sh-&gt;len = len; sh-&gt;free = totlen-sh-&gt;len; // 返回新的 sds return s;&#125;/** * 对 sds 中 buf 的长度进行扩展，确保在函数执行之后， * buf 至少会有 addlen + 1 长度的空余空间（额外的 1 字节是为 \0 准备的） * @param s * @param addlen * @return 扩展成功返回扩展后的 sds，扩展失败返回 NULL * T = O(N) */sds sdsMakeRoomFor(sds s, size_t addlen) &#123; struct sdshdr *sh, *newsh; // 获取 s 目前的空余空间长度 size_t free = sdsavail(s); size_t len, newlen; // s 目前的空余空间已经足够，无须再进行扩展，直接返回 if (free &gt;= addlen) return s; // 获取 s 目前已占用空间的长度 len = sdslen(s); sh = (void*) (s-(sizeof(struct sdshdr))); // s 最少需要的长度 newlen = (len+addlen); // 根据新长度，为 s 分配新空间所需的大小 if (newlen &lt; SDS_MAX_PREALLOC) // 如果新长度小于 SDS_MAX_PREALLOC // 那么为它分配两倍于所需长度的空间 newlen *= 2; else // 否则，分配长度为目前长度加上 SDS_MAX_PREALLOC newlen += SDS_MAX_PREALLOC; // T = O(N) newsh = zrealloc(sh, sizeof(struct sdshdr)+newlen+1); // 内存不足，分配失败，返回 if (newsh == NULL) return NULL; // 更新 sds 的空余长度 newsh-&gt;free = newlen - len; // 返回 sds return newsh-&gt;buf;&#125;/* * 最大预分配长度 */#define SDS_MAX_PREALLOC (1024*1024) sdscatlen-根据字符串长度将字符串追加到sds末尾12345678910111213141516171819202122232425262728293031323334353637/** * 将长度为 len 的字符串 t 追加到 sds 的字符串末尾 * @param s * @param t 字符串t * @param len t的长度 * @return 追加成功返回新 sds ，失败返回 NULL * T = O(N) */sds sdscatlen(sds s, const void *t, size_t len) &#123; struct sdshdr *sh; // 原有字符串长度 size_t curlen = sdslen(s); // 扩展 sds 空间 // T = O(N) s = sdsMakeRoomFor(s,len); // 内存不足？直接返回 if (s == NULL) return NULL; // 复制 t 中的内容到字符串后部 // T = O(N) sh = (void*) (s-(sizeof(struct sdshdr))); memcpy(s+curlen, t, len); // 更新属性 sh-&gt;len = curlen+len; sh-&gt;free = sh-&gt;free-len; // 添加新结尾符号 s[curlen+len] = '\0'; // 返回新 sds return s;&#125; sdscat-将字符串追加到sds末尾12345678/** * 将给定字符串 t 追加到 sds 的末尾 * @return 追加成功返回新 sds ，失败返回 NULL * T = O(N) */sds sdscat(sds s, const char *t) &#123; return sdscatlen(s, t, strlen(t));&#125; sdscatsds-将sds追加到另一个sds末尾12345678/** * 将另一个 sds 追加到一个 sds 的末尾 * @return 追加成功返回新 sds ，失败返回 NULL * T = O(N) */sds sdscatsds(sds s, const sds t) &#123; return sdscatlen(s, t, sdslen(t));&#125; sdscpylen-将字符串前len复制到sds123456789101112131415161718192021222324252627282930313233343536/** * 将字符串 t 的前 len 个字符复制到 sds s 当中,覆盖原有的字符 * 如果 sds 的长度少于 len 个字符，那么扩展 sds * @return 复制成功返回新的 sds ，否则返回 NULL * T = O(N) */sds sdscpylen(sds s, const char *t, size_t len) &#123; struct sdshdr *sh = (void*) (s-(sizeof(struct sdshdr))); // sds 现有 buf 的长度 size_t totlen = sh-&gt;free+sh-&gt;len; // 如果 s 的 buf 长度不满足 len ，那么扩展它 if (totlen &lt; len) &#123; // T = O(N) s = sdsMakeRoomFor(s,len-sh-&gt;len); if (s == NULL) return NULL; sh = (void*) (s-(sizeof(struct sdshdr))); totlen = sh-&gt;free+sh-&gt;len; &#125; // 复制内容 // T = O(N) memcpy(s, t, len); // 添加终结符号 s[len] = '\0'; // 更新属性 sh-&gt;len = len; sh-&gt;free = totlen-len; // 返回新的 sds return s;&#125; sdscpy-将字符串复制到 sds 当中12345678/** * 将字符串复制到 sds 当中,覆盖原有的字符 * @return 复制成功返回新的 sds ，否则返回 NULL * T = O(N) */sds sdscpy(sds s, const char *t) &#123; return sdscpylen(s, t, strlen(t));&#125;]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《自己动手写JAVA虚拟机》学习笔记三【解析class文件】]]></title>
    <url>%2FJVM%2FJVM3%2F</url>
    <content type="text"><![CDATA[java虚拟机规范中使用一种类似C语言结构体来描述Class文件的基本结构，具体如下： 123456789101112131415161718ClassFile &#123; u4 magic;//魔数 u2 minor_version;//主版本号 u2 major_version;//次版本号 u2 constant_pool_count;//常量池长度 cp_info constant_pool[constant_pool_count-1];//常量池信息 u2 access_flags;//该类的访问修饰符 u2 this_class;//类索引 u2 super_class;//父类索引 u2 interfaces_count;//接口个数 u2 interfaces[interfaces_count];//接口详细信息 u2 fields_count;//属性个数 field_info fields[fields_count];//属性详细信息 u2 methods_count;//方法个数 method_info methods[methods_count];//方法详情 u2 attributes_count;//类文件属性个数 attribute_info attributes[attributes_count];//类文件属性详细信息&#125; 准备工作把ch02的目录结构复制一份改名ch03，在ch03的目录中创建一个classfile子目录。 12345678|-jvmgo |-ch01 |-ch01 |-ch03 |-classfile |-classpath |-cmd.go |-main.go 为了学习编译后的class文件，新建一个classFileTest.java然后编译 12345678910111213public class ClassFileTest &#123; public static final boolean FLAG = true; public static final byte BYTE = 123; public static final char X = 'X'; public static final short SHORT = 12345; public static final int INT = 123456789; public static final long LONG = 12345678901L; public static final float PI = 3.14f; public static final double E = 2.71828; public static void main(String[] args) throws RuntimeException &#123; System.out.println("Hello, World!"); &#125;&#125; 用作者提供的classpy的图形化工具，可以查看反编译后的class文件。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>JVM</tag>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《自己动手写JAVA虚拟机》学习笔记二【搜索class文件】]]></title>
    <url>%2FJVM%2FJVM2%2F</url>
    <content type="text"><![CDATA[12345public class HelloWorld &#123; public static void main(String[] args)&#123; System.out.println("Hello, world!"); &#125;&#125; 运行上面的java程序时，我们知道首先要启动java虚拟机，然后加载主类，最后调用主类的main方法。但是在加载HelloWorld类之前，首先要加载它的超类java.lang.Object，在调用main()函数之前，虚拟机要准备好参数数组，所以需要加载java.lang.String和java.lang.String[]类。把字符串打印到控制台还需要加载java.lang.System类，等等。。那么java虚拟机如何寻找这些类的呢？ 类路径类路径可以分为以下三种： 启动类路径(bootstrap classpath)：启动类路径默认对应jre/lib目录，Java标准库位于该路径。 扩展类路径(extention classpath)：扩展类路径默认对应jre/lib/ext目录，使用Java扩展机制的类位于该路径。 用户类路径(user classpath)：我们自己实现的类，以及第三方类库则位于用户类路径。用户类路径的默认值是当前路径，也就是”.”，可以给java命令传递-classpath选项来指定。 准备工作把ch01的目录结构复制一份改名ch02，在ch02的目录中创建一个classpath子目录。 123456|-jvmgo |-ch01 |-ch01 |-classpath |-cmd.go |-main.go 修改cmd结构体，添加XjreOption字段 12345678type Cmd struct &#123; helpFlag bool versionFlag bool cpOption string XjreOption string class string args []string&#125; parseCmd()函数也对应添加Xjre 123456789//命令解析func parseCmd() *Cmd &#123; ...//其他代码不变 flag.StringVar(&amp;cmd.cpOption,"cp","","classpath") flag.StringVar(&amp;cmd.XjreOption,"Xjre","","path to jre") //解析命令行参数到定义的flag flag.Parse() ...//其他代码不变&#125; 实现类路径采用组合模式来实现类路径，把类路径当成一个大的整体，由启动类路径、扩展类路径和用户类路径三个小路径构成，三个小路径又分别由更小的路径构成。 首先定义一个Entry接口 1234567891011//获取系统分隔符，windows是;类UNIX系统是:号const pathListSeparator = string(os.PathListSeparator)type Entry interface &#123; //寻找和加载class文件 参数：class文件相对路径，路径之间用/，文件名有.class后缀 //例如读取java.lang.Object入参是java/lang/Object.class readClass(classname string) ([]byte, Entry, error) //toString String() string&#125; Entry接口一共有四种实现，CompositeEntry，WildcardEntry，ZipEntry，DirEntry DirEntryDirEntry相对简单些，表示目录形式的类路径 12345678910111213141516171819202122232425262728293031323334package classpathimport ( "path/filepath" "io/ioutil")type DirEntry struct &#123; //存放目录的绝对路径 absDir string&#125;//相当于构造函数func newDirEntry(path string) *DirEntry &#123; //将参数转换成绝对路径 absDir, err := filepath.Abs(path) if err != nil &#123; panic(err) &#125; return &amp;DirEntry&#123;absDir&#125;&#125;//读取class文件func (self *DirEntry) readClass (className string) ([]byte, Entry, error) &#123; //把目录和class名拼成完成路径 fileName := filepath.Join(self.absDir,className) //读取class文件内容 data, err := ioutil.ReadFile(fileName) return data,self,err&#125;//直接返回目录func (self *DirEntry) String() string&#123; return self.absDir&#125; ZipEntryZipEntry表示ZIP或者JAR文件形式的类路径 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package classpathimport ( "path/filepath" "archive/zip" "io/ioutil" "errors")type ZipEntry struct &#123; //存放目录的绝对路径 absPath string&#125;//相当于构造函数func newZipEntry(path string) *ZipEntry &#123; //将参数转换成绝对路径 absPath, err := filepath.Abs(path) if err != nil &#123; panic(err) &#125; return &amp;ZipEntry&#123;absPath&#125;&#125;//读取class文件func (self *ZipEntry) readClass(classname string) ([]byte, Entry, error) &#123; //打开zip文件 r, err := zip.OpenReader(self.absPath) if err != nil &#123; return nil,nil,err &#125; defer r.Close() //遍历zip包里的文件 for _, f := range r.File &#123; //找到class文件 if f.Name == classname &#123; //打开class文件 rc , err := f.Open() if err != nil &#123; return nil,nil,err &#125; defer rc.Close() //读取class文件内容 data, err := ioutil.ReadAll(rc) if err != nil &#123; return nil,nil,err &#125; return data,self,err &#125; &#125; //未找到class文件 return nil,nil,errors.New("class not found :" +classname)&#125;//直接返回目录func (self *ZipEntry) String() string &#123; return self.absPath&#125; CompositeEntryCompositeEntry表示有分隔符的类路径，CompositeEntry由更小的Entry组成，可以表示成[]Entry，go语言中则使用便利的slice 12345678910111213141516171819202122232425262728293031323334353637383940package classpathimport ( "strings" "errors")type CompositeEntry []Entry//将每个小路径转换成具体的Entryfunc newCompositeEntry(pathList string) CompositeEntry &#123; var compositeEntry []Entry //将路径按照分隔符进行分割 for _, path := range strings.Split(pathList,pathListSeparator)&#123; entry := newEntry(path) compositeEntry = append(compositeEntry,entry) &#125; return compositeEntry&#125;func (self CompositeEntry) readClass(classname string) ([]byte, Entry, error) &#123; //遍历entry数据 for _, entry := range self&#123; //读取class文件，依次调用每一个子路径的readClass方法 data, from, err := entry.readClass(classname) if err == nil&#123; return data,from,err &#125; &#125; return nil,nil,errors.New("class not found :" +classname)&#125;//调用每个子路径的String方法，用分隔符拼接起来func (self CompositeEntry) String() string &#123; strs := make([]string,len(self)) for i, entry := range self&#123; strs[i] = entry.String() &#125; return strings.Join(strs,pathListSeparator)&#125; WildcardEntryWildcardEntry表示以*结尾的类路径，实际上也是CompositeEntry，因此就不再新定义类型类 1234567891011121314151617181920212223242526272829303132package classpathimport ( "strings" "os" "path/filepath")func newWildcardEntry(path string) CompositeEntry &#123; //去掉尾部的* baseDir := path[:len(path)-1] var compositeEntry []Entry walkFn := func(path string, info os.FileInfo, err error) error&#123; if err != nil&#123; return err &#125; //如果不是目录，返回跳过标识 if info.IsDir() &amp;&amp; path != baseDir &#123; return filepath.SkipDir &#125; //选出jar文件 if strings.HasSuffix(path,".jar") || strings.HasSuffix(path,".JAR")&#123; jarEntry := newZipEntry(path) compositeEntry = append(compositeEntry,jarEntry) &#125; return nil &#125; //遍历baseDir路径，创建zipEntry filepath.Walk(baseDir,walkFn) //fmt.Printf("compositeEntry : %s\n",compositeEntry) return compositeEntry&#125; Entry四种类路径都实现完之后，再来完善下Entry接口，添加Entry实例的构造方法。 12345678910111213141516func newEntry(path string) Entry &#123; //如果路径中含有分隔符 if strings.Contains(path,pathListSeparator)&#123; return newCompositeEntry(path) &#125; //如果路径末尾是* if strings.HasSuffix(path,"*")&#123; return newWildcardEntry(path) &#125; //如果路径以jar或者zip结尾 if strings.HasSuffix(path,".jar") || strings.HasSuffix(path,".JAR")|| strings.HasSuffix(path,".zip") || strings.HasSuffix(path,".ZIP")&#123; return newZipEntry(path) &#125; return newDirEntry(path)&#125; 实现Classpath1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package classpathimport ( "path/filepath" "os" "fmt")type Classpath struct &#123; bootClasspath Entry extClasspath Entry userClasspath Entry&#125;//使用-Xjre选项解析启动类路径和扩展类路径，使用-classpath/-cp选项解析用户类路径func Parse(jreOption,cpOption string) *Classpath &#123; cp := &amp;Classpath&#123;&#125; //解析启动类路径和扩展类路径 cp.parseBootAndExtClasspath(jreOption) //解析用户类路径 cp.parseUserClasspath(cpOption) return cp&#125;func getJreDir(jreOption string) string &#123; //优先使用用户输入的-Xjre作为目录 if jreOption != "" &amp;&amp; exists(jreOption)&#123; return jreOption &#125; //在当前目录下寻找jre目录 if exists("./jre") &#123; return "./jre" &#125; //尝试使用JAVA_HOME环境变量 if jh := os.Getenv("JAVA_HOME"); jh != ""&#123; return filepath.Join(jh,"jre") &#125; panic("Can not find jre folder")&#125;//判断目录是否存在func exists(path string) bool &#123; if _, err := os.Stat(path); err != nil&#123; if os.IsNotExist(err)&#123; return false &#125; &#125; return true&#125;func (self *Classpath) parseBootAndExtClasspath(jreOption string) &#123; // 获取jre目录 jreDir := getJreDir(jreOption) //jre/lib/* jreLibPath := filepath.Join(jreDir,"lib","*") self.bootClasspath = newWildcardEntry(jreLibPath) //jre/lib/ext/* jreExtPath := filepath.Join(jreDir,"lib","ext","*") self.extClasspath = newWildcardEntry(jreExtPath)&#125;//解析用户类路径func (self *Classpath) parseUserClasspath(cpOption string) &#123; // 如果用户没有提供-classpath/-cp选项，则使用当前目录作为用户类路径 if cpOption == ""&#123; cpOption = "." &#125; self.userClasspath = newEntry(cpOption)&#125;//寻找class方法func (self *Classpath) ReadClass(classname string) ([]byte, Entry, error) &#123; //访问ReadClass方法只需传递类名，不用包含".class"后缀 classname = classname + ".class" // 从bootClasspath寻找class文件 if data, entry, err := self.bootClasspath.readClass(classname); err == nil&#123; return data, entry, err &#125; // 从extClasspath寻找class文件 if data, entry, err := self.extClasspath.readClass(classname); err == nil&#123; return data, entry, err &#125; // 从userClasspath寻找class文件 return self.userClasspath.readClass(classname)&#125;func (self *Classpath) String() string &#123; return self.userClasspath.String()&#125; 测试代码完善main.go中的startJVM 123456789101112131415//模拟启动jvmfunc startJVM(cmd *Cmd) &#123; // 获取Classpath cp := classpath.Parse(cmd.XjreOption,cmd.cpOption) fmt.Printf("classpath:%s class:%s args:%v\n",cp,cmd.class,cmd.args) // 将.替换成/(java.lang.String -&gt; java/lang/String) className := strings.Replace(cmd.class,".","/",-1) // 读取class classData, _, err := cp.ReadClass(className) if err != nil &#123; fmt.Printf("Could not find or load main class %s\n",cmd.class) return &#125; fmt.Printf("class data : %v\n",classData)&#125; 编译main.go，并测试-version 12345$ go install jvmgo/ch02 $ ch02 java.lang.String# 没有传递-Xjre，会去读取$JAVA_HOME，成功打印出String.class的内容$ ch02 -Xjre /opt java.lang.Object # 传递错误-Xjre会打印出Could not find or load main class java.lang.Object]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>JVM</tag>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《自己动手写JAVA虚拟机》学习笔记一【命令行工具】]]></title>
    <url>%2FJVM%2FJVM1%2F</url>
    <content type="text"><![CDATA[最近正在看张秀宏著的《自己动手写Java虚拟机》，这本书适合初学者更深入的理解java虚拟机的含义，也可以简单学习go语言的基本使用。 准备工作安装JDK从Oracle官网下载最新的JDK，双击运行即可。我使用的是1.8.0_161 安装GO从GO语言官网下载最新版本的GO安装文件，双击运行即可,我使用的是1.11.2。测试Go环境是否安装成功 12～$ go versiongo version go1.11.2 darwin/amd64 设置环境变量 1234#添加Go的运行环境路径export PATH=$PATH:/usr/local/go/bin#添加Go工程的工作空间,可自行修改export GOPATH=/home/XXX/XXX/jvmgo/go 执行以下命令，如果GOPATH与你设置的相同环境变量设置成功, 1～$ go env 实现JAVA命令java命令常用选项及其用途 选项 用途 -version 输出版本信息，然后退出 -?/-help 输出帮助信息，然后退出 -cp/-classpath 指定用户类路径 -Dproperty=value 设置Java系统属性 -Xms 设置初始堆空间大小 -Xmx 设置最大堆空间大小 -Xss 设置线程栈空间大小 编写命令行工具首先创建项目结构 12|-jvmgo |-ch01 在ch01目录下创建cmd.go文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package mainimport "flag"import "fmt"import "os"//用法: java [-options] class [args...] (执行类)//或 java [-options] -jar jarfile [args...] (执行 jar 文件)type Cmd struct &#123; helpFlag bool versionFlag bool cpOption string class string args []string&#125;//把命令的用法打印到控制台func printUsage() &#123; fmt.Printf("Usage：%s [-options] class [args...]\n",os.Args[0])&#125;//命令解析func parseCmd() *Cmd &#123; //声明cmd为指向空的Cmd对象的指针 cmd := &amp;Cmd&#123;&#125; //定义flag参数 //Usage是一个函数，默认输出所有定义了的命令行参数和帮助信息 flag.Usage = printUsage flag.BoolVar(&amp;cmd.helpFlag,"help",false,"print help message") flag.BoolVar(&amp;cmd.helpFlag,"?",false,"print help message") flag.BoolVar(&amp;cmd.versionFlag,"version",false,"print version and exit") flag.StringVar(&amp;cmd.cpOption,"classpath","","classpath") flag.StringVar(&amp;cmd.cpOption,"cp","","classpath") //在所有的flag定义完成之后，可以通过调用flag.Parse()进行解析。 flag.Parse() //flag.Args()可以捕获未被解析的参数 args := flag.Args() if len(args) &gt; 0&#123; cmd.class = args[0] cmd.args = args[1:] &#125; return cmd&#125; 测试代码在ch01目录下创建main.go文件 12345678910111213141516171819package mainimport "fmt"func main() &#123; cmd := parseCmd() if cmd.versionFlag &#123; fmt.Println("version 0.0.1") &#125;else if cmd.helpFlag || cmd.class == ""&#123; printUsage() &#125;else &#123; startJVM(cmd) &#125;&#125;//模拟启动jvmfunc startJVM(cmd *Cmd) &#123; //还未开始写，暂时打印 fmt.Printf("classpath:%s class:%s args:%v\n",cmd.cpOption,cmd.class,cmd.args)&#125; 编译main.go，并测试-version 123$ go install jvmgo/ch01 $ ch01 -versionversion 0.0.1]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>学习笔记</tag>
        <tag>JVM</tag>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下利用Github Pages快速搭建免费博客]]></title>
    <url>%2Fblog%2FGithub-Pages-Blog%2F</url>
    <content type="text"><![CDATA[本博客利用Hexo + Next + GitHubPages 搭建博客。 准备工作安装node.js1$ brew install node 安装 git1$ brew install git 安装yarn1$ npm install yarn 如果安装失败用 1$ brew install yarn 安装hexo进入你的项目目录，举个例子我的目录是blog 1$ cd blog 然后用 yarn 安装 hexo 1$ yarn add hexo -S 如果安装失败用 12$ npm install hexo-cli -g$ npm install hexo-server --save 详细可见官方文档 搭建本地博客项目初始化进入你的项目目录,初始化hexo 12$ cd blog$ hexo init 安装依赖1$ yarn install 启动本地服务1$ hexo server 在浏览器输入 localhost:4000 即可 关联 GitHub创建项目 项目名称一定一定是 [用户名].github.io 设置 在设置中找到GitHub Pages ，分支选择master，点击choose a theme 选择喜欢的主题，点击select theme即可 到这里就可以访问你的主页啦 https://[用户名].github.io/,接下来复制你的项目链接 到你的本地项目根目录找到 _config.yml 文件，粘贴到以下位置 站点配置站点配置包括修改博客名称，描述，作者等等，建议直接查看官方教程 关联安装 hexo-deployer-git 插件 1$ yarn add hexo-deployer-git -S 如果失败的话尝试下面的方法 1$ npm install --save hexo-deployer-git 在你的项目文件夹下运行 hexo d -g （生成本地文件并将本地文件推送到 GitHub ，和 git push 功能相同）,如果失败的话前面加sudo 1$ hexo d -g 这时访问你的github主页https://[用户名].github.io/就可以啦 NexT主题配置NexT 是一个高质量并且优雅的Hexo 主题。详细可见官方文档 安装NexT主题在你的项目文件夹下，执行以下命令，安装NexT主题 1$ git clone https://github.com/iissnan/hexo-theme-next.git themes/next 更改项目根目录下_config.yml 文件，如下 重新生成本地文件并推送到github 1$ hexo d -g 访问你的github主页https://[用户名].github.io/就可以看到啦 主题配置themes/next文件夹下的_config.yml是主题的配置文件next内置了4种主题方案，选择你喜欢的方案解开注释即可 123456789# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Musescheme: Mist#scheme: Pisces#scheme: Gemini Menu Settings是控制图中菜单的位置，根据需要解开注释即可 123456789101112# ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------menu: home: / || home# about: /about/ || user tags: /tags/ || tags categories: /categories/ || th# archives: /archives/ || archive# schedule: /schedule/ || calendar# sitemap: /sitemap.xml || sitemap# commonweal: /404.html || heartbeat 还有很多很多的配置请参考官方教程,主题个性配置教程,主题美化 修改之后，运行以下命令就可以再你的主页看到啦 12$ hexo clean$ hexo d -g 也可以在本地启动服务器，在浏览器输入 localhost:4000 观看效果 1$ hexo server 创建文章添加【标签】页面新建标签页面 1$ hexo new page tags 修改项目根目录下 source/tags 的 index.md 文件如下： 1234title: tagstype: "tags"comments: false--- 修改themes/next文件夹下的_config.yml主题配置文件，取消 tags: /tags/ || tags 这行注释新建测试文章 1$ hexo new 'test' 在测试文章的头部添加tags信息，如下： 12345title: 测试文章tags: - Testing - Another Tag--- 启动本地服务，就可以看到标签菜单，点击可进入标签页，看到 如图所示 证明标签页面添加成功。 添加【分类】页面新建标签页面 1$ hexo new page categories 修改项目根目录下 source/tags 的 index.md 文件如下： 1234title: categoriestype: "categories"comments: false--- 修改themes/next文件夹下的_config.yml主题配置文件，取消 categories: /categories/ || th 这行注释在测试文章的头部添加categories信息，如下： 123456title: 测试文章tags: - Testing - Another Tag---categories: Testing 启动本地服务，就可以看到标签菜单，点击可进入标签页，看到 如图所示 证明分类页面添加成功。 给博客添加图片在项目目录下执行 1$ npm install hexo-asset-image --save 在用下面命令生成md文章时，会在_post目录下看到一个与文章同名的文件夹 1$ hexo new '文章名' 将想要上传的图片先放到文件夹下，然后在博客中使用markdown的格式引入图片： 1![文字](xxxx/图片名.jpg) 文章名和文件夹名字相同，所以不需要绝对路径，只要xxxx是文件夹的名字就可以了。 添加Valine评论系统首先需要去注册一个Leancloud账号,验证邮箱然后随便创建一个应用，按如图所示找到appid，appkey 按下图修改themes/next文件夹下的_config.yml主题配置文件，重新启动服务器就可以啦 12345678910111213# Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: true appid: your appid # your leancloud application appid appkey: your appkey # your leancloud application appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 描述 # comment box placeholder avatar: monsterid # gravatar style guest_info: nick,mail # custom comment header pageSize: 10 # pagination size 如果要删除评论请到Leancloud里删除哦 hexo添加多作者在项目目录下执行 1$ npm install hexo-generator-author --save 在文章的头部添加author信息，如下： 123456title: 测试文章tags: - Testing - Another Tag---author: Alice 修改/layout/_macro/下的post.swig文件 +为新添加的行 123456789 &lt;div class=&quot;post-meta&quot;&gt;+ &lt;span itemprop=&quot;about&quot; itemscope itemtype=&quot;https://schema.org/Thing&quot;&gt;+ &lt;a href=&quot;/authors/&#123;&#123; post.author &#125;&#125;&quot; itemprop=&quot;url&quot; rel=&quot;index&quot;&gt;+ &lt;span itemprop=&quot;name&quot;&gt;&#123;&#123; post.author &#125;&#125;&lt;/span&gt;+ &lt;/a&gt;+ &lt;/span&gt; &lt;span class=&quot;post-time&quot;&gt; &lt;span class=&quot;post-meta-item-icon&quot;&gt; &lt;i class=&quot;fa fa-calendar-o&quot;&gt;&lt;/i&gt; 在/layout下新创建author.swig文件 12345678910111213141516171819202122232425262728293031323334&#123;% extends &apos;_layout.swig&apos; %&#125;&#123;% import &apos;_macro/post-collapse.swig&apos; as post_template %&#125;&#123;% import &apos;_macro/sidebar.swig&apos; as sidebar_template %&#125;&#123;% block title %&#125; &#123;&#123; __(&apos;title.author&apos;) &#125;&#125;: &#123;&#123; page.author &#125;&#125; | &#123;&#123; config.title &#125;&#125; &#123;% endblock %&#125;&#123;% block content %&#125; &lt;div class=&quot;post-block category&quot;&gt; &lt;div id=&quot;posts&quot; class=&quot;posts-collapse&quot;&gt; &lt;div class=&quot;collection-title&quot;&gt; &lt;&#123;% if theme.seo %&#125;h2&#123;% else %&#125;h1&#123;% endif %&#125;&gt;&#123;# #&#125;&#123;&#123; page.author &#125;&#125;&#123;# #&#125;&lt;small&gt;&#123;&#123; __(&apos;title.author&apos;) &#125;&#125;&lt;/small&gt; &lt;/&#123;% if theme.seo %&#125;h2&#123;% else %&#125;h1&#123;% endif %&#125;&gt; &lt;/div&gt; &#123;% for post in page.posts %&#125; &#123;&#123; post_template.render(post) &#125;&#125; &#123;% endfor %&#125; &lt;/div&gt; &lt;/div&gt; &#123;% include &apos;_partials/pagination.swig&apos; %&#125;&#123;% endblock %&#125;&#123;% block sidebar %&#125; &#123;&#123; sidebar_template.render(false) &#125;&#125;&#123;% endblock %&#125; 修改/layout下page.swig文件 1234567891011121314151617181920212223242526 &#123;&#123; __(&apos;title.category&apos;) + page_title_suffix &#125;&#125; &#123;% elif page.type === &quot;tags&quot; %&#125; &#123;&#123; __(&apos;title.tag&apos;) + page_title_suffix &#125;&#125;+ &#123;% elif page.type === &quot;authors&quot; %&#125;+ &#123;&#123; __(&apos;title.author&apos;) + page_title_suffix &#125;&#125; &#123;% else %&#125; &#123;&#123; page.title + page_title_suffix &#125;&#125; &#123;% endif %&#125;。。。。。。。 &#123;&#123; list_categories() &#125;&#125; &lt;/div&gt; &lt;/div&gt;+ &#123;% elif page.type === &apos;authors&apos; %&#125;+ &lt;div class=&quot;author-all-page&quot;&gt;+ &lt;div class=&quot;author-all-title&quot;&gt;+ &#123;&#123; _p(&apos;counter.authors&apos;, site.authors.length) &#125;&#125;+ &lt;/div&gt;+ &lt;div class=&quot;author-all&quot;&gt;+ &#123;&#123; list_authors() &#125;&#125;+ &lt;/div&gt;+ &lt;/div&gt; &#123;% else %&#125; &#123;&#123; page.content &#125;&#125; &#123;% endif %&#125; 修改{项目名称}/themes/next下zh-Hans.yml文件 123456789101112131415title: archive: 归档 category: 分类 tag: 标签 schedule: 日程表 author : 作者 。。。counter: authors: zero: 暂无分类 one: 目前共计 1 个分类 other: "目前共计 %d 个作者" 在{项目名称}/themes/next/source/css/_common/components/pages/添加authors.styl，复制categories.styl内容将categorie改成author 在同级文件pages.styl中添加@import “authors”; 修改之后，运行以下命令就可以再你的主页看到啦 12$ hexo clean$ hexo d -g ###博文压缩 12$ npm install gulp -g$ npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save 在项目根目录下创建gulpfile.js并填入以下内容： 123456789101112131415161718192021222324252627282930313233var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html','minify-css','minify-js']); 生成博文是执行 hexo g &amp;&amp; gulp 就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>github</tag>
        <tag>hexo</tag>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
