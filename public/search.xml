<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Idea-Mac激活]]></title>
    <url>%2FIdea%2FIdea-Activation%2F</url>
    <content type="text"><![CDATA[首先下载jar包：百度网盘链接 密码:b8ye 将其放到合适的文件夹 进入idea（首次进入可以选择免费30天，激活码失效后进入免费30分钟）如果没有项目随便建个项目，点击菜单栏 Help -&gt; Edit Custom VM Options 注意：切记一定要通过 IDEA 来修改 .vmoptions 文件，不要手动直接去修改，现在 IDEA 针对反破解已经越来越严格 在末尾添加路径：-javaagent:/Users/XXXX/XXXX/jetbrainsCrack.jar 注意：补丁全路径中不要包含中文，否则，可能导致破解失败！ 重启idea！！！一定要重启 重启完成后，开始填入激活码，点击菜单栏 Help -&gt; Register: 1A82DEE284F-eyJsaWNlbnNlSWQiOiJBODJERUUyODRGIiwibGljZW5zZWVOYW1lIjoiaHR0cHM6Ly96aGlsZS5pbyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiJVbmxpbWl0ZWQgbGljZW5zZSB0aWxsIGVuZCBvZiB0aGUgY2VudHVyeS4iLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlMwIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiV1MiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSRCIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJDIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREMiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQiIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJNIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiRE0iLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJBQyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRQTiIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkdPIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUFMiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJDTCIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlBDIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlNVIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In1dLCJoYXNoIjoiODkwNzA3MC8wIiwiZ3JhY2VQZXJpb2REYXlzIjowLCJhdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlLCJpc0F1dG9Qcm9sb25nYXRlZCI6ZmFsc2V9-5epo90Xs7KIIBb8ckoxnB/AZQ8Ev7rFrNqwFhBAsQYsQyhvqf1FcYdmlecFWJBHSWZU9b41kvsN4bwAHT5PiznOTmfvGv1MuOzMO0VOXZlc+edepemgpt+t3GUHvfGtzWFYeKeyCk+CLA9BqUzHRTgl2uBoIMNqh5izlDmejIwUHLl39QOyzHiTYNehnVN7GW5+QUeimTr/koVUgK8xofu59Tv8rcdiwIXwTo71LcU2z2P+T3R81fwKkt34evy7kRch4NIQUQUno//Pl3V0rInm3B2oFq9YBygPUdBUbdH/KHROyohZRD8SaZJO6kUT0BNvtDPKF4mCT1saWM38jkw==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g== 13AGXEJXFK9-eyJsaWNlbnNlSWQiOiIzQUdYRUpYRks5IiwibGljZW5zZWVOYW1lIjoiaHR0cHM6Ly96aGlsZS5pbyIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiIiLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkFDIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRQTiIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJQUyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJHTyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJETSIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJDTCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSUzAiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUkMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUkQiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUEMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUk0iLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiV1MiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREIiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlNVIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9XSwiaGFzaCI6IjEyNzk2ODc3LzAiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-WGTHs6XpDhr+uumvbwQPOdlxWnQwgnGaL4eRnlpGKApEEkJyYvNEuPWBSrQkPmVpim/8Sab6HV04Dw3IzkJT0yTc29sPEXBf69+7y6Jv718FaJu4MWfsAk/ZGtNIUOczUQ0iGKKnSSsfQ/3UoMv0q/yJcfvj+me5Zd/gfaisCCMUaGjB/lWIPpEPzblDtVJbRexB1MALrLCEoDv3ujcPAZ7xWb54DiZwjYhQvQ+CvpNNF2jeTku7lbm5v+BoDsdeRq7YBt9ANLUKPr2DahcaZ4gctpHZXhG96IyKx232jYq9jQrFDbQMtVr3E+GsCekMEWSD//dLT+HuZdc1sAIYrw==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g== 1KNBB2QUUR1-eyJsaWNlbnNlSWQiOiJLTkJCMlFVVVIxIiwibGljZW5zZWVOYW1lIjoiZ2hib2tlIiwiYXNzaWduZWVOYW1lIjoiIiwiYXNzaWduZWVFbWFpbCI6IiIsImxpY2Vuc2VSZXN0cmljdGlvbiI6IiIsImNoZWNrQ29uY3VycmVudFVzZSI6ZmFsc2UsInByb2R1Y3RzIjpbeyJjb2RlIjoiSUkiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiQUMiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiRFBOIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlBTIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkdPIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRNIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkNMIiwiZmFsbGJhY2tEYXRlIjoiMjA4OS0wNy0wNyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJTMCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSRCIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJQQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSTSIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJXUyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQiIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQyIsImZhbGxiYWNrRGF0ZSI6IjIwODktMDctMDciLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSU1UiLCJmYWxsYmFja0RhdGUiOiIyMDg5LTA3LTA3IiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In1dLCJoYXNoIjoiMTI3OTY4NzcvMCIsImdyYWNlUGVyaW9kRGF5cyI6NywiYXV0b1Byb2xvbmdhdGVkIjpmYWxzZSwiaXNBdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlfQ==-1iV7BA/baNqv0Q5yUnAphUmh66QhkDRX+qPL09ICuEicBqiPOBxmVLLCVUpkxhrNyfmOtat2LcHwcX/NHkYXdoW+6aS0S388xe1PV2oodiPBhFlEaOac42UQLgP4EidfGQSvKwC9tR1zL5b2CJPQKZ7iiHh/iKBQxP6OBMUP1T7j3Fe1rlxfYPc92HRZf6cO+C0+buJP5ERZkyIn5ZrVM4TEnWrRHbpL8SVNq4yqfc+NwoRzRSNC++81VDS3AXv9c91YeZJz6JXO7AokIk54wltr42FLNuKbozvB/HCxV9PA5vIiM+kZY1K0w5ytgxEYKqA87adA7R5xL/crpaMxHQ==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g== 或者随便在网上找个激活码 点击激活，可以看到激活日期到2089年]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-内存模型]]></title>
    <url>%2FJAVA%2FJava-MemoryModel%2F</url>
    <content type="text"><![CDATA[JVM内存模型点击这里查看这篇文章 Java内存模型Java内存模型(Java Memory Model，简称JMM)，本身是种抽象的概念，并不是像硬件架构一样真实存在的；它描述的是一组规则或规范，通过这组规范定义了程序中各个变量(包括实例字段、静态字段和构成数组对象的元素)的访问方式。 主内存：共享的信息 工作内存：私有信息，基本数据类型，直接分配到工作内存，引用的地址存放在工作内存，引用的对象存放在堆中 工作方式： 线程修改私有数据，直接在工作空间修改 线程修改共享数据，把数据复制到工作空间中去，在工作空间中修改，修改完成以后，刷新内存中的数据 硬件架构 多CPU：一个现代计算机通常由两个或者多个CPU。其中一些CPU还有多核。从这一点可以看出，在一个有两个或者多个CPU的现代计算机上同时运行多个线程是可能的。每个CPU在某一时刻运行一个线程是没有问题的。这意味着，如果你的Java程序是多线程的，在你的Java程序中每个CPU上一个线程可能同时（并发）执行。 CPU寄存器：每个CPU都包含一系列的寄存器，它们是CPU内内存的基础。CPU在寄存器上执行操作的速度远大于在主存上执行的速度。这是因为CPU访问寄存器的速度远大于主存。 高速缓存cache：由于计算机的存储设备与处理器的运算速度之间有着几个数量级的差距，所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存（Cache）来作为内存与处理器之间的缓冲：将运算需要使用到的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了。CPU访问缓存层的速度快于访问主存的速度，但通常比访问内部寄存器的速度还要慢一点。每个CPU可能有一个CPU缓存层，一些CPU还有多层缓存。在某一时刻，一个或者多个缓存行（cache lines）可能被读到缓存，一个或者多个缓存行可能再被刷新回主存。 内存：一个计算机还包含一个主存。所有的CPU都可以访问主存。主存通常比CPU中的缓存大得多。 运作原理：通常情况下，当一个CPU需要读取主存时，它会将主存的部分读到CPU缓存中。它甚至可能将缓存中的部分内容读到它的内部寄存器中，然后在寄存器中执行操作。当CPU需要将结果写回到主存中去时，它会将内部寄存器的值刷新到缓存中，然后在某个时间点将值刷新回主存。 解决方案： 总线加锁 但是降低CPU的吞吐量 缓存上的一致性协议 缓存一致性协议(MESI)多核CPU硬件架构厂商，设计之初就预测到多线程操作数据不一致的问题，因此出现了——缓存一致性协议。 不同的CPU硬件生产厂商，具体的实现不一样。Intel的MESI协议最出名。MESI协议文档：https://en.wikipedia.org/wiki/MESI_protocol 在MESI协议中，每个Cache line有4个状态，可用2个bit表示，它们分别是： M(Modified): 这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。 E(Exclusive): 这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中。 S(Shared): 这行数据有效，数据和内存中的数据一致，数据存在于很多Cache中。 I(Invalid): 这行数据无效。 E状态示例如下：只有Core 0访问变量x，它的Cache line状态为E(Exclusive)。 S状态示例如下：3个Core都访问变量x，它们对应的Cache line为S(Shared)状态。 M状态和I状态示例如下：Core 0修改了x的值之后，这个Cache line变成了M(Modified)状态，其他Core对应的Cache line变成了I(Invalid)状态 有了MESI，为什么还需要JMM？既然有了MESI协议，是不是就不需要volatile的可见性语义了？当然不是，还有以下问题： 并不是所有的硬件架构都提供了相同的一致性保证，不同的硬件厂商实现不同，JVM需要volatile统一语义。 可见性问题不仅仅局限于CPU缓存内，JVM自己维护的内存模型(JMM)中也有可见性问题。使用volatile做标记，可以解决JVM层面的可见性问题。 Java线程与硬件处理器Java线程的实现是基于一对一的线程模型，实际上就是通过语言级别层面程序去间接调用系统内核的线程模型，即我们在使用Java线程时，Java虚拟机内部是转而调用当前操作系统的内核线程来完成当前任务。如图所示，每个线程最终都会映射到CPU中进行处理，如果CPU存在多核，那么一个CPU将可以并行执行多个线程任务。 Java内存模型与硬件内存架构的关系多线程的执行最终都会映射到硬件处理器上进行执行，但Java内存模型和硬件内存架构并不完全一致。对于硬件内存来说只有寄存器、缓存内存、主内存的概念，并没有工作内存(线程私有数据区域)和主内存(堆内存)之分，也就是说Java内存模型对内存的划分对硬件内存并没有任何影响,不管是工作内存的数据还是主内存的数据，对于计算机硬件来说都会存储在计算机主内存中，当然也有可能存储到CPU缓存或者寄存器中，因此总体上来说，Java内存模型和计算机硬件内存架构是一个相互交叉的关系，是一种抽象概念划分与真实物理硬件的交叉。 Java内存模型的必要性如下图，主内存中存在一个共享变量x，现在有A和B两线程分别对该变量x=1进行操作,A线程想要修改x的值为2，而B线程却想要读取x的值,那么B线程读取到到是1还是2呢？答案：都可能，这是不确定的，这也就是所谓的线程安全问题。为了解决类似上述的问题，JVM定义了一组规则，通过这组规则来决定一个线程对共享变量的写入何时对另一个线程可见。 JMM对三个特征的保证原子性操作不可分割 X=10 如果是私有数据具有原子性，如果是共享数据没原子性（需要先把10读到共享空间再把10写入x） Y=x 没有原子性 把数据X读到工作空间（原子性） 把X的值写到Y（原子性） I++ 没有原子性 读i到工作空间 +1 刷新结果到内存 多个原子性的操作合并到一起没有原子性,但是可以通过Synchronized和JUC中Lock的lock来保证原子性。 可见性线程只能操作自己工作空间中的数据，当一个线程修改了某个共享变量的值，其他线程是否能够马上得知这个修改的值。 Volatile:在JMM模型上实现MESI协议 Synchronized:加锁 JUC Lock的lock有序性有序性是指对于单线程的执行代码，我们总是认为代码的执行是按顺序依次执行的，对于单线程而言确实如此，但对于多线程环境，则可能出现乱序现象，因为程序编译成机器码指令后可能会出现指令重排现象，重排后的指令与原指令的顺序未必一致。 Volatile Synchronized Happens-before原则 程序次序原则，即在一个线程内必须保证语义串行性，也就是说按照代码顺序执行。 锁定原则：后一次加锁必须等前一次解锁 Volatile原则：volatile变量的写，先发生于读，这保证了volatile变量的可见性，简单的理解就是，volatile变量在每次被线程访问时，都强迫从主内存中读该变量的值，而当该变量发生变化时，又会强迫将最新的值刷新到主内存，任何时刻，不同的线程总是能够看到该变量的最新值。 传递原则：A先于B ，B先于C 那么A必然先于C 线程启动规则：如果线程A在执行线程B的start方法之前修改了共享变量的值，那么当线程B执行start方法时，线程A对共享变量的修改对线程B可见 线程终止规则：假设在线程B终止之前，修改了共享变量，线程A从线程B的join方法成功返回后，线程B对共享变量的修改将对线程A可见。 线程中断规则：对线程 interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测线程是否中断。 对象终结规则：对象的构造函数执行，结束先于finalize()方法]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机]]></title>
    <url>%2FJVM%2FJVM-Fundamentals%2F</url>
    <content type="text"><![CDATA[Java虚拟机(java virtual machine，JVM)，一种能够运行java字节码的虚拟机。作为一种编程语言的虚拟机，实际上不只是专用于Java语言，只要生成的编译文件匹配JVM对加载编译文件格式要求，任何语言都可以由JVM编译运行。 比如kotlin、scala等。 JVM基本结构JVM由三个主要的子系统构成 类加载子系统 运行时数据区(内存结构) 执行引擎 类加载机制类的生命周期 加载：将.class文件从磁盘读到内存 通过类的全限定名(com.xxx.xxx)+类加载器确定唯一的类，来获取定义此类的二进制字节流 将这个类字节流代表的静态存储结构转为方法区的运行时数据结构 在堆中生成一个代表此类的java.lang.Class对象，作为访问方法区这些数据结构的入口。 连接 验证：验证字节码文件的正确性 文件格式验证：基于字节流验证。 元数据验证：基于方法区的存储结构验证。 字节码验证：基于方法区的存储结构验证。 符号引用验证：基于方法区的存储结构验证。 准备：给类的静态变量分配内存，并赋予默认值（不包括实例变量） public static int value = 123; //此时在准备阶段过后的初始值为0而不是123，在初始化过程才会被赋值为123 public static final int value = 123;//value的值在准备阶段过后就是123。 解析：类装载器装入类所引用的其它所有类 初始化：为类的静态变量赋予正确的初始值，上述的准备阶段为静态变量赋予的是虚拟机默认的初始值，此处赋予的才是程序编写者为变量分配的真正的初始值，执行静态代码块 使用 卸载 类加载器的种类总体上分为两种：启动类加载器（C++实现） 和 其他类加载器（JAVA实现） 启动类加载器(Bootstrap ClassLoader)负责加载JRE的核心类库，如JRE目标下的rt.jar，charsets.jar等 扩展类加载器(Extension ClassLoader)负责加载JRE扩展目录ext中jar类包 系统类加载器(Application ClassLoader)负责加载ClassPath路径下的类包 用户自定义加载器(User ClassLoader)负责加载用户自定义路径下的类包 类加载机制全盘负责委托机制当一个ClassLoader加载一个类的时候，除非显示的使用另一个ClassLoader，该类所依赖和引用的类也由这个 ClassLoader载入 双亲委派机制指先委托父类加载器寻找目标类，在找不到的情况下，在自己的路径中查找并载入目标类 当有类需要加载，系统类加载器先判断有没有父类，有交给扩展类加载器加载 扩展类加载器判断有没有父类，有交给启动类加载器 启动类加载器没有父类，去实际加载该类，该类不是JRE包下的类，交给子类扩展类加载器去加载 扩展类加载器去加载该类，发现该类不是ext中的包，交给系统类加载器加载 系统类加载器加载，发现是classPath路径下的包，进行加载。 双亲委派模式的优势 沙箱安全机制:比如自己写的String.class类不会被加载，这样可以防止核心库被随意篡改 避免类的重复加载:当父ClassLoader已经加载了该类的时候，就不需要子ClassLoader再加载一次 为什么要打破双亲委派模式例如：tomcatTomcat是个web容器,可能需要部署两个应用程序，不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求同一个类库在同一个服务器只有一份，因此要保证每个应用程序的类库都是独立的，保证相互隔离。如果使用默认的类加载器机制，那么是无法加载两个相同类库的不同版本的，默认的类加载器是不管你是什么版本的，只在乎你的全限定类名，并且只有一份。 如何打破双亲委派模式 继承ClassLoader 重写findClass()方法 重写loadClass()方法 运行时数据区(内存结构) 虚拟机栈java虚拟机栈是线程私有的，每个方法执行都会创建一个栈帧，栈帧包含局部变量表、操作数栈、动态连接、方法出口等。 栈与栈帧每一个方法的执行到执行完成，对应着一个栈帧在虚拟机中从入栈到出栈的过程。java虚拟机栈栈顶的栈帧就是当前执行方法的栈帧。PC寄存器会指向该地址。当这个方法调用其他方法的时候久会创建一个新的栈帧，这个新的栈帧会被方法Java虚拟机栈的栈顶，变为当前的活动栈，在当前只有当前活动栈的本地变量才能被使用，当这个栈帧所有指令都完成的时候，这个栈帧被移除，之前的栈帧变为活动栈，前面移除栈帧的返回值变为这个栈帧的一个操作数。 栈帧栈帧包含局部变量表、操作数栈、动态连接、方法返回地址 局部变量表 局部变量表是变量值的存储空间，用于存放方法参数和方法内部定义的局部变量。在java编译成class文件的时候，就在方法的Code属性的max_locals数据项中确定该方法需要分配的最大局部变量表的容量。 局部变量表的容量以变量槽（Slot）为最小单位，32位虚拟机中一个Slot可以存放32位（4 字节）以内的数据类型（ boolean、byte、char、short、int、float、reference和returnAddress八种） 对于64位长度的数据类型（long，double），虚拟机会以高位对齐方式为其分配两个连续的Slot空间，也就是相当于把一次long和double数据类型读写分割成为两次32位读写。 reference类型虚拟机规范没有明确说明它的长度，但一般来说，虚拟机实现至少都应当能从此引用中直接或者间接地查找到对象在Java堆中的起始地址索引和方法区中的对象类型数据。 Slot是可以重用的，当Slot中的变量超出了作用域，那么下一次分配Slot的时候，将会覆盖原来的数据。Slot对对象的引用会影响GC（要是被引用，将不会被回收）。 系统不会为局部变量赋予初始值（实例变量和类变量都会被赋予初始值）。也就是说不存在类变量那样的准备阶段。 操作数栈 操作数栈和局部变量表一样，在编译时期就已经确定了该方法所需要分配的局部变量表的最大容量。 操作数栈的每一个元素可用是任意的Java数据类型，包括long和double。32位数据类型所占的栈容量为1，64位数据类型占用的栈容量为2。 当一个方法刚刚开始执行的时候，这个方法的操作数栈是空的，在方法执行的过程中，会有各种字节码指令往操作数栈中写入和提取内容，也就是出栈 / 入栈操作（例如：在做算术运算的时候是通过操作数栈来进行的，又或者在调用其它方法的时候是通过操作数栈来进行参数传递的）。 动态连接直接引用：有具体引用地址的指针，被引用的类、方法或者变量已经被加载到内存中符号引用：即用用字符串符号的形式来表示引用，其实被引用的类、方法或者变量还没有被加载到内存中。举个例子： 123456789/*** 符号引用*/String str = "abc";System.out.println("str=" + str);/*** 直接引用*/System.out.println("str=" + "abc"); 动态链接：在程序运行过程中，由符号引用转化为直接引用。静态链接：在类加载过程中，由符号引用转化为直接引用。 方法返回地址当一个方法开始执行时，可能有两种方式退出该方法： 正常完成出口 正常完成出口是指方法正常完成并退出，没有抛出任何异常(包括Java虚拟机异常以及执行时通过throw语句显示抛出的异常)。如果当前方法正常完成，则根据当前方法返回的字节码指令，这时有可能会有返回值传递给方法调用者(调用它的方法)，或者无返回值。具体是否有返回值以及返回值的数据类型将根据该方法返回的字节码指令确定。 异常完成出口 异常完成出口是指方法执行过程中遇到异常，并且这个异常在方法体内部没有得到处理，导致方法退出。 无论方法采用何种方式退出，在方法退出后都需要返回到方法被调用的位置，程序才能继续执行，方法返回时可能需要在当前栈帧中保存一些信息，用来帮他恢复它的上层方法执行状态。方法退出过程实际上就等同于把当前栈帧出栈，因此退出可以执行的操作有：恢复上层方法的局部变量表和操作数栈，把返回值(如果有的话)压如调用者的操作数栈中，调整PC计数器的值以指向方法调用指令后的下一条指令。一般来说，方法正常退出时，调用者的PC计数值可以作为返回地址，栈帧中可能保存此计数值。而方法异常退出时，返回地址是通过异常处理器表确定的，栈帧中一般不会保存此部分信息。 123456789101112public class Demo &#123; public int math()&#123; int a = 1; int b = 2; int c = (a + b)*10; return c; &#125; public static void main(String[] args) &#123; Demo demo = new Demo(); demo.math(); &#125;&#125; 当执行👆Demo的math方法时，主线程内存会如何操作第一步，现将1放入操作数栈第二步，将1放入局部变量表中第一个槽里第三步，第四步同上，最终将2放入局部变量表中第二个槽里第五步，将1复制一份放入操作数栈的栈顶第六步，将2复制一份放入操作数栈的栈顶第七步，将2，1弹出操作数栈交给cpu去运算得到3，放到操作数栈的栈顶第八步，从常量池（-128～127）里拿到10，放入操作数栈顶第九步，弹出10，3交给cpu去运算得到30，放到操作数栈的栈顶（jvm1.6开始进行了指令优化，第8、9步合并成了一步操作）第十步，将30放到局部变量表中第3个槽里。第十一步，将30复制一份放入操作数栈第栈顶。第十二步，将30弹出操作数栈，通过返回地址返回。 程序计数器就是一个指针，指向方法区中的方法字节码(用来存储指向下一跳指令的地址，也就是当前线程将要执行的指令代码)，由执行引擎读取下一条指令，是一个非常小的内存空间，几乎可以忽略不计，用来保证线程间切换后正确执行。 本地方法栈和栈作用很相似，区别不过是Java栈为JVM执行Java方法服务，而本地方法栈为JVM执行native方法服务。登记native方法，在Execution Engine执行时加载本地方法库。 方法区（永久代/持久代，元空间）类的所有字段和方法字节码，以及一些特殊方法如构造函数，接口代码也在这里定义。简单来说，所有定义的方法的信息都保存在该区域，静态变量+常量+类信息(构造方法/接口定义)+运行时常量池都存在方法区中，虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap(非堆)，目的应该是为了和Java的堆区分开(jdk1.8以前hotspot虚拟机叫永久代、持久代，jdk1.8时叫元空间) 永久代和元空间区别在于元数据区不在虚拟机当中，而是用的本地内存，永久代在虚拟机当中，永久代逻辑结构上也属于堆，但是物理上不属于。 为什么移除了永久代?参考官方解释http://openjdk.java.net/jeps/122大概意思是移除永久代是为融合HotSpot与 JRockit而做出的努力，因为JRockit没有永久代，不需要配置永久代。 堆虚拟机启动时自动分配创建，用于存放对象的实例，几乎所有对象都在堆上分配内存，当对象无法在该空间申请到内存是将抛出OutOfMemoryError异常。同时也是垃圾收集器管理的主要区域。 新生代(Young Generation)类出生、成长、消亡的区域，一个类在这里产生，应用，最后被垃圾回收器收集，结束生命。新生代分为两部分:伊甸区(Eden space)和幸存者区(Survivor space)，所有的类都是在伊甸区被new出来的。幸存区(Survivor space):分为From和To区,TO区永远保持空。当Eden区的空间用完是，程序又需要创建对象，JVM的垃圾回收器将Eden区进行垃圾回收(Minor GC)，将Eden区中的不再被其它对象应用的对象进行销毁。然后将Eden区中剩余的对象移到From Survivor区。若From Survivor区也满了，再对该区进行垃圾回收，然后移动到To Survivor区，From区为空后，将To和From区转换，保证To区为空，并且对象年龄加一。当对象年龄默认加到15（因为对象头只有4个bits是存对象年龄，最大为15）时将剩下的对象移到老年代。 老年代(Old Generation)新生代经过多次GC仍然存货的对象移动到老年区。若老年代也满了，这时候将发生Major GC(也可以叫Full GC)， 进行老年区的内存清理。若老年区执行了Full GC之后发现依然无法进行对象的保存，就会抛出 OOM(OutOfMemoryError)异常. GC算法和收集器几种常见GC：MinorGC/YoungGC 新生代OldGC CMS特有FullGC/MajorGC 回收所有MixedGC（FullGC+YoungGC） G1特有 如何判断对象可以被回收堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡(即不能再被任何途径使用的对象) 引用计数法给对象添加一个引用计数器，每当有一个地方引用，计数器就加1。当引用失效，计数器就减1。任何时候计数器为0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中没有选择这个算法来管理内存，最主要的原因是它很难解决对象之前相互循环引用的问题。所谓对象之间的相互引用问题，通过下面代码所示:除了对象a和b相互引用着对方之外，这两个对象之间再无任何引用。但是它们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数器法无法通知GC回收器回收它们。 1234567891011public class CounterGC&#123; Object instance = null; public static void main(String[] args)&#123; CounterGC a = new CounterGC(); CounterGC b = new CounterGC(); a.instance = b; b.instance = a; a = null; b = null; &#125;&#125; 可达性分析算法这个算法的基本思想就是通过一系列的称为”GC Roots”的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连的话，则证明此对象时不可用的。 GC Roots根节点:类加载器、Thread、虚拟机栈的局部变量表、static成员、常量引用、本地方法栈的变量等等. 如何判断一个常量是废弃常量运行时常量池主要回收的是废弃的常量。那么，我们怎么判断一个常量时废弃常量呢?假如在常量池中存在字符串”abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量”abc”就是废弃常量，如果这时发生内存回收的话而且有必要的话（内存不够用时才会发生回收），”abc”会被系统清理出常量池。 如何判断一个类是无用的类需要满足以下三个条件: 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。虚拟机可以对满足上述3个条件的无用类进行回收，这里仅仅是”可以“，而并不是和对象一样不适用了就必然会被回收。 垃圾回收算法 标记-清除算法它是最基础的收集算法，这个算法分为两个阶段，标记和清除。首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它有两个不足的地方: 效率问题，标记和清除两个过程的效率都不高; 空间问题，标记清除后会产生大量不连续的碎片; 复制算法为了解决效率问题，复制算法出现了。它可以把内存分为大小相同的两块，每次只使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块区，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收 标记-整理算法根据老年代的特点提出的一种标记算法，标记过程和“标记-清除”算法一样，但是后续步骤不是直接对可回收对象进行回收，而是让所有存活的对象向一段移动，然后直接清理掉边界以外的内存 分代收集算法现在的商用虚拟机的垃圾收集器基本都采用”分代收集”算法，这种算法就是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。在新生代中，每次收集都有大量对象死去，所以可以选择复制算法，只要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率时比较高的，而且没有额外的空间对它进行分配担保，就必须选择“标记-清除”或 者“标记-整理”算法进行垃圾收集。 垃圾收集器 Serial收集器Serial(串行)收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的单线程的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程( “Stop The World” )，直到它收集结束。新生代采用复制算法，老年代采用标记-整理算法。虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短(仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续)。但是Serial收集器有没有优于其他垃圾收集器的地方呢?当然有，它简单而高效(与其他收集器的单线程相比)。 Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是个不错的选择。 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为(控制参数、收集算法、回收策略等等)和Serial收集器完全一样。新生代采用复制算法，老年代采用标记-整理算法。它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器(真正意义上的并发收集器，后面会介绍到)配合工作。 Parallel Scavenge收集器(JDK1.8)Parallel Scavenge 收集器类似于ParNew收集器。Parallel Scavenge收集器关注点是吞吐量(高效率的利用CPU)。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间(提高用户体验)。 Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。新生代采用复制算法，老年代采用标记-整理算法。 Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器，采用标记-整理算法。它主要有两大用途:一种用途是在JDK1.5以及以前的版本 中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。 Parallel Old收集器Parallel Scavenge收集器的老年代版本。使用多线程和标记-整理算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。 CMS收集器并行和并发概念补充: 并行(Parallel) :指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发(Concurrent):指用户线程与垃圾收集线程同时执行(但不一定是并行，可能会交替执行)，用户程序 在继续运行，而垃圾收集器运行在另一个CPU上。 CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用。CMS(Concurrent Mark Sweep)收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程(基本上)同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种标记-清除算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤: 初始标记(CMS initial mark): 暂停所有的其他线程，并记录下直接与root相连的对象，速度很快 并发标记(CMS concurrent mark): 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记(CMS remark): 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶 段时间短 并发清除(CMS concurrent sweep): 开启用户线程，同时GC线程开始对为标记的区域做清扫。CMS主要优点:并发收集、低停顿。但是它有下面三个明显的缺点: 对CPU资源敏感; 无法处理浮动垃圾; 它使用的回收算法-标记-清除算法会导致收集结束时会有大量空间碎片产生。 G1收集器G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC停顿时间要求的同时,还具备高吞吐量性能特征.被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备一下特点: 并行与并发:G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU(CPU或者CPU核心)来缩短Stop- The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行 分代收集:虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。 空间整 合:与CMS的“标记–清理”算法不同，G1从整体来看是基于标记整理算法实现的收集器;从局部上来看是基于标记复制算法实现的 可预测的停顿:这是G1相对于CMS的另一个大优势，降低停顿时间是G1 和 CMS 共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内 G1收集器的运作大致分为以下几个步骤: 初始标记 并发标记 最终标记 筛选回收 G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名 字Garbage-First的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率(把内存化整为零)。 怎么选择垃圾收集器?（尽量由JVM自己选择） 优先调整堆的大小让服务器自己来选择 如果内存小于100m，使用串行收集器 如果是单核，并且没有停顿时间的要求，串行或JVM自己选择 如果允许停顿时间超过1秒，选择并行或者JVM自己选 如果响应时间最重要，并且不能超过1秒，使用并发收集器官方推荐ZGC(java最新版本垃圾收器器，可预测的停顿最低2ms)，性能高。 JDK性能调优监控工具虚拟机参数分析网站：https://www.perfma.com/product/opts jps显示当前系统的java进程情况 123456gdeMacBook-Pro:~ g$ jps94673 AppServiceApplication5499555011 RemoteMavenServer3694696 AppServiceApplication96956 Jps (空白的54995是idea) Jinfo查看正在运行的Java程序的扩展参数 查看JVM的参数123gdeMacBook-Pro:~ g$ jinfo -flags 94673VM Flags:-XX:-BytecodeVerificationLocal -XX:-BytecodeVerificationRemote -XX:CICompilerCount=4 -XX:ConcGCThreads=3 -XX:G1ConcRefinementThreads=10 -XX:G1HeapRegionSize=1048576 -XX:GCDrainStackTargetSize=64 -XX:InitialHeapSize=268435456 -XX:+ManagementServer -XX:MarkStackSize=4194304 -XX:MaxHeapSize=4294967296 -XX:MaxNewSize=2576351232 -XX:MinHeapDeltaBytes=1048576 -XX:NonNMethodCodeHeapSize=7549744 -XX:NonProfiledCodeHeapSize=244108496 -XX:ProfiledCodeHeapSize=0 -XX:ReservedCodeCacheSize=251658240 -XX:+SegmentedCodeCache -XX:TieredStopAtLevel=1 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC 查看java系统属性1jinfo -sysprops 94673 Jstatjstat命令可以查看堆内存各部分的使用量，以及加载类的数量。命令格式:jstat [-命令选项] [vmid] [间隔时间/毫秒] [查询次数] 123gdeMacBook-Pro:~ g$ jstat -gc 94673 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT CGC CGCT GCT 0.0 2048.0 0.0 2048.0 96256.0 68608.0 163840.0 132934.6 131280.0 127368.5 14720.0 13850.3 185 1.219 0 0.000 112 0.661 1.880 S0C：第一个幸存区的大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 EC：伊甸园区的大小 EU：伊甸园区的使用大小 OC：老年代大小 OU：老年代使用大小 MC：方法区大小 MU：方法区使用大小 CCSC:压缩类空间大小 CCSU:压缩类空间使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间 Jmap可以用来查看内存信息 堆的对象统计1gdeMacBook-Pro:~ g$ jmap -histo 94673 &gt; xxx.txt 部分如下 1234567891011121314151617181920212223242526272829303132 num #instances #bytes class name (module)------------------------------------------------------- 1: 380588 39857240 [B (java.base@11.0.4) 2: 71568 8387344 [Ljava.lang.Object; (java.base@11.0.4) 3: 301995 7247880 java.lang.String (java.base@11.0.4) 4: 25762 5316280 [I (java.base@11.0.4) 5: 47736 4200768 java.lang.reflect.Method (java.base@11.0.4) 6: 126275 4040800 java.util.concurrent.ConcurrentHashMap$Node (java.base@11.0.4) 7: 69142 3871952 java.util.LinkedHashMap (java.base@11.0.4) 8: 2483 3656120 [C (java.base@11.0.4) 9: 46566 3352752 io.netty.channel.DefaultChannelHandlerContext 10: 104582 3346624 java.util.HashMap$Node (java.base@11.0.4) 11: 20887 2551232 java.lang.Class (java.base@11.0.4) 12: 32461 2077504 java.util.concurrent.ConcurrentHashMap (java.base@11.0.4) 13: 48062 1922480 java.util.HashMap$KeyIterator (java.base@11.0.4) 14: 17400 1720040 [Ljava.util.HashMap$Node; (java.base@11.0.4) 15: 2184 1432704 io.netty.util.internal.shaded.org.jctools.queues.MpscArrayQueue 16: 35693 1427720 java.util.LinkedHashMap$Entry (java.base@11.0.4) 17: 88742 1419872 java.lang.Object (java.base@11.0.4) 18: 1671 1195280 [Ljava.util.concurrent.ConcurrentHashMap$Node; (java.base@11.0.4) 19: 10270 1150240 sun.nio.ch.SocketChannelImpl (java.base@11.0.4) 20: 10237 1064648 io.netty.channel.socket.nio.NioSocketChannel 21: 42359 1016616 java.util.ArrayList (java.base@11.0.4) 22: 60868 973888 java.lang.Integer (java.base@11.0.4) 23: 20436 817440 io.netty.util.DefaultAttributeMap$DefaultAttribute 24: 10533 758376 org.springframework.core.type.classreading.AnnotationMetadataReadingVisitor 25: 18570 742800 com.meituan.service.mobile.mtthrift.mtrace.MtraceServerTBinaryProtocol$Factory 26: 10239 737208 io.netty.channel.DefaultChannelPipeline$HeadContext 27: 10239 737208 io.netty.channel.DefaultChannelPipeline$TailContext 28: 30914 729768 [Ljava.lang.Class; (java.base@11.0.4) 29: 21894 700608 java.util.concurrent.locks.ReentrantLock$NonfairSync (java.base@11.0.4)------省略---- Num:序号 Instances:实例数量 Bytes:占用空间大小 Class Name:类名 堆信息123gdeMacBook-Pro:~ g$ jmap -heap 94673Error: -heap option usedCannot connect to core dump or remote debug server. Use jhsdb jmap instead jdk9及以上版本使用jmap -heap pid命令查看当前heap使用情况时，发现报错，提示需要使用jhsdb jmap来替代 1gdeMacBook-Pro:~ g$ jhsdb jmap --heap --pid 94673]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-装饰器模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Decorator%2F</url>
    <content type="text"></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-观察者模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Observer%2F</url>
    <content type="text"><![CDATA[认识观察者模式我们看看报纸和杂志的订阅是怎么回事: 报社的业务就是出版报纸。 向某家报社订阅报纸，只要他们有新报纸出版，就会给你送来。只要你是他们的订户，你就会一直收到新报纸。 当你不想再看报纸的时候，取消订阅，他们就不会再送新报纸来。 只要报社还在运营，就会一直有人(或单位)向他们订阅报 纸或取消订阅报纸。 观察者模式出版者+订阅者=观察者模式出版者改称为“主题”(Subject)，订阅者改称为“观察者”(Observer)。观察者模式：定义了对象之间的一对多依赖，当一个对象改变状态时，它的所有依赖者都会收到通知并自动更新。 示例建立气象站：该气象站必须建立在WeatherData对象上，由WeatherData对象负责追踪目前的天气状况(温度、湿度、气压)。希望能建立一个应用，有三种布告板，分别显示目前的状况、气象统计及简单的预报。当WeatherObject对象获得最新的测量数据时，三种布告板 必须实时更新。而且，这是一个可以扩展的气象站，Weather-O-Rama气象 站希望公布一组API，好让其他开发人员可以写出自己的气象布告板，并插入此应用中。 12345678public interface Subject &#123; // 注册 public void registerObserver(Observer o); // 移除 public void removeObserver(Observer o); // 通知 public void notifyObservers();&#125; 123public interface Observer &#123; public void update(float temp, float humidity, float pressure);&#125; 1234public interface DisplayElement &#123; // 显示布告板 public void display();&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.ArrayList;public class WeatherData implements Subject&#123; private ArrayList observers; // 温度 private float temperature; // 湿度 private float humidity; // 气压 private float pressure; public WeatherData() &#123; observers = new ArrayList(); &#125; @Override public void registerObserver(Observer o) &#123; observers.add(o); &#125; @Override public void removeObserver(Observer o) &#123; int i = observers.indexOf(o); if (i &gt;= 0) &#123; observers.remove(i); &#125; &#125; @Override public void notifyObservers() &#123; for (int i = 0; i &lt; observers.size(); i++) &#123; Observer observer = (Observer)observers.get(i); observer.update(temperature, humidity, pressure); &#125; &#125; // 当从气象站得到更新观测值时，我们通知观察者。 public void measurementsChanged() &#123; notifyObservers(); &#125; public void setMeasurements(float temperature, float humidity, float pressure) &#123; this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; measurementsChanged(); &#125; //其他方法省略&#125; 布告板 12345678910111213141516171819202122232425public class CurrentConditionsDisplay implements Observer, DisplayElement&#123; private float temperature; private float humidity; private Subject weatherData; public CurrentConditionsDisplay(Subject weatherData) &#123; this.weatherData = weatherData; weatherData.registerObserver(this); &#125; @Override public void display() &#123; System.out.println("Current conditions: " + temperature + "F degrees and " + humidity + "% humidity"); &#125; @Override public void update(float temp, float humidity, float pressure) &#123; this.temperature = temp; this.humidity = humidity; display(); &#125;&#125; 气象站测试类 1234567891011public class WeatherStation &#123; public static void main(String[] args) &#123; WeatherData weatherData = new WeatherData(); CurrentConditionsDisplay currentDisplay = new CurrentConditionsDisplay(weatherData);// StatisticsDisplay statisticsDisplay = new StatisticsDisplay(weatherData);// ForecastDisplay forecastDisplay = new ForecastDisplay(weatherData); weatherData.setMeasurements(80, 65, 30.4f); weatherData.setMeasurements(82, 70, 29.2f); weatherData.setMeasurements(78, 90, 29.2f); &#125;&#125; 运行结果 123Current conditions: 80.0F degrees and 65.0% humidityCurrent conditions: 82.0F degrees and 70.0% humidityCurrent conditions: 78.0F degrees and 90.0% humidity JAVA内置的观察者模式Java API有内置的观察者模式。java.util包(package)内包含最基本的Observer接口与Observable类。 Observer接口 123456package java.util;public interface Observer &#123; void update(Observable o, Object arg);&#125; Observable类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package java.util;public class Observable &#123; private boolean changed = false;// 数据更新标记 private Vector&lt;Observer&gt; obs; // 观察者列表 //构造函数 public Observable() &#123; obs = new Vector&lt;&gt;(); &#125; //添加观察者 public synchronized void addObserver(Observer o) &#123; if (o == null) throw new NullPointerException(); if (!obs.contains(o)) &#123; obs.addElement(o); &#125; &#125; // 删除某一个观察者 public synchronized void deleteObserver(Observer o) &#123; obs.removeElement(o); &#125; // 通知观察者 public void notifyObservers() &#123; notifyObservers(null); &#125; // 如果changed = true 意味着数据被修改，通知每个观察者。 public void notifyObservers(Object arg) &#123; Object[] arrLocal; synchronized (this) &#123; if (!changed) return; arrLocal = obs.toArray(); clearChanged(); &#125; for (int i = arrLocal.length-1; i&gt;=0; i--) ((Observer)arrLocal[i]).update(this, arg); &#125; // 删除所有观察者 public synchronized void deleteObservers() &#123; obs.removeAllElements(); &#125; // 设置changed值 protected synchronized void setChanged() &#123; changed = true; &#125; // 清除changed值 protected synchronized void clearChanged() &#123; changed = false; &#125; // 获取changed值 public synchronized boolean hasChanged() &#123; return changed; &#125; // 返回观察者个数 public synchronized int countObservers() &#123; return obs.size(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Observable;public class WeatherData extends Observable &#123; // 温度 private float temperature; // 湿度 private float humidity; // 气压 private float pressure; public WeatherData() &#123; &#125; // 当从气象站得到更新观测值时，我们通知观察者。 public void measurementsChanged() &#123; setChanged(); notifyObservers(); &#125; public void setMeasurements(float temperature, float humidity, float pressure) &#123; this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; measurementsChanged(); &#125; //使用“拉”的做法 public float getTemperature() &#123; return temperature; &#125; public float getHumidity() &#123; return humidity; &#125; public float getPressure() &#123; return pressure; &#125; //其他方法省略&#125; 12345678910111213141516171819202122232425262728293031import java.util.Observable;import java.util.Observer;public class CurrentConditionsDisplay implements Observer, DisplayElement&#123; private float temperature; private float humidity; Observable observable; public CurrentConditionsDisplay(Observable observable) &#123; this.observable = observable; observable.addObserver(this); &#125; @Override public void display() &#123; System.out.println("Current conditions: " + temperature + "F degrees and " + humidity + "% humidity"); &#125; @Override public void update(Observable o, Object arg) &#123; if (o instanceof WeatherData) &#123; WeatherData weatherData = (WeatherData)o; this.temperature = weatherData.getTemperature(); this.humidity = weatherData.getHumidity(); display(); &#125; &#125;&#125; 测试代码和测试结果同上 java.util.Observable的缺点java.util.Observable的实现 有许多问题，限制了它的使用和复用。 观察者是一个“类”而不是一个“接 口” 你必须设计一个类继承它。如果某类想同时 具有Observable类和另一个超类的行为，就会陷入两难，毕竟Java不支持多重继承。 Observable将关键的方法保护起来 setChanged()方法被保护起来了(被定义成 protected)。这意味着:除非你继承自Observable，否则你无法创建Observable实例并组合到你自己的对象中来。这个设计违反了第二个设计原 则:“多用组合，少用继承”。 要点 观察者模式定义了对象之间一对多的关系。 主题(也就是可观察者)用一个共同的接口来更新观察者。 观察者和可观察者之间用松耦合方式结合(loosecoupl- ing)，可观察者不知道观察者的细节，只知道观察者实现了观察者接口。 使用此模式时，你可从被观察者处推(push)或拉(pull)数据(然而，推的方式被认为更“正确”)。 有多个观察者时，不可以依赖特定的通知次序。 Java有多种观察者模式的实现，包括了通用的java.util.Observable。 要注意java.util.Observable实现上所带来的一些问题。 如果有必要的话，可以实现自己的Observable。 Swing大量使用观察者模式，许多GUI框架也是如此。 此模式也被应用在许多地方，例如:JavaBeans、RMI。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-策略模式]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FDesign-Pattern-Strategy%2F</url>
    <content type="text"><![CDATA[什么是策略模式策略这个词应该怎么理解，打个比方说，我们出门的时候会选择不同的出行方式，比如骑自行车、坐公交、坐火车、坐飞机、坐火箭等等，这些出行方式，每一种都是一个策略。 再比如我们去逛商场，商场现在正在搞活动，有打折的、有满减的、有返利的等等，其实不管商场如何进行促销，说到底都是一些算法，这些算法本身只是一种策略，并且这些算法是随时都可能互相替换的，比如针对同一件商品，今天打八折、明天满100减30，这些策略间是可以互换的。 策略模式（Strategy），定义了一组算法，将每个算法都封装起来，并且使它们之间可以互换。 示例模拟鸭子项目 12345678910public abstract class Duck &#123; public void Quack() &#123; System.out.println("~~gaga~~"); &#125; public abstract void display(); public void swim() &#123; System.out.println("~~im swim~~"); &#125;&#125; GreenHeadDuck继承Duck ： 1234567public class GreenHeadDuck extends Duck &#123; @Override public void display() &#123; System.out.println("**GreenHead**"); &#125;&#125; 新需求添加会飞的鸭子 123456public abstract class Duck &#123; //...; public void Fly() &#123; System.out.println("~~im fly~~"); &#125;&#125; 问题来了,这个Fly让所有子类都会飞了，这是不科学的。并非Duck所有的子类都会飞。在Duck超类中加上新的行为，会使得某些并不适合该行为的子类也具有该行为。这个导致，后面几十个鸭子不没有这个功能，不会飞，那么他们的都要去实现。工作量大，而且重复劳动。所以：超类挖的一个坑，每个子类都要来填，增加工作量，复杂度O(N^2)。不是好的设计方式 用策略模式来解决新需求需要新的设计方式，应对项目的扩展性，降低复杂度： 1）分析项目变化与不变部分，提取变化部分，然后把变化的部分抽象成接口+实现； 2）鸭子哪些功能是会根据新需求变化的？叫声、飞行… 重新设计模拟鸭子项目123456789101112131415161718192021222324public abstract class Duck &#123; FlyBehavior mFlyBehavior; QuackBehavior mQuackBehavior; public Duck() &#123; &#125; public void Fly() &#123; mFlyBehavior.fly(); &#125; public void Quack() &#123; mQuackBehavior.quack(); &#125; public abstract void display();&#125;public class GreenHeadDuck extends Duck &#123; public GreenHeadDuck() &#123; mFlyBehavior = new GoodFlyBehavior(); mQuackBehavior = new GaGaQuackBehavior(); &#125; @Override public void display() &#123; System.out.println("I’m a real GreenHeadDuck"); &#125;&#125; 总结 分析项目中变化部分与不变部分（方法论）——&gt;这个方法论不仅是策略模式中才可以用的，用来分析项目中变法的何不变化的，变化的就可以怎么来抽取替换。而且变化的抽离出来的行为族，行为族之间是可以来相互替换的。 多用组合少用继承；用行为类组合，而不是行为的继承。更有弹性 策略模式中的设计原则 开闭原则（Open-Closed Principle，缩写为OCP） 一个软件实体应当对扩展开放(例如对抽象层的扩展)，对修改关闭(例如对抽象层的修改)。即在设计一个模块的时候，应当使这个模块可以在不被修改的前提下被扩展。 开闭原则的关键，在于抽象。策略模式，是开闭原则的一个极好的应用范例。 里氏替换原则（Liskov Substitution Principle，缩写为LSP） 里氏替换原则里一个软件实体如果使用的是一个基类的话，那么一定适用于其子类，而且它根本不能察觉到基类对象和子类对象的区别。 比如，假设有两个类，一个是Base类，一个是Derived类，并且Derived类是Base类的子类。那么一个方法如果可以接受一个基类对象b的话：method1(Base b)，那么它必然可以接受一个子类对象d，也即可以有method1(d)。反之，则不一定成立。 里氏替换原则讲的是基类与子类的关系。只有当这种关系存在时，里氏替换关系才存在，反之则不存在。 策略模式之所以可行的基础便是里氏替换原则：策略模式要求所有的策略对象都是可以互换的，因此它们都必须是一个抽象策略角色的子类。在客户端则仅知道抽象策略角色类型，虽然变量的真实类型可以是任何一个具体策略角色的实例。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Guava-总结]]></title>
    <url>%2FGUAVA%2FGuava%2F</url>
    <content type="text"><![CDATA[项目相关信息官方首页：http://code.google.com/p/guava-libraries英文文档：http://www.ostools.net/apidocs/apidoc?api=guava中文文档：https://www.kancloud.cn/wizardforcel/java-opensource-doc/112616 博客 Guava 学习笔记：Google Guava 类库简介 Guava 学习笔记：Optional 优雅的使用null Guava学习笔记：Preconditions优雅的检验参数 Guava学习笔记：复写的Object常用方法 Guava学习笔记：Ordering犀利的比较器 Guava学习笔记：简化异常处理的Throwables类 Guava学习笔记：Immutable(不可变)集合 Guava学习笔记：Guava新增集合类型-Multiset Guava 学习笔记：Guava 新增集合类型 —— Multimap Guava学习笔记：Guava新增集合类型-Bimap Guava学习笔记：Guava新集合-Table等 Guava学习笔记：Guava cache Guava学习笔记：EventBus Guava学习笔记：Range]]></content>
      <categories>
        <category>GUAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>GUAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-CAS]]></title>
    <url>%2FJAVA%2FJava-Cas%2F</url>
    <content type="text"><![CDATA[CAS在看线程池源码的时候发现有很多CAS操作，那么什么是CAS？ 定义CAS是英文单词 Compare And Swap 的缩写，翻译过来就是比较并替换，它是一种原子操作，同时 CAS 是一种乐观机制。java.util.concurrent 包很多功能都是建立在 CAS 之上，如 ReenterLock 内部的 AQS，各种原子类，其底层都用 CAS来实现原子操作。 如何解决并发安全问题在我们认识 CAS 之前，我们是通过什么来解决并发带来的安全问题呢？volatile 关键字可以保证变量的可见性，但保证不了原子性；synchronized 关键字利用 JVM 字节码层面来实现同步机制，它是一个悲观锁机制。 123456public class Test &#123; public volatile int i; public void add() &#123; i++; &#125;&#125; 使用 javap -c Test.class 命令查看看add方法的字节码指令 123456789public void add(); Code: 0: aload_0 1: dup 2: getfield #2 // Field n:I 5: iconst_1 6: iadd 7: putfield #2 // Field n:I 10: return i++被拆分成了几个指令： 1. 执行getfield拿到原始i； 2. 执行iadd进行加1操作； 3. 执行putfield写把累加后的值写回i； 当线程 1 执行到加 1 步骤时，由于还没有执行赋值改变变量的值，这时候并不会刷新主内存区中的变量，如果此时线程 2 正好要拷贝该变量的值到自己私有缓存中，问题就出现了，当线程 2 拷贝完以后，线程1正好执行赋值运算，立马更新主内存区的值，那么此时线程 2 的副本就是旧的了，脏读又出现了。 怎么解决这个问题呢？在 add 方法加上 synchronized 修饰解决。 123456public class Test &#123; public volatile int i; public synchronized void add() &#123; i++; &#125;&#125; 这个方案当然可行，但是大大降低了性能。 CAS原理CAS机制当中使用了3个基本操作数：内存地址V，旧的预期值A，要修改的新值B。更新一个变量的时候，只有当变量的预期值A和内存地址V当中的实际值相同时，才会将内存值修改为 B 并返回 true，否则什么都不做，并返回 false。 源码分析下面以AtomicInteger的实现为例，分析一下CAS是如何实现的。 1234567891011121314151617public class AtomicInteger extends Number implements java.io.Serializable &#123; private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125; private volatile int value; // 省略部分代码&#125; Unsafe：是CAS的核心类，它可以提供硬件级别的原子操作，它可以获取某个属性在内存中的位置，也可以修改对象的字段值，其底层是用 C/C++valueOffset：表示该变量值在内存中的偏移地址，因为Unsafe就是根据内存偏移地址获取数据的。value：用volatile修饰，保证了多线程之间的内存可见性。 看看AtomicInteger如何实现并发下的累加操作： 1234567891011121314// AtomicInteger.getAndAddpublic final int getAndAdd(int delta) &#123; return unsafe.getAndAddInt(this, valueOffset, delta);&#125;// unsafe.getAndAddIntpublic final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; 假设线程A和线程B同时执行getAndAdd操作（分别跑在不同CPU上）： AtomicInteger里面的value原始值为 n，根据Java内存模型，线程A和线程B各自持有一份value的副本，值为n。 线程A通过getIntVolatile(var1, var2)拿到value值 n，这时线程A被挂起。 线程B也通过getIntVolatile(var1, var2)方法获取到value值 n，运气好，线程B没有被挂起，并执行compareAndSwapInt方法比较内存值也为 n，成功修改内存值为 m。 这时线程A恢复，执行compareAndSwapInt方法比较，发现自己手里的值(n)和内存的值(m)不一致，说明该值已经被其它线程提前修改过了，那只能重新来一遍了。 重新获取value值，因为变量value被volatile修饰，所以其它线程对它的修改，线程A总是能够看到，线程A继续执行compareAndSwapInt进行比较替换，直到成功。 继续深入看看Unsafe类中的compareAndSwapInt方法实现。 1public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); Java 并没有直接实现 CAS，CAS 相关的实现是通过 C++ 内联汇编的形式实现的。Java 代码需通过 JNI 才能调用，位于 unsafe.cpp，在OpenJDK8里的路径为: openjdk/hotspot/src/share/vm/prims/unsafe.cpp。 123456UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) UnsafeWrapper("Unsafe_CompareAndSwapInt"); oop p = JNIHandles::resolve(obj); jint* addr = (jint *) index_oop_from_field_offset_long(p, offset); return (jint)(Atomic::cmpxchg(x, addr, e)) == e;UNSAFE_END 逻辑执行流程： obj是AtomicInteger对象，通过 JNIHandles::resolve() 获取obj在内存中OOP实例p 根据成员变量value反射后计算出的内存偏移值offset去内存中取指针addr 获得更新值x、指针addr、期待值e三个参数后，调用Atomic::cmpxchg(x, addr, e) 通过Atomic::cmpxchg(x, addr, e)实现CAS对应OpenJDK8的路径是: openjdk/hotspot/src/share/vm/runtime/atomic.cpp 12345678910111213141516171819jbyte Atomic::cmpxchg(jbyte exchange_value, volatile jbyte* dest, jbyte compare_value) &#123; assert(sizeof(jbyte) == 1, "assumption."); uintptr_t dest_addr = (uintptr_t)dest; uintptr_t offset = dest_addr % sizeof(jint); volatile jint* dest_int = (volatile jint*)(dest_addr - offset); jint cur = *dest_int; jbyte* cur_as_bytes = (jbyte*)(&amp;cur); jint new_val = cur; jbyte* new_val_as_bytes = (jbyte*)(&amp;new_val); new_val_as_bytes[offset] = exchange_value; while (cur_as_bytes[offset] == compare_value) &#123; jint res = cmpxchg(new_val, dest_int, cur); if (res == cur) break; cur = res; new_val = cur; new_val_as_bytes[offset] = exchange_value; &#125; return cur_as_bytes[offset];&#125; 其中的cmpxchg为核心内容. 但是这句代码根据操作系统和处理器的不同, 使用不同的底层代码. 1#include "runtime/atomic.inline.hpp" atomic.inline.hpp中定义如下，可见不同不同操作系统, 不同的处理器, 都要走不同的cmpxchg()方法的实现. 123456789101112131415161718192021222324252627282930313233#include "runtime/atomic.hpp"// Linux#ifdef TARGET_OS_ARCH_linux_x86# include "atomic_linux_x86.inline.hpp"#endif#ifdef TARGET_OS_ARCH_linux_sparc# include "atomic_linux_sparc.inline.hpp"#endif#ifdef TARGET_OS_ARCH_linux_zero# include "atomic_linux_zero.inline.hpp"#endif#ifdef TARGET_OS_ARCH_linux_arm# include "atomic_linux_arm.inline.hpp"#endif#ifdef TARGET_OS_ARCH_linux_ppc# include "atomic_linux_ppc.inline.hpp"#endif// Solaris#ifdef TARGET_OS_ARCH_solaris_x86# include "atomic_solaris_x86.inline.hpp"#endif#ifdef TARGET_OS_ARCH_solaris_sparc# include "atomic_solaris_sparc.inline.hpp"#endif// Windows#ifdef TARGET_OS_ARCH_windows_x86# include "atomic_windows_x86.inline.hpp"#endif// ..省略 以其中的linux操作系统 x86处理器为例, atomic_linux_x86.inline.hpp在OpenJDK中路径如下: openjdk/hotspot/src/os_cpu/linux_x86/vm/atomic_linux_x86.inline.hpp 12345678910#define LOCK_IF_MP(mp) "cmp $0, " #mp "; je 1f; lock; 1: "inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) &#123; int mp = os::is_MP(); __asm__ volatile (LOCK_IF_MP(%4) "cmpxchgl %1,(%3)" : "=a" (exchange_value) : "r" (exchange_value), "a" (compare_value), "r" (dest), "r" (mp) : "cc", "memory"); return exchange_value;&#125; 已经开始内联汇编了，头疼 __asm__：表示汇编的开始volatile：表示禁止编译器优化cmpxchgl：就是汇编中x86的比较并交换指令了。LOCK_IF_MP：是个内联函数，根据当前系统是否为多核处理器决定是否为cmpxchg1指令添加lock前缀。 简单说下C内联汇编的语法格式： 1234567__asm__ volatile("Instruction List" : Output : Input : Clobber/Modify); instruction list：它是汇编指令列表Clobber/Modify：寄存器/内存修改标示。有时候,当你想通知GCC当前内联汇编语句可能会对某些寄存器或内存进行修改,希望GCC在编译时能够将这一点考虑进去;那么你就可以在Clobber/Modify部分声明这些寄存器或内存 所以上述汇编指令解释为：嵌入式汇编规定把输出和输入寄存器按统一顺序编号，顺序是从输出寄存器序列从左到右从上到下以%0开始，分别记为%0、%1···%9。也就是说，输出的eax是%0，输入的exchange_value、compare_value、dest、mp分别是%1、%2、%3、%4。然后看asm里的第一行指令，cmpxchgl %1,(%3)，比较eax(compare_value在eax中)与dest的值，如果相等，那么将exchange_value的值赋值给dest；否则，将dest的值赋值给eax。然后看输出: “=a” (exchange_value) 表示把eax中存的值(compare_value)写入exchange_value变量中。 Atomic::cmpxchg这个函数最终返回值是exchange_value，也就有两种情况： 如果cmpxchgl执行时compare_value和dest指针指向内存值相等则会使得dest指针指向内存值变成exchange_value，最终eax存的compare_value赋值给了exchange_value变量，即函数最终返回的值是原先的compare_value。此时Unsafe_CompareAndSwapInt的返回值(jint)(Atomic::cmpxchg(x, addr, e)) == e就是true，表明CAS成功。 如果cmpxchgl执行时compare_value和(dest)不等则会把当前dest指针指向内存的值写入eax，最终输出时赋值给exchange_value变量作为返回值，导致(jint)(Atomic::cmpxchg(x, addr, e)) == e得到false，表明CAS失败。 lock前缀在单处理器系统中是不需要加lock的，因为能够在单条指令中完成的操作都可以认为是原子操作，中断只能发生在指令与指令之间。在多处理器系统中,由于系统中有多个处理器在独立的运行，即使在能单条指令中完成的操作也可能受到干扰。 在所有的 X86 CPU 上都具有锁定一个特定内存地址的能力，当这个特定内存地址被锁定后，它就可以阻止其他的系统总线读取或修改这个内存地址。这种能力是通过 LOCK 指令前缀再加上前面的汇编指令来实现的。当使用 LOCK 指令前缀时，它会使 CPU 宣告一个 LOCK# 信号，这样就能确保在多处理器系统或多线程竞争的环境下互斥地使用这个内存地址。当指令执行完毕，这个锁定动作也就会消失。 缺点 CPU开销较大：在并发量比较高的情况下，如果许多线程反复尝试更新某一个变量，却又一直更新不成功，循环往复，会给CPU带来很大的压力。 不能保证代码块的原子性：CAS机制所保证的只是一个变量的原子性操作，而不能保证整个代码块的原子性。比如需要保证多个变量共同进行原子性的更新，就得使用Synchronized了。 ABA问题：这是CAS机制最大的问题所在。 ABA问题线程 1 从内存位置 V 取出 A，这时候线程 2 也从内存位置 V 取出 A，此时线程 1 处于挂起状态，线程 2 将位置 V 的值改成 B，最后再改成 A，这时候线程 1 再执行，发现位置 V 的值没有变化，尽管线程 1 也更改成功了，但内存地址V中的变量已经经历了A-&gt;B-&gt;A的改变。 举个例子：假设有一个遵循CAS原理的提款机，小灰有100元存款，要用这个提款机来提款50元。由于提款机硬件出了点小问题，小灰的提款操作被同时提交两次，开启了两个线程，两个线程都是获取当前值100元，要更新成50元。理想情况下，应该一个线程更新成功，另一个线程更新失败，小灰的存款只被扣一次。线程1首先执行成功，把余额从100改成50。线程2因为某种原因阻塞了。这时候，小灰的妈妈刚好给小灰汇款50元。线程2仍然是阻塞状态，线程3执行成功，把余额从50改成100。线程2恢复运行，由于阻塞之前已经获得了“当前值”100，并且经过compare检测，此时存款实际值也是100，所以成功把变量值100更新成了50。小灰凭空少了50元钱。 所以真正要做到严谨的CAS机制，我们在Compare阶段不仅要比较期望值A和地址V中的实际值，还要比较变量的版本号是否一致。在Java当中，AtomicStampedReference类就实现了用版本号做比较的CAS机制。 1234567891011private static class Pair&lt;T&gt; &#123; final T reference; final int stamp; private Pair(T reference, int stamp) &#123; this.reference = reference; this.stamp = stamp; &#125; static &lt;T&gt; Pair&lt;T&gt; of(T reference, int stamp) &#123; return new Pair&lt;T&gt;(reference, stamp); &#125;&#125; AtomicStampedReference 的内部类 Pair, reference 维护对象的引用，stamp 维护修改的版本号。 123456789101112public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) &#123; Pair&lt;V&gt; current = pair; return expectedReference == current.reference &amp;&amp; expectedStamp == current.stamp &amp;&amp; ((newReference == current.reference &amp;&amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp)));&#125; 从 compareAndSet 方法得知，如果要更改内存中的值，不但要值相同，还要版本号相同。 参考 https://www.jianshu.com/p/0e312402f6ca https://blog.csdn.net/dlh0313/article/details/52172833 https://www.cnblogs.com/noKing/p/9094983.html https://www.jianshu.com/p/fb6e91b013cc https://objcoding.com/2018/11/29/cas/ https://mp.weixin.qq.com/s/nRnQKhiSUrDKu3mz3vItWg]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>CAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-线程池]]></title>
    <url>%2FJAVA%2FJava-ThreadPool%2F</url>
    <content type="text"><![CDATA[线程概念操作系统调度的最小单元是线程，也叫轻量级进程（Light Weight Process），在一个进程里可以创建多个线程， 这些线程都拥有各自的计数器、 堆栈和局部变量等属性， 并且能够访问共享的内存变量。 处理器在这些线程上高速切换， 让使用者感觉到这些线程在同时执行。 线程的创建 通过继承Thread类来创建一个线程 实现Runnable接口并重写run()方法，new Thread(runnable).start()，线程启动时就会自动调用该对象的run方法 实现Callable接口并实现call()方法，使用FutureTask类包装Callable对象，使用FutureTask对象作为Thread对象的targer创建并启动线程；也可以使用线程池启动Runnable 和 Callable 的区别 1. Runnable规定方法是run方法，Callable规定方法是call方法 2. Runnable任务执行后无返回值，Callable任务执行后可返回值 3. run方法无法抛出异常，call方法可以抛出异常 4. 运行Callable任务可以拿到一个Future对象，Future表示异步计算结果，他提供了检查计算是否完成的方法，以等待计算完成并获取结果。计算完成后用get()方法获取结果，如果线程没有执行完，get()方法会阻塞当前线程执行。如果线程出现异常，get()方法会抛出异常。 线程池：Executors类提供了方便的工厂方法来创建不同类型的 executor services。无论Runnable还是Callable都可以被ThreadPoolExecutor或ScheduledThreadPoolExecutor执行1. public static ExecutorService newCachedThreadPool() 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程，但是在之前构造的线程可用时将重用它们。 2. public static ExecutorService newFixedThreadPool(int nThreads) 创建一个定长线程池，可控制线程最大并发数，以共享的无界队列方式来运行线程，超出的线程会在队列中等待。 3. public static ExecutorService newSingleThreadExecutor() 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，以无界队列方式来运行线程，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 4. public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) 创建一个周期线程池，支持定时及周期性任务执行。 5. public static ExecutorService newWorkStealingPool() 创建持有足够线程的线程池来支持给定的并行级别，并通过使用多个队列，减少竞争，它需要穿一个并行级别的参数，如果不传，则被设定为默认的CPU数量，这个线程池实际上是ForkJoinPool的扩展，适合使用在很耗时的任务中，能够合理的使用CPU进行并行操作。 线程的管理 ForkJoinPool 的每个工作线程都维护了一个工作队列(WorkQueue)，这是一个双端队列，里面存放的对象是任务(ForkJoinTask) 每个工作线程在运行中产生新的任务(通常是因为调用了fork())，会放入工作队列的队尾，并且工作线程在处理自己的工作队列时，使用的是LIFO方式，也就是每次从队尾取任务执行。 每个工作线程在处理自己的工作队列时，会尝试窃取一个任务(或是来自刚刚提交到pool的任务，或是来自其他的工作队列)，窃取的任务位于其他线程工作队列的队首，也就是使用FIFO方式。 在遇到join()时如果join的任务尚未完成，则会先处理其他任务，并等待其完成。 ExecutorCompletionService 内部维护了一个阻塞队列(BlockingQueue), 只有完成的任务才被加入到队列中。如果队列中的数据为空时, 调用take()就会阻塞直到有完成的任务加入队列，基于FutureTask实现。 线程池原理ThreadPoolExecutor123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; corePoolSize 核心线程数量，当有新任务在exectue()方法提交时，会执行以下判断：1. 如果运行的线程少于 corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的； 2. 如果线程池中的线程数量大于等于 corePoolSize 且小于 maximumPoolSize，则只有当workQueue满时才创建新的线程去处理任务； 3. 如果设置的corePoolSize 和 maximumPoolSize相同，则创建的线程池的大小是固定的，这时如果有新任务提交，若workQueue未满，则将请求放入workQueue中，等待有空闲的线程去从workQueue中取任务并处理； 4. 如果运行的线程数量大于等于maximumPoolSize，这时如果workQueue已经满了，则通过handler所指定的策略来处理任务； 5. 所以，任务提交时，判断的顺序为 corePoolSize –&gt; workQueue –&gt; maximumPoolSize maximumPoolSize 最大线程数量； keepAliveTime 线程池维护线程所允许的空闲时间。当线程池中的线程数量大于corePoolSize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了keepAliveTime； workQueue 保存等待执行的任务的阻塞队列，当提交一个新的任务到线程池以后, 线程池会根据当前线程池中正在运行着的线程的数量来决定对该任务的处理方式，主要有以下几种处理方式: 直接切换：这种方式常用的队列是SynchronousQueue 使用无界队列：一般使用基于链表的阻塞队列LinkedBlockingQueue。如果使用这种方式，那么线程池中能够创建的最大线程数就是corePoolSize，而maximumPoolSize就不会起作用了。当线程池中所有的核心线程都是RUNNING状态时，这时一个新的任务提交就会放入等待队列中。 使用有界队列：一般使用ArrayBlockingQueue。使用该方式可以将线程池的最大线程数量限制为maximumPoolSize，这样能够降低资源的消耗，但同时这种方式也使得线程池对线程的调度变得更困难，因为线程池和队列的容量都是有限的值，所以要想使线程池处理任务的吞吐率达到一个相对合理的范围，又想使线程调度相对简单，并且还要尽可能的降低线程池对资源的消耗，就需要合理的设置这两个数量。 threadFactory 它是ThreadFactory类型的变量，用来创建新线程。默认使用Executors.defaultThreadFactory() 来创建线程。使用默认的ThreadFactory来创建线程时，会使新创建的线程具有相同的NORM_PRIORITY优先级并且是非守护线程，同时也设置了线程的名称。 handler 它是RejectedExecutionHandler类型的变量，表示线程池的饱和策略。如果阻塞队列满了并且没有空闲的线程，这时如果继续提交任务，就需要采取一种策略处理该任务。线程池提供了4种策略： AbortPolicy：直接抛出异常，这是默认策略； CallerRunsPolicy：用调用者所在的线程来执行任务； DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy：直接丢弃任务； 核心源码线程池执行源码execute123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); // clt记录着runState和workerCount int c = ctl.get(); // workerCountOf方法取出低29位的值，表示当前活动的线程数； // 如果当前活动线程数小于corePoolSize，则新建一个线程放入线程池中，并把任务添加到该线程中； if (workerCountOf(c) &lt; corePoolSize) &#123; // addWorker中的第二个参数表示限制添加线程的数量是根据corePoolSize来判断还是maximumPoolSize来判断； // 如果为true，根据corePoolSize来判断； // 如果为false，则根据maximumPoolSize来判断 if (addWorker(command, true)) return; // 如果添加失败，则重新获取ctl值 c = ctl.get(); &#125; // 如果当前线程池是运行状态 并且 任务能够成功添加到工作队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; // 重新获取ctl值 int recheck = ctl.get(); // 再次判断线程池的运行状态，如果不是运行状态，由于之前已经把command添加到workQueue中了， // 这时需要移除该command // 执行过后通过handler使用拒绝策略对该任务进行处理，整个方法返回 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 获取线程池中的有效线程数，如果数量是0，则执行addWorker方法 // 1. 第一个参数为null，表示在线程池中创建一个线程，但不去启动； // 2. 第二个参数为false，将线程池的有限线程数量的上限设置为maximumPoolSize，添加线程时根据maximumPoolSize来判断； // 如果判断workerCount大于0，则直接返回，在workQueue中新增的command会在将来的某个时刻被执行。 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; // 如果执行到这里，有两种情况： // 1.线程池已经不是RUNNING状态； // 2.线程池是RUNNING状态，但workerCount &gt;= corePoolSize并且workQueue已满; // 这时，再次调用addWorker方法，但第二个参数传入为false，将线程池的有限线程数量的上限设置为maximumPoolSize； // 如果失败则拒绝该任务 else if (!addWorker(command, false)) reject(command);&#125; runState和workCount变量怎么存储在一个int中？参考：https://blog.csdn.net/weixin_34396902/article/details/94527424 addWorker123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105private boolean addWorker(Runnable firstTask, boolean core) &#123; // 循环CAS操作，将线程池中的线程数+1 retry: for (;;) &#123; // clt记录着runState和workerCount int c = ctl.get(); // 获取运行状态 int rs = runStateOf(c); // 如果rs &gt;= SHUTDOWN，则表示此时不再接收新任务； // 接着判断以下3个条件，只要有1个不满足，则返回false： // 1. rs == SHUTDOWN，这时表示关闭状态，不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务 // 2. firsTask为空 // 3. 阻塞队列不为空 // // rs == SHUTDOWN的情况 // 这种情况下不会接受新提交的任务，所以在firstTask不为空的时候会返回false； // 如果firstTask为空，并且workQueue也为空，因为队列中已经没有任务了，不需要再添加线程了，则返回false， if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; // 获取线程数 int wc = workerCountOf(c); // 如果wc超过CAPACITY(最大线程数线程数),也就是ctl的低29位的最大值（二进制是29个1），返回false； // core是addWorker方法的第二个参数,如果为true表示根据corePoolSize来比较，如果为false则根据maximumPoolSize来比较; if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // CAS操作尝试增加workerCount，修改clt的值+1，如果成功，则跳出第一个for循环 if (compareAndIncrementWorkerCount(c)) break retry; // 如果增加workerCount失败，则重新获取ctl的值 c = ctl.get(); // 如果当前的运行状态不等于rs，说明状态已被改变，返回第一个for循环继续执行 if (runStateOf(c) != rs) continue retry; &#125; &#125; // 新建线程，并加入到线程池workers中。 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; // 根据firstTask来创建Worker对象 w = new Worker(firstTask); // 每一个Worker对象都会创建一个线程 final Thread t = w.thread; if (t != null) &#123; // 对workers操作要通过加锁来实现 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 获取运行状态 int rs = runStateOf(ctl.get()); // rs &lt; SHUTDOWN表示是RUNNING状态； // 如果rs是RUNNING状态或者rs是SHUTDOWN状态并且firstTask为null，向线程池中添加线程。 // 因为在SHUTDOWN时不会在添加新的任务，但还是会执行workQueue中的任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; // 判断添加的任务状态,如果已经开始丢出异常 if (t.isAlive()) throw new IllegalThreadStateException(); // 将新建的线程加入到线程池中，workers是一个hashSet workers.add(w); int s = workers.size(); // largestPoolSize记录着线程池中出现过的最大线程数量 if (s &gt; largestPoolSize) largestPoolSize = s; //标记任务添加 workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 启动线程 t.start(); // 标记线程启动 workerStarted = true; &#125; &#125; &#125; finally &#123; // 线程添加线程池失败或者线程start失败，则需要调用addWorkerFailed函数 // 如果添加成功则需要移除线程，并恢复复clt的值 if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; t.start()这个语句，启动时会调用Worker类中的run方法，Worker本身实现了Runnable接口，所以一个Worker类型的对象也是一个线程。 Worker类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; private static final long serialVersionUID = 6138294804551838833L; /** 线程池中正真运行的线程。通过我们指定的线程工厂创建而来 **/ final Thread thread; /** 线程包装的任务。thread 在run时主要调用了该任务的run方法 */ Runnable firstTask; /** 记录当前线程完成的任务数 */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) &#123; setState(-1); // 在调用runWorker()前，禁止interrupt中断，在interruptIfStarted()方法中会判断 getState()&gt;=0 this.firstTask = firstTask; // 利用我们指定的线程工厂创建一个线程 this.thread = getThreadFactory().newThread(this); &#125; /** Delegates main run loop to outer runWorker */ public void run() &#123; runWorker(this); &#125; // Lock methods // // The value 0 represents the unlocked state. // The value 1 represents the locked state. protected boolean isHeldExclusively() &#123; return getState() != 0; &#125; /** * 尝试获取锁 */ protected boolean tryAcquire(int unused) &#123; //尝试一次将state从0设置为1，即“锁定”状态， if (compareAndSetState(0, 1)) &#123; //设置exclusiveOwnerThread=当前线程 setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; /** * 尝试释放锁 */ protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true; &#125; public void lock() &#123; acquire(1); &#125; public boolean tryLock() &#123; return tryAcquire(1); &#125; public void unlock() &#123; release(1); &#125; public boolean isLocked() &#123; return isHeldExclusively(); &#125; /** * 中断（如果运行） * shutdownNow时会循环对worker线程执行 * 且不需要获取worker锁，即使在worker运行时也可以中断 */ void interruptIfStarted() &#123; Thread t; // 如果state&gt;=0、t!=null、且t没有被中断 // new Worker()时state==-1，说明不能中断 if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125; &#125;&#125; Worker类投机取巧的继承了AbstractQueuedSynchronizer来简化在执行任务时的获取、释放锁,这样防止了中断在运行中的任务，只会唤醒(中断)在等待从workQueue中获取任务的线程.不直接执行execute(command)提交的command，而要在外面包一层Worker主要是为了使用用AQS锁控制中断，当运行时上锁，就不能中断，TreadPoolExecutor的shutdown()方法中断前都要获取worker锁，只有在等待从workQueue中获取任务getTask()时才能中断。 runWorker 方法在Worker类中的run方法调用了runWorker方法来执行任务. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); // 获取第一个任务 Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // 允许中断 // 是否因为异常退出循环 boolean completedAbruptly = true; try &#123; // 如果task为空，则通过getTask来获取任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt // 线程池处于stop状态或者当前线程被中断时，线程池状态是stop状态 // 但是当前线程没有中断，则发出中断请求 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; //开始执行任务前的Hook，类似回调函数 beforeExecute(wt, task); Throwable thrown = null; try &#123; //执行任务 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; //任务执行后的Hook，类似回调函数 afterExecute(task, thrown); &#125; &#125; finally &#123; //执行完毕后task重置，completedTasks计数器++，解锁 task = null; w.completedTasks++; w.unlock(); &#125; &#125; //标记正常退出 completedAbruptly = false; &#125; finally &#123; //线程空闲达到我们设定的值时，Worker退出销毁。 processWorkerExit(w, completedAbruptly); &#125;&#125; getTask 方法runWorker函数中最重要的是getTask()，不断的从阻塞队列中取任务交给线程执行，并且负责线程回收 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private Runnable getTask() &#123; // 表示上次从阻塞队列中取任务时是否超时 boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // 如果线程池处于shutdown状态， // 并且队列为空，或者线程池处于stop或者terminate状态， // 在线程池数量-1，返回null，回收线程 if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; // 获取线程数 int wc = workerCountOf(c); // timed变量用于判断是否需要进行超时控制。 // allowCoreThreadTimeOut默认是false，也就是核心线程不允许进行超时； // wc &gt; corePoolSize，表示当前线程池中的线程数量大于核心线程数量； // 对于超过核心线程数量的这些线程，需要进行超时控制 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; // 如果线程数目大于最大线程数目 或 当前操作需要进行超时控制，并且上次从阻塞队列中获取任务发生了超时 // 并且 线程数目大于1 或 工作队列为空 // 尝试将workerCount减1； if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; //**保证核心线程不被销毁** // 根据timed来判断，如果为true，则通过阻塞队列的poll方法进行超时控制，如果在keepAliveTime时间内没有获取到任务，则返回null； // 否则通过take方法，如果这时队列为空，则take方法会阻塞直到队列不为空。 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; // 如果 r == null，说明已经超时，timedOut设置为true，进入下一个循环 timedOut = true; &#125; catch (InterruptedException retry) &#123; // 如果获取任务时当前线程发生了中断，则设置timedOut为false并返回循环重试 timedOut = false; &#125; &#125;&#125; FutureTask源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; /** * state字段用来保存FutureTask内部的任务执行状态，一共有7中状态，每种状态及其对应的值如下 * NEW:表示是个新的任务或者还没被执行完的任务。这是初始状态。 * COMPLETING:任务已经执行完成或者执行任务的时候发生异常，但是任务执行结果或者异常原因还没有保存到outcome字段(outcome字段用来保存任务执行结果，如果发生异常，则用来保存异常原因)的时候，状态会从NEW变更到COMPLETING。但是这个状态会时间会比较短，属于中间状态。 * NORMAL:任务已经执行完成并且任务执行结果已经保存到outcome字段，状态会从COMPLETING转换到NORMAL。这是一个最终态。 * EXCEPTIONAL:任务执行发生异常并且异常原因已经保存到outcome字段中后，状态会从COMPLETING转换到EXCEPTIONAL。这是一个最终态。 * CANCELLED:任务还没开始执行或者已经开始执行但是还没有执行完成的时候，用户调用了cancel(false)方法取消任务且不中断任务执行线程，这个时候状态会从NEW转化为CANCELLED状态。这是一个最终态。 * INTERRUPTING: 任务还没开始执行或者已经执行但是还没有执行完成的时候，用户调用了cancel(true)方法取消任务并且要中断任务执行线程但是还没有中断任务执行线程之前，状态会从NEW转化为INTERRUPTING。这是一个中间状态。 * INTERRUPTED:调用interrupt()中断任务执行线程之后状态会从INTERRUPTING转换到INTERRUPTED。这是一个最终态。 * * NEW -&gt; COMPLETING -&gt; NORMAL 正常执行并返回 * NEW -&gt; COMPLETING -&gt; EXCEPTIONAL 执行过程中出现了异常 * NEW -&gt; CANCELLED 执行前被取消 * NEW -&gt; INTERRUPTING -&gt; INTERRUPTED 取消时被中断 */ private volatile int state; private static final int NEW = 0; private static final int COMPLETING = 1;//大于这个值就是完成状态 private static final int NORMAL = 2; private static final int EXCEPTIONAL = 3; private static final int CANCELLED = 4; private static final int INTERRUPTING = 5; private static final int INTERRUPTED = 6; /** The underlying callable; nulled out after running */ private Callable&lt;V&gt; callable; /** The result to return or exception to throw from get() */ private Object outcome; // non-volatile, protected by state reads/writes /** 执行callable的线程 **/ private volatile Thread runner; /** 使用Treiber算法实现的无阻塞的Stack，用于存放等待的线程 */ private volatile WaitNode waiters; @SuppressWarnings("unchecked") private V report(int s) throws ExecutionException &#123; // 拿到返回结果 Object x = outcome; // 判断状态 if (s == NORMAL) // 状态正常，就返回结果值 return (V)x; // 判断异常，就抛出异常。 if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x); &#125; /** * 构造方法 */ public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable &#125; /** * 这个构造方法会把传入的Runnable封装成一个Callable对象保存在callable字段中，同时如果任务执行成功的话就会返回传入的result。 * 这种情况下如果不需要返回值的话可以传入一个null。 */ public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable &#125; //判断任务是否被取消 public boolean isCancelled() &#123; return state &gt;= CANCELLED; &#125; //判断任务是否完成 public boolean isDone() &#123; return state != NEW; &#125; public boolean cancel(boolean mayInterruptIfRunning) &#123; // 1. 任务是new状态 并且 根据mayInterruptIfRunning把状态从NEW转化到INTERRUPTING或CANCELLED // 不符合上述状态，返回false if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // 2. 如果需要中断任务执行线程 if (mayInterruptIfRunning) &#123; try &#123; // runner保存着当前执行任务的线程 Thread t = runner; if (t != null) //中断任务执行线程 t.interrupt(); &#125; finally &#123; // final state // 修改状态为INTERRUPTED UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true; &#125; public V get() throws InterruptedException, ExecutionException &#123; int s = state; // 判断任务当前的state &lt;= COMPLETING是否成立。 if (s &lt;= COMPLETING) // 如果成立，表明任务还没有结束(这里的结束包括任务正常执行完毕，任务执行异常，任务被取消) // 调用awaitDone()进行阻塞等待。 s = awaitDone(false, 0L); // 任务已经结束，调用report()返回结果。 return report(s); &#125; public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; if (unit == null) throw new NullPointerException(); int s = state; // 如果awaitDone()超时返回之后任务还没结束，则抛出异常 if (s &lt;= COMPLETING &amp;&amp; (s = awaitDone(true, unit.toNanos(timeout))) &lt;= COMPLETING) throw new TimeoutException(); return report(s); &#125; protected void done() &#123; &#125; protected void set(V v) &#123; // 尝试CAS操作，把当前的状态从NEW变更为COMPLETING(中间状态)状态。 if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; // 把任务执行结果保存在outcome字段中。 outcome = v; // CAS的把当前任务状态从COMPLETING变更为NORMAL UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // final state finishCompletion(); &#125; &#125; protected void setException(Throwable t) &#123; // 尝试CAS操作，把当前的状态从NEW变更为COMPLETING(中间状态)状态。 if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; // 把异常原因保存在outcome字段中，outcome字段用来保存任务执行结果或者异常原因。 outcome = t; // CAS的把当前任务状态从COMPLETING变更为EXCEPTIONAL。 UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); // final state finishCompletion(); &#125; &#125; public void run() &#123; // 状态如果不是NEW，说明任务或者已经执行过，或者已经被取消，直接返回 // 状态如果是NEW，则尝试把当前执行线程保存在runner字段(runnerOffset)中，如果赋值失败则直接返回 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; // 只有初始状态才会执行 if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; // 执行任务 计算逻辑 result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; // 保存异常 setException(ex); &#125; if (ran) // 任务执行成功，保存返回结果 set(result); &#125; &#125; finally &#123; // 无论是否执行成功，把runner设置为null runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; // 如果任务被中断，执行中断处理 if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; &#125; /** * 与run方法类似，区别在于这个方法不会设置任务的执行结果值 * * @return &#123;@code true&#125; if successfully run and reset */ protected boolean runAndReset() &#123; if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return false; boolean ran = false; int s = state; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; s == NEW) &#123; try &#123; // 不获取和设置返回值 c.call(); // don't set result ran = true; &#125; catch (Throwable ex) &#123; setException(ex); &#125; &#125; &#125; finally &#123; runner = null; s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; // 是否正确的执行并复位 return ran &amp;&amp; s == NEW; &#125; private void handlePossibleCancellationInterrupt(int s) &#123; if (s == INTERRUPTING) while (state == INTERRUPTING) Thread.yield(); // wait out pending interrupt // 确保cancel(true)产生的中断发生在run或runAndReset方法执行的过程中。 //这里会循环的调用Thread.yield()来确保状态在cancel方法中被设置为INTERRUPTED。 &#125; /** * Simple linked list nodes to record waiting threads in a Treiber * stack. See other classes such as Phaser and SynchronousQueue * for more detailed explanation. */ static final class WaitNode &#123; volatile Thread thread; volatile WaitNode next; WaitNode() &#123; thread = Thread.currentThread(); &#125; &#125; /** * Removes and signals all waiting threads, invokes done(), and * nulls out callable. */ private void finishCompletion() &#123; // assert state &gt; COMPLETING; // 执行该方法时state必须大于COMPLETING // 依次遍历waiters链表 for (WaitNode q; (q = waiters) != null;) &#123; // 设置栈顶节点为null if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; for (;;) &#123; Thread t = q.thread; if (t != null) &#123; q.thread = null; // 唤醒等待线程 LockSupport.unpark(t); &#125; WaitNode next = q.next; // 如果next为空，说明栈空了，跳出循环 if (next == null) break; // 方便gc回收 q.next = null; // 重新设置栈顶node q = next; &#125; break; &#125; &#125; // 空方法，留给子类扩展 done(); callable = null; // to reduce footprint &#125; /** * Awaits completion or aborts on interrupt or timeout. * * @param timed true if use timed waits * @param nanos time to wait, if timed * @return state upon completion */ private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; // 计算等待截止时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; for (;;) &#123; // 1. 判断阻塞线程是否被中断 if (Thread.interrupted()) &#123; // 被中断则在等待队列中删除该节点 removeWaiter(q); // 抛出InterruptedException异常 throw new InterruptedException(); &#125; int s = state; // 2. 获取当前状态，如果状态大于COMPLETING if (s &gt; COMPLETING) &#123; // 说明任务已经结束(要么正常结束，要么异常结束，要么被取消) if (q != null) // 把thread显示置空 q.thread = null; // 返回结果 return s; &#125; // 3. 如果状态处于中间状态COMPLETING // 表示任务已经结束但是任务执行线程还没来得及给outcome赋值 else if (s == COMPLETING) // cannot time out yet Thread.yield();// 让出执行权让其他线程优先执行 // 4. 如果等待节点为空，则构造一个等待节点 else if (q == null) q = new WaitNode(); // 5. 如果还没有入队列，则把当前节点加入waiters首节点并替换原来waiters else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); else if (timed) &#123; // 如果需要等待特定时间，则先计算要等待的时间 // 如果已经超时，则删除对应节点并返回对应的状态 nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; // 6. 阻塞等待特定时间 LockSupport.parkNanos(this, nanos); &#125; // 6. 阻塞等待直到被其他线程唤醒 else LockSupport.park(this); &#125; &#125; private void removeWaiter(WaitNode node) &#123; if (node != null) &#123; // 将thread设置为null是因为下面要根据thread是否为null判断是否要把node移出 node.thread = null; // 这里自旋保证删除成功 retry: for (;;) &#123; // restart on removeWaiter race for (WaitNode pred = null, q = waiters, s; q != null; q = s) &#123; s = q.next; // q.thread != null说明该q节点不需要移除 if (q.thread != null) pred = q; // 如果q.thread == null，且pred != null，需要删除q节点 else if (pred != null) &#123; // 删除q节点 pred.next = s; // pred.thread == null时说明在并发情况下被其他线程修改了； // 返回第一个for循环重试 if (pred.thread == null) // check for race continue retry; &#125; // 如果q.thread != null且pred == null，说明q是栈顶节点 // 设置栈顶元素为s节点，如果失败则返回重试 else if (!UNSAFE.compareAndSwapObject(this, waitersOffset, q, s)) continue retry; &#125; break; &#125; &#125; &#125; // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long stateOffset; private static final long runnerOffset; private static final long waitersOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = FutureTask.class; stateOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("state")); runnerOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("runner")); waitersOffset = UNSAFE.objectFieldOffset (k.getDeclaredField("waiters")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; 线程池中的线程初始化 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程； prestartAllCoreThreads()：初始化所有核心线程 线程池的关闭ThreadPoolExecutor提供了两个方法，用于线程池的关闭 shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务 线程池大小 粗略 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1 如果是IO密集型任务，参考值可以设置为2*NCPU 精确：（(线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 最佳：压测 任务缓存队列workQueue，它用来存放等待执行的任务。BlockingQueue 是个接口，你需要使用它的实现之一来使用BlockingQueue，java.util.concurrent包下具有以下 BlockingQueue 接口的实现类： ArrayBlockingQueue：ArrayBlockingQueue 是一个有界的阻塞队列，其内部实现是将对象放到一个数组里。有界也就意味着，它不能够存储无限多数量的元素。它有一个同一时间能够存储元素数量的上限。你可以在对其初始化的时候设定这个上限，但之后就无法对这个上限进行修改了(译者注：因为它是基于数组实现的，也就具有数组的特性：一旦初始化，大小就无法修改)。 LinkedBlockingQueue：LinkedBlockingQueue 内部以一个链式结构(链接节点)对其元素进行存储。如果需要的话，这一链式结构可以选择一个上限。如果没有定义上限，将使用 Integer.MAX_VALUE 作为上限。 DelayQueue：DelayQueue 对元素进行持有直到一个特定的延迟到期。注入其中的元素必须实现 java.util.concurrent.Delayed 接口。 PriorityBlockingQueue：PriorityBlockingQueue 是一个无界的并发队列。它使用了和类 java.util.PriorityQueue 一样的排序规则。你无法向这个队列中插入 null 值。所有插入到 PriorityBlockingQueue 的元素必须实现 java.lang.Comparable 接口。因此该队列中元素的排序就取决于你自己的 Comparable 实现。 SynchronousQueue：SynchronousQueue 是一个特殊的队列，它的内部同时只能够容纳单个元素。如果该队列已有一元素的话，试图向队列中插入一个新元素的线程将会阻塞，直到另一个线程将该元素从队列中抽走。同样，如果该队列为空，试图向队列中抽取一个元素的线程将会阻塞，直到另一个线程向队列中插入了一条新的元素。据此，把这个类称作一个队列显然是夸大其词了。它更多像是一个汇合点。 线程池总结 线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。 当调用 execute() 方法添加一个任务时，线程池会做如下判断： 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务； 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列； 如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务； 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会抛出异常RejectExecutionException。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-索引与算法]]></title>
    <url>%2FMysql%2FMysql05%2F</url>
    <content type="text"><![CDATA[InnoDB存储引擎支持以下几种常见的索引： B+树索引、全文索引、哈希索引 二叉查找树在介绍B+树前，需要先了解一下二叉查找树。B+树是通过二叉查找树，再由平衡二叉树，B树演化而来。 定义左孩子比父节点小，右孩子比父节点大，中序遍历可以得到键值的排序输出。 插入 删除单孩子的情况：如果删除的节点有左孩子那就把左孩子顶上去，如果有右孩子就把右孩子顶上去 左右都有孩子的情况：可以这么想象，如果我们要删除一个数组的元素，那么我们在删除后会将其后面的一个元素顶到被删除的位置。二叉树操作同样，我们根据中序遍历找到要删除结点的后一个结点，然后顶上去，原理跟数组一样。 查找查找的平均时间复杂度log(N)，在最坏的情况下会出现链表的形式，复杂度退化到O(N)。 平衡二叉树当二叉查找树以完全二叉树的形式展现，这样我才能做到查找是严格的O(logN)， 定义首先符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度最大差为1。 旋转节点再怎么失衡都逃不过4种情况 左子树的左边节点 右子树的右边节点 左子树的右边节点找到失衡点，失衡点的左子树进行右子树的右边节点情况旋转，然后进行左子树的左边节点旋转 右子树的左边节点 插入&amp;删除步骤同二叉查找树，只是在插入或删除节点之后多了一步旋转的过程 B+树]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql-存储程序]]></title>
    <url>%2FMysql%2FMysql-storage%2F</url>
    <content type="text"><![CDATA[MySQL中的存储程序本质上封装了一些可执行的语句，然后给用户提供一种简单的调用方式来执行这些语句，根据调用方式的不同，我们可以把存储程序分为存储例程、触发器和事件这几种类型。其中，存储例程又可以被细分为存储函数和存储过程。 自定义变量MySQL中对我们自定义的变量的命名有个要求，那就是变量名称前必须加一个@符号。我们自定义变量的值的类型可以是任意MySQL支持的类型，例如我们自定义一个变量a： 12mysql&gt; SET @a = 1;Query OK, 0 rows affected (0.00 sec) 如果我们想查看这个变量的值的话，使用SELECT语句就好了，不过仍然需要在变量名称加一个@符号： 1234567mysql&gt; SELECT @a;+------+| @a |+------+| 1 |+------+1 row in set (0.00 sec) 同一个变量也可以存储存储不同类型的值，比方说我们再把一个字符串值赋值给变量a： 12345678910mysql&gt; SET @a = '啦';Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT @a;+------+| @a |+------+| 啦 |+------+1 row in set (0.00 sec) 除了把一个常量赋值给一个变量以外，我们还可以把一个变量赋值给另一个变量： 12345678910mysql&gt; SET @b = @a;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT @b;+------+| @b |+------+| 啦 |+------+1 row in set (0.00 sec) 我们还可以将某个查询的结果赋值给一个变量，前提是这个查询的结果只有一个值： 12mysql&gt; SET @a = (SELECT first_column FROM first_table LIMIT 1);Query OK, 0 rows affected (0.00 sec) 还可以用另一种形式的语句来将查询的结果赋值给一个变量： 12mysql&gt; SELECT first_column FROM first_table LIMIT 1 INTO @b;Query OK, 1 row affected (0.00 sec) 我们查看一下这两个变量的值： 1234567mysql&gt; SELECT @a, @b;+------+------+| @a | @b |+------+------+| 1 | 1 |+------+------+1 row in set (0.00 sec) 如果我们的查询结果是一条记录，该记录中有多个列的值的话，我们想把这几个值分别赋值到不同的变量中，只能使用INTO语句了： 12345678910mysql&gt; SELECT first_column, second_column FROM first_table LIMIT 1 INTO @a, @b;Query OK, 1 row affected (0.00 sec)mysql&gt; SELECT @a, @b; +------+------+| @a | @b |+------+------+| 1 | aaa |+------+------+1 row in set (0.00 sec) 复合语句在MySQL客户端的交互界面处，当我们完成键盘输入并按下回车键时，MySQL客户端会检测我们输入的内容中是否包含;、\g或者\G这三个符号之一，如果有的话，会把我们输入的内容发送到服务器。这样一来，如果我们想给服务器发送复合语句（也就是由一条或多条语句组成的语句）的话，就需要把这些语句写到一行中，比如这样： 123456789101112131415161718mysql&gt; SELECT first_column FROM first_table ;SELECT second_column FROM first_table;+--------------+| first_column |+--------------+| 1 || 2 || NULL |+--------------+3 rows in set (0.00 sec)+---------------+| second_column |+---------------+| aaa || NULL || ccc |+---------------+3 rows in set (0.00 sec) 我们也可以用delimiter命令来自定义MySQL的检测输入结束的符号，如下： 123456789101112131415161718192021mysql&gt; delimiter $mysql&gt; SELECT first_column FROM first_table ; -&gt; SELECT second_column FROM first_table; -&gt; $+--------------+| first_column |+--------------+| 1 || 2 || NULL |+--------------+3 rows in set (0.00 sec)+---------------+| second_column |+---------------+| aaa || NULL || ccc |+---------------+3 rows in set (0.00 sec) delimiter $命令意味着修改MySQL客户端检测输入结束的符号为$,也可以使用任何符号来作为MySQL客户端检测输入结束的符号，也包括多个字符，如下： 123456789101112131415161718192021mysql&gt; delimiter 666mysql&gt; SELECT first_column FROM first_table; -&gt; SELECT second_column FROM first_table; -&gt; 666+--------------+| first_column |+--------------+| 1 || 2 || NULL |+--------------+3 rows in set (0.00 sec)+---------------+| second_column |+---------------+| aaa || NULL || ccc |+---------------+3 rows in set (0.00 sec) 存储函数创建存储函数存储函数其实就是一种函数，只不过在这个函数里可以执行命令语句而已。MySQL中定义存储函数的语句如下： 12345CREATE FUNCTION 存储函数名称([参数列表])RETURNS 返回值类型BEGIN 函数体内容END 举个🌰： 123456789mysql&gt; delimiter $mysql&gt; CREATE FUNCTION second_column(a INT) -&gt; RETURNS VARCHAR(100) -&gt; BEGIN -&gt; RETURN (SELECT second_column FROM first_table WHERE first_column = a); -&gt; END $Query OK, 0 rows affected (0.00 sec)mysql&gt; delimiter ; 存储函数的调用我们自定义的函数和系统内置函数的使用方式是一样的，都是在函数名后加小括号()表示函数调用 1234567mysql&gt; SELECT second_column(1);+------------------+| second_column(1) |+------------------+| aaa |+------------------+1 row in set (0.00 sec) 查看存储函数查看定义了多少个存储函数: 1SHOW FUNCTION STATUS [LIKE 需要匹配的函数名] 查看某个函数的具体定义: 1SHOW CREATE FUNCTION 函数名 删除存储函数删除某个存储函数 1DROP FUNCTION 函数名 在函数体中定义变量在函数体中使用变量前必须先声明这个变量，函数体中的变量名不允许加@前缀,声明方式如下： 12345678910111213DECLARE 变量名 数据类型 [DEFAULT 默认值]; mysql&gt; delimiter $mysql&gt; CREATE FUNCTION var_demo(a INT) -&gt; RETURNS INT -&gt; BEGIN -&gt; DECLARE b INT; -&gt; SET b = 5; -&gt; RETURN b+a; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; delimiter ; 我们调用一下这个函数： 1234567mysql&gt; SELECT var_demo(2);+-------------+| var_demo(2) |+-------------+| 7 |+-------------+1 row in set (0.00 sec) 如果不对声明的变量赋值，它的默认值就是NULL，也可以通过DEFAULT子句来显式的指定变量的默认值. 123456789101112131415161718mysql&gt; delimiter $mysql&gt; CREATE FUNCTION var_default_demo()-&gt; RETURNS INT-&gt; BEGIN-&gt; DECLARE c INT DEFAULT 1;-&gt; RETURN c;-&gt; END $Query OK, 0 rows affected (0.00 sec)mysql&gt; delimiter ;mysql&gt; SELECT var_default_demo();+--------------------+| var_default_demo() |+--------------------+| 1 |+--------------------+1 row in set (0.00 sec) 参数定义比如我们上边编写的这个second_column函数： 12345mysql&gt; CREATE FUNCTION second_column(a INT) -&gt; RETURNS VARCHAR(100) -&gt; BEGIN -&gt; RETURN (SELECT second_column FROM first_table WHERE first_column = a); -&gt; END $ 需要注意的是，参数名不要和函数体语句中其他的变量名、命令语句的标识符冲突。并且函数参数不可以指定默认值，我们在调用函数的时候，必须显式的指定所有的参数，参数类型也一定要匹配 判断语句语法格式如下： 1234567IF 布尔表达式 THEN 处理语句[ELSEIF 布尔表达式 THEN 处理语句][ELSE 处理语句] END IF; 举个🌰： 12345678910111213141516171819202122232425262728mysql&gt; delimiter $mysql&gt; CREATE FUNCTION condition_demo(i INT) -&gt; RETURNS VARCHAR(10) -&gt; BEGIN -&gt; DECLARE result VARCHAR(10); -&gt; IF i = 1 THEN -&gt; SET result = '结果是1'; -&gt; ELSEIF i = 2 THEN -&gt; SET result = '结果是2'; -&gt; ELSEIF i = 3 THEN -&gt; SET result = '结果是3'; -&gt; ELSE -&gt; SET result = '非法参数'; -&gt; END IF; -&gt; RETURN result; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; delimiter;mysql&gt; SELECT condition_demo(2);+-------------------+| condition_demo(2) |+-------------------+| 结果是2 |+-------------------+1 row in set (0.00 sec) 循环语句while循环语法格式如下： 123WHILE 布尔表达式 DO 循环语句END WHILE; 举个🌰： 1234567891011121314151617181920212223mysql&gt; delimiter $mysql&gt; CREATE FUNCTION sum_all(n INT UNSIGNED) -&gt; RETURNS INT -&gt; BEGIN -&gt; DECLARE result INT DEFAULT 0; -&gt; DECLARE i INT DEFAULT 1; -&gt; WHILE i &lt;= n DO -&gt; SET result = result + i; -&gt; SET i = i + 1; -&gt; END WHILE; -&gt; RETURN result; -&gt; END $Query OK, 0 rows affected (0.00 sec)mysql&gt; delimiter;mysql&gt; select sum_all(10);+-------------+| sum_all(10) |+-------------+| 55 |+-------------+1 row in set (0.00 sec) REPEAT循环语法格式如下： 123REPEAT 循环语句UNTIL 布尔表达式 END REPEAT; 举个🌰： 123456789101112131415161718192021mysql&gt; CREATE FUNCTION sum_repeat(n INT UNSIGNED) -&gt; RETURNS INT -&gt; BEGIN -&gt; DECLARE result INT DEFAULT 0; -&gt; DECLARE i INT DEFAULT 1; -&gt; REPEAT -&gt; -- 循环开始 -&gt; SET result = result + i; -&gt; SET i = i + 1; -&gt; UNTIL i &gt; n END REPEAT; -&gt; RETURN result; -&gt; END $Query OK, 0 rows affected (0.02 sec)mysql&gt; select sum_repeat(5);+---------------+| sum_repeat(5) |+---------------+| 15 |+---------------+1 row in set (0.01 sec) LOOP循环语法格式如下： 1234循环标记:LOOP 循环语句 LEAVE 循环标记;END LOOP 循环标记; 举个🌰： 12345678910111213141516171819202122mysql&gt; CREATE FUNCTION sum_loop(n INT UNSIGNED) -&gt; RETURNS INT -&gt; BEGIN -&gt; DECLARE result INT DEFAULT 0; -&gt; DECLARE i INT DEFAULT 1; -&gt; LOOP_NAME:LOOP -- 循环开始 -&gt; IF i &gt; n THEN -&gt; LEAVE LOOP_NAME; -&gt; END IF; -&gt; SET result = result + i; -&gt; SET i = i + 1; -&gt; END LOOP LOOP_NAME; -&gt; RETURN result; -&gt; END $ mysql&gt; select sum_loop(10);+--------------+| sum_loop(10) |+--------------+| 55 |+--------------+1 row in set (0.00 sec) 存储过程存储函数侧重于执行语句并返回一个值，而存储过程更侧重于单纯的去执行语句。 创建存储过程1234CREATE PROCEDURE 存储过程名称([参数列表])BEGIN 需要执行的语句END 举个🌰： 1234567mysql&gt; CREATE PROCEDURE insert_first_table(c1 INT,c2 VARCHAR(100)) -&gt; BEGIN -&gt; SELECT * FROM first_table; -&gt; INSERT INTO first_table(first_column,second_column) VALUES(c1,c2); -&gt; SELECT * FROM first_table; -&gt; END $Query OK, 0 rows affected (0.02 sec) 存储过程的调用存储函数执行语句并返回一个值，所以常用在表达式中。存储过程偏向于调用那些语句，并不能用在表达式中。我们需要显式的使用CALL语句来调用一个存储过程： 1CALL 存储过程([参数列表]); 举个🌰： 123456789101112131415161718192021mysql&gt; CALL insert_first_table(4,'test'); +--------------+---------------+| first_column | second_column |+--------------+---------------+| 1 | aaa || 2 | NULL || NULL | ccc |+--------------+---------------+3 rows in set (0.00 sec)+--------------+---------------+| first_column | second_column |+--------------+---------------+| 1 | aaa || 2 | NULL || NULL | ccc || 4 | test |+--------------+---------------+4 rows in set (0.00 sec) 查看存储过程12345查看当前数据库中创建的存储过程都有哪些的语句：SHOW PROCEDURE STATUS [LIKE 需要匹配的函数名]查看某个存储过程定义的语句：SHOW CREATE PROCEDURE 存储过程名称 删除存储过程删除某个存储过程 1DROP PROCEDURE 存储过程名称 存储过程参数类型 参数类型 实际参数是否必须是变量 作用 IN 否 用于调用者向过程传递数据，如果该参数在过程中被修改，调用者不可见 OUT 是 用于把过程产生的结果放到此参数中，过程结束后调用者可以通过该参数来获取过程执行的结果 INOUT 是 综合IN和OUT特点，即可用于调用者向过程传递数据，也可用于存放过程中产生的结果 IN123456789101112131415161718192021mysql&gt; CREATE PROCEDURE test_in(IN num INT) -&gt; BEGIN -&gt; SELECT num; -&gt; SET num = 666; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; SET @a = 111;Query OK, 0 rows affected (0.01 sec)mysql&gt; CALL test_in(@a);+------+| num |+------+| 111 |+------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) IN参数类型的变量只能用于读取，对类型的变量赋值是不会被调用者看到的。如果我们不写明参数类型的话，该参数的类型默认是IN。 OUT12345678910111213141516171819202122232425mysql&gt; CREATE PROCEDURE test_out(OUT num INT) -&gt; BEGIN -&gt; SELECT num; -&gt; SET num = 666; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; CALL test_out(@a);+------+| num |+------+| NULL |+------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT @a;+------+| @a |+------+| 666 |+------+1 row in set (0.00 sec) OUT参数类型的变量只能用于赋值，对类型的变量赋值是会被调用者看到的,因此参数就不允许是常量。 存储过程中向调用者返回多个值，举个例子： 1234567891011121314151617mysql&gt; CREATE PROCEDURE data_out(OUT a INT,OUT b INT) -&gt; BEGIN -&gt; SET a = 100; -&gt; SET b = 200; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; CALL data_out(@a,@b);Query OK, 0 rows affected (0.01 sec)mysql&gt; select @a,@b;+------+------+| @a | @b |+------+------+| 100 | 200 |+------+------+1 row in set (0.00 sec) INOUT这种类型的参数既可以在存储过程中被读取，也可以被赋值后被调用者看到，因此参数就不允许是常量。 存储过程和函数的区别 存储函数在定义时需要显式用RETURNS语句标明返回的数据类型，而且在函数体中必须使用RETURN语句来显式指定返回的值，存储过程不需要。 存储函数的参数类型只能是IN，而存储过程支持IN、OUT、INOUT三种参数类型。 存储函数只能返回一个值，而存储过程可以通过设置多个OUT类型的参数来返回多个结果。 存储函数执行过程中产生的结果集并不会被显示到客户端，而存储过程执行过程中产生的结果集会被显示到客户端。 存储函数的调用直接使用在表达式中，而存储过程只能通过CALL语句来显式调用。 游标游标（Cursor）是处理数据的一种方法，为了查看或者处理结果集中的数据，游标提供了在结果集中一次一行或者多行前进或向后浏览数据的能力。初始状态下它标记查询结果集中的第一条记录,根据这个游标取出它对应记录的信息，随后再移动游标，让它指向别的记录。 创建游标1DECLARE 游标名称 CURSOR FOR 查询语句; 举个🌰： 12345mysql&gt; CREATE PROCEDURE cursor_demo() -&gt; BEGIN -&gt; DECLARE first_table_cursor CURSOR FOR select * from first_table; -&gt; END $Query OK, 0 rows affected (0.01 sec) 打开和关闭游标123OPEN 游标名称;CLOSE 游标名称; 打开游标意味着执行查询语句，让创建好的游标与该查询语句得到的结果集关联起来，关闭游标意味着会释放该游标占用的内存，所以一旦我们使用完了游标，就要把它关闭掉。 游标获取记录1FETCH 游标名 INTO 变量1, 变量2, ... 变量n 举个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354mysql&gt; CREATE PROCEDURE cursor_demo() -&gt; BEGIN -&gt; DECLARE c1 INT; -&gt; DECLARE c2 VARCHAR(100); -&gt; DECLARE record_count INT; -&gt; DECLARE i INT DEFAULT 0; -&gt; -- 声明游标 -&gt; DECLARE first_table_cursor CURSOR FOR select * from first_table; -&gt; -&gt; -- 统计表行数 -&gt; SELECT COUNT(*) FROM first_table INTO record_count; -&gt; -&gt; -- 使用游标遍历 -&gt; OPEN first_table_cursor; -&gt; -&gt; WHILE i &lt; record_count DO -&gt; FETCH first_table_cursor INTO c1 , c2; -&gt; SELECT c1,c2; -&gt; SET i = i + 1; -&gt; END WHILE; -&gt; CLOSE first_table_cursor; -&gt; END $Query OK, 0 rows affected (0.01 sec)mysql&gt; CALL cursor_demo();+------+------+| c1 | c2 |+------+------+| 1 | aaa |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 2 | NULL |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| NULL | ccc |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 4 | test |+------+------+1 row in set (0.01 sec)Query OK, 0 rows affected (0.01 sec) i表示当前游标对应的记录位置。每调用一次 FETCH 语句，游标就移动到下一条记录的位置。 遍历结束的执行策略其实在FETCH语句获取不到记录的时候会触发一个事件，从而我们可以得知所有的记录都被获取过了，然后我们就可以去主动的停止循环。MySQL中响应这个事件的语句如下： 1DECLARE CONTINUE HANDLER FOR NOT FOUND 语句; 举个🌰，再来改写一下cursor_demo存储过程： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263mysql&gt; delimiter $mysql&gt; CREATE PROCEDURE cursor_demo() -&gt; BEGIN -&gt; -- 声明变量 -&gt; DECLARE c1 INT; -&gt; DECLARE c2 VARCHAR(100); -&gt; DECLARE not_done INT DEFAULT 1; -&gt; -&gt; -- 声明游标 -&gt; DECLARE first_table_cursor CURSOR FOR select * from first_table; -&gt; -&gt; -- 在游标遍历完记录的时候将变量 not_done 的值设置为 0，并且继续执行后边的语句 -&gt; DECLARE CONTINUE HANDLER FOR NOT FOUND SET not_done = 0; -&gt; -&gt; -- 使用游标遍历 -&gt; OPEN first_table_cursor; -&gt; -&gt; WHILE not_done = 1 DO -&gt; -&gt; FETCH first_table_cursor INTO c1 , c2; -&gt; SELECT c1,c2; -&gt; END WHILE; -&gt; CLOSE first_table_cursor; -&gt; END $Query OK, 0 rows affected (0.00 sec)mysql&gt; CALL cursor_demo();+------+------+| c1 | c2 |+------+------+| 1 | aaa |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 2 | NULL |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| NULL | ccc |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 4 | test |+------+------+1 row in set (0.01 sec)+------+------+| c1 | c2 |+------+------+| 4 | test |+------+------+1 row in set (0.01 sec)Query OK, 0 rows affected (0.01 sec) 我们发现结果集中最后一条记录输出两遍怎么办呢，我们可以使用EXIT来替代上边的CONTINUE：CONTINUE表示在FETCH语句获取不到记录的时候仍然会执行之后存储过程的语句，也就是会将最后一次关联的记录中的值放入指定的变量EXIT表示在FETCH语句获取不到记录的时候仍然不会执行之后存储过程的语句 触发器存储函数与存储过程都是需要我们手动调用的，如果想在执行某条语句之前或者之后自动去调用另外一些语句，就需要用到触发器。 创建触发器触发器的定义： 12345678CREATE TRIGGER 触发器名&#123;BEFORE|AFTER&#125; &#123;INSERT|DELETE|UPDATE&#125;ON 表名FOR EACH ROW BEGIN 触发器内容END MySQL中目前只支持对INSERT、DELETE、UPDATE这三种类型的语句设置触发器。 因为触发器会对某个语句影响的所有记录依次调用我们自定义的触发器内容，所以我们需要一种访问该记录中的内容的方式，MySQL提供了NEW和OLD两个单词来分别代表新记录和旧记录，它们在不同操作中的含义不同： 对于INSERT语句设置的触发器来说，NEW代表准备插入的记录，不能使用OLD。 对于DELETE语句设置的触发器来说，OLD代表删除前的记录，不能使用NEW。 对于UPDATE语句设置的触发器来说，NEW代表修改后的记录，OLD代表修改前的记录。 举个🌰： 12345678910111213141516171819202122232425262728293031323334353637383940mysql&gt; CREATE TRIGGER test_trigger -&gt; BEFORE INSERT ON first_table -&gt; FOR EACH ROW -&gt; BEGIN -&gt; IF NEW.first_column &lt; 1 THEN -&gt; SET NEW.first_column = 1; -&gt; ELSEIF NEW.first_column &gt; 10 THEN -&gt; SET NEW.first_column = 10; -&gt; END IF; -&gt; END $Query OK, 0 rows affected (0.02 sec)mysql&gt; select * from first_table;+--------------+---------------+| first_column | second_column |+--------------+---------------+| 1 | aaa || 2 | NULL || NULL | ccc || 4 | test |+--------------+---------------+4 rows in set (0.00 sec)mysql&gt; INSERT INTO first_table(first_column,second_column) VALUES(5,'5'),(20,'20');Query OK, 2 rows affected (0.01 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from first_table;+--------------+---------------+| first_column | second_column |+--------------+---------------+| 1 | aaa || 2 | NULL || NULL | ccc || 4 | test || 5 | 5 || 10 | 20 |+--------------+---------------+6 rows in set (0.00 sec) (20,’20’)的插入结果变成了(10,’20’)说明触发器生效了。 查看触发器12345查看当前数据库中的所有触发器的语句：SHOW TRIGGERS;查看某个具体的触发器的定义：SHOW CREATE TRIGGER 触发器名; 删除触发器：1DROP TRIGGER 触发器名; 触发器使用注意事项 触发器内容中不能有输出结果集的语句。 一个表最多只能定义6个触发器分别是： BEFORE INSERT触发器 BEFORE DELETE触发器 BEFORE UPDATE触发器 AFTER INSERT触发器 AFTER DELETE触发器 AFTER UPDATE触发器 NEW中的值可以被更改，OLD中的值无法更改。 如果我们的BEFORE触发器内容执行过程中遇到了ERROR，那这个触发器对应的具体语句将无法执行；如果具体的操作语句执行过程中遇到了ERROR，那与它对应的AFTER触发器的内容将无法执行。 事件如果我们想指定某些语句在某个时间点或者每隔一个时间段执行一次的话,就需要创建一个事件。 创建事件1234567CREATE EVENT 事件名ON SCHEDULE&#123;AT 某个确定的时间点 | EVERY 期望的时间间隔 [STARTS datetime][END datetime]&#125;DOBEGIN 具体的语句END 事件支持两种类型的定时执行： 某个确定的时间点执行 1234567mysql&gt; CREATE EVENT insert_first_table -&gt; ON SCHEDULE -&gt; AT '2019-09-10 11:30:30' -&gt; DO -&gt; BEGIN -&gt; INSERT INTO first_table(first_column,second_column) VALUES(6,'6'); -&gt; END $ 除了直接填某个时间常量，也可以填写一些表达式： 1234567mysql&gt; CREATE EVENT insert_first_table -&gt; ON SCHEDULE -&gt; AT DATE_ADD(NOW(), INTERVAL 2 DAY) -&gt; DO -&gt; BEGIN -&gt; INSERT INTO first_table(first_column,second_column) VALUES(6,'6'); -&gt; END $ DATE_ADD(NOW(), INTERVAL 2 DAY)表示该事件将在当前时间的两天后执行。 每隔一段时间执行一次 1234567mysql&gt; CREATE EVENT insert_first_table -&gt; ON SCHEDULE -&gt; EVERY 1 HOUR -&gt; DO -&gt; BEGIN -&gt; INSERT INTO first_table(first_column,second_column) VALUES(6,'6'); -&gt; END $ 默认情况下，采用这种每隔一段时间执行一次的方式将从创建事件的事件开始，无限制的执行下去。我们也可以指定该事件开始执行时间和截止时间： 1234567mysql&gt; CREATE EVENT insert_first_table -&gt; ON SCHEDULE -&gt; EVERY 1 HOUR STARTS '2019-09-10 11:30:30' ENDS '2019-09-12 11:30:30' -&gt; DO -&gt; BEGIN -&gt; INSERT INTO first_table(first_column,second_column) VALUES(6,'6'); -&gt; END $ 在创建好事件之后我们就不用管了，到了指定时间，MySQL服务器会帮我们自动执行的。 查看事件12345查看当前数据库中的所有事件的语句：SHOW EVENTS;查看某个具体的事件的定义:SHOW CREATE EVENT 事件名; 删除事件1DROP EVENT 事件名; 事件使用注意事项默认情况下，MySQL服务器并不会帮助我们执行事件，除非我们在启动服务器的时候就指定了下边这个选项： 1event_scheduler = ON 如果在服务器已经启动的情况下，我们可以通过设置event_scheduler的系统变量来让MySQL服务器帮助我们执行事件，设置方式如下： 12mysql&gt; SET GLOBAL event_scheduler = ON;Query OK, 0 rows affected (0.00 sec) 错误解决在MySql中创建自定义函数报错信息如下： 1ERROR 1418 (HY000): This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you *might* want to use the less safe log_bin_trust_function_creators variable) 解决方法： 1mysql&gt; set global log_bin_trust_function_creators=1;]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-Mysql文件]]></title>
    <url>%2FMysql%2FMysql03%2F</url>
    <content type="text"><![CDATA[文件种类 参数文件：告诉MySQL实例启动时在哪里可以找到数据库文件，并且指定某些初始化参数，这些参数定义了某种内存结构的大小等设置，还会介绍各种参数的类型。 日志文件：用来记录MySQL实例对某种条件做出响应时写入的文件，如错误日志文件、二进制日志文件、慢查询日志文件、查询日志文件等。 socket文件：当用UNIX域套接字方式进行连接时需要的文件。 pid文件：MySQL实例的进程ID文件。 MySQL表结构文件：用来存放MySQL表结构定义文件。 存储引擎文件：因为MySQL表存储引擎的关系，每个存储引擎都会有自己的文件来保存各种数据。这些存储引擎真正存储了记录和索引等数据。本章主要介绍与InnoDB有关的存储引擎文件。” 参数文件参数分为两类： 动态参数：在 Mysql 实例运行中可以进行更改 静态参数：在整个实例生命周期内都不得更改 更改动态参数的语法如下： 123456789101112131415SET| [global | session] system_var_name=expr| [@@global. | @@session. | @@] system_var_name = expr# 改变当前会话，不会改变全局SET read_buffer_size = 524288# 改变全局会话参数，不会改变当前SET @@global.read_buffer_size = 1048576;# 查询当前会话参数SELECT @@session.read_buffer_size;# 查询全局会话参数SELECT @@global.read_buffer_size; ​global：全局的，session：当前会话。这种修改，并不最终修改配置文件my.cnf的参数值，所以重新启动后，参数还是按照配置文件中的加载。 日志文件MySQL中常见的日志文件有： 错误日志（error log）：对MySQL的启动、运行、关闭过程进行记录错误信息、警告信息。 慢查询日志（slow query log） 二进制日志（bin log） 查询日志（log） 慢查询日志在MySQL启动时设一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询日志文件中。 参数 作用 set global log_slow_queries = on; 开启慢查询命令，默认启动慢查询 set global long_query_time = 1; 设置慢查询时间超过1s即被认为慢查询，默认10s set global log_queries_not_using_indeces = on; 如果SQL语句没有使用索引，会记录到慢查询中 set global log_throttle_queries_not_using_indexs = on; 设置每分钟允许记录到slow log的且未使用索引的SQL语句次数，默认为0，表示没有限制。 套接字文件pid文件表结构定义文件innoDB存储引擎文件参考 MySQL技术内幕：InnoDB存储引擎(第2版) https://www.jianshu.com/p/c1ffd6956e6a https://www.cnblogs.com/BlueMountain-HaggenDazs/p/9297883.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-LSN]]></title>
    <url>%2FMysql%2FMysql-LSN%2F</url>
    <content type="text"><![CDATA[LSN(log sequence number)——日志序列号：递增产生，表示事务写入重做日志的字节总量，占用8个字节。 LSN存在什么地方？有什么含义？1234567---LOG---Log sequence number 15151135824 -- redo log buffer 的 lsn，存放在redo log buffer 中称： redo_mem_lsnLog flushed up to 15151135824 -- redo log file 的 lsn，存放在redo log 中称： redo_log_lsnPages flushed up to 15151135824 -- 最后一个刷到磁盘上的页的最新的 lsn Last checkpoint at 15151135815 -- 共享表空间上的日志记录点，最后一次检查点，及崩溃恢复时指定的起点 , checkpoint 所在的 lsn, 存放在redo log第一个文件的头部，称： cp_lsn LSN 有什么用？主要用于MySQL重启恢复 恢复的算法假设： redo_log_lsn = 15000 , cp_lsn=10000 , 这时候MySQL crash了，重启后的恢复流程如下： cp_lsn = 10000 之前的redo 日志，不需要恢复： 因为checkpoint之前的日志已经可以确保刷新完毕 那么 10000 &lt;= redo_log_LSN &lt;= 15000 的日志需要结合page_lsn判断，哪些需要重做，哪些不需要重做。 redo_log_LSN 日志里面记录的page 操作，如果redo_log_LSN &lt;= page_lsn , 这些日志不需要重做，因为page已经是最新的 redo_log_LSN 日志里面记录的page 操作, 如果redo_log_LSN &gt;= page_lsn , 这些日志是需要应用到page 里面去的，这一系列操作我们称为恢复.]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-Redo与Undo]]></title>
    <url>%2FMysql%2FMysql-RedoAndUndo%2F</url>
    <content type="text"><![CDATA[redo logredo 概念重做日志(redo log)：在InnoDB存储引擎中，大部分情况下 Redo 是物理日志，记录的是数据页的物理变化。 redo 结构Redo log可以简单分为以下两个部分： 重做日志缓冲 (redo log buffer),是易失的，在内存中 日志会先写到redo log buffer ，根据制定条件刷新到redo log file 由log block组成 每个log block 512字节，所以不需要 double write，因为每次刷新都是原子的 重做日志文件 (redo log file)，是持久的，保存在磁盘中 redo log的物理文件，一般有2个,大小可配置 redo 写入时机 在数据页修改完成之后，在脏页刷出磁盘之前，写入redo日志。注意的是先修改数据，后写日志 redo日志比数据页先写回磁盘 聚集索引、非聚集索引、undo页面的修改，均需要记录Redo日志。 redo 的整体流程 redo如何保证事务的持久性？InnoDB 通过 Force Log at Commit 机制实现事务的持久性，即当事务提交时，先将 redo log buffer 写入到 redo log file 进行持久化，待事务的commit操作完成时才算完成。这种做法也被称为 Write-Ahead Log(预先日志持久化)，在持久化一个数据页之前，先将内存中相应的日志页持久化。 为了保证每次日志都写入redo log file，在每次将redo buffer写入redo log file之后，默认情况下，InnoDB存储引擎都需要调用一次 fsync操作,因为重做日志打开并没有 O_DIRECT选项，所以重做日志先写入到文件系统缓存。为了确保重做日志写入到磁盘，必须进行一次 fsync操作。fsync操作 将数据提交到硬盘中，强制硬盘同步，将一直阻塞到写入硬盘完成后返回，大量进行fsync操作就有性能瓶颈，因此磁盘的性能也影响了事务提交的性能，也就是数据库的性能。(O_DIRECT选项是在Linux系统中的选项，使用该选项后，对文件进行直接IO操作，不经过文件系统缓存，直接写入磁盘) undo logundo 概念undo log主要记录的是数据的逻辑变化，为了在发生错误时回滚之前的操作，需要将之前的操作都记录下来，然后在发生错误时才可以回滚。 undo 结构在InnoDB存储引擎中，undo存储在回滚段(Rollback Segment)中,每个回滚段记录了1024个undo log segment，而在每个undo log segment段中进行undo 页的申请，在5.6以前，Rollback Segment是在共享表空间里的，5.6.3之后，可通过 innodb_undo_tablespace设置undo存储的位置。 undo 写入时机 DML操作修改聚集索引前，记录undo日志 非聚集索引记录的修改，不记录undo日志 undo 的整体流程undo log 采用顺序IO写入磁盘共享表空间。 undo 类型 insert undo log：在insert 操作中产生的undo log，因为insert操作的记录，只对事务本身可见，对其他事务不可见。故该undo log可以在事务提交后直接删除，不需要进行purge操作。 update undo log：在delete 和update操作产生的undo log，该undo log可能需要提供MVCC机制，因此不能再事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除。 ## DML的相关物理实现算法 * 主键索引 12341. 对于delete --需要undo绑定该记录才能进行回滚，所以只能打上标记，delete mark 2. 对于update --原记录可以物理删除，因为可以在新插入进来的地方进行undo绑定 * 如果不能原地更新： delete(注意：这里是直接delete,而不是delete mark) + insert * 如果可以原地更新，那么直接update就好 非聚集索12341. 对于delete --不能直接被物理删除，因为二级索引没有undo，只能通过打标记，然后回滚。否则如果被物理删除，则无法回滚 delete mark 2. 对于update --不能直接被物理删除，因为二级索引没有undo，只能通过打标记，然后回滚。否则如果被物理删除，则无法回滚 delete mark + insert redo &amp; undoundo log 是否是 redo log 的逆过程？undo log是逻辑日志，对事务回滚时，只是将数据库逻辑地恢复到原来的样子。redo log是物理日志，记录的是数据页的物理变化，显然undo log不是redo log的逆过程。 事务实现过程事务B要将字段A的值由原来的1修改为3，要将B的值由原来的2修改为4，redo日志记录的是： 1234567891011假设有A、B两个数据，值分别为1,2.1. 事务B开始2. 记录A=1到undo log3. 修改A=34. 记录A=3到 redo log5. 记录B=2到 undo log6. 修改B=47. 记录B=4到redo log8. 将redo log写入磁盘9. 事务提交，将数据写入磁盘10.事物B结束 在insert/update/delete操作中，redo和undo分别记录的内容都不一样，量也不一样。在InnoDB内存中，一般的顺序如下： 写undo的redo 写undo 修改数据页 写Redo 如果上面事务B回滚（当做新的事务C），则redo记录的是： 1234567891011121314151. 事务C开始2. 记录A=1到undo log3. 修改A=34. 记录A=3到 redo log5. 记录B=2到 undo log6. 修改B=47. 记录B=4到redo log &lt;!--回滚--&gt;8. 修改B=29. 记录B=2到redo log10.修改A=111.记录A=1到redo log12.将redo log写入磁盘13.事务提交，将数据写入磁盘14.事物C结束 恢复策略：恢复时，先根据redo重做所有事务（包括未提交和回滚了的），再根据undo回滚未提交的事务。当系统发生宕机时，如果一个事务的 redo log 已经全部刷入磁盘，那么该事务一定可以恢复；如果一个事务的 redo log 没有全部刷入磁盘，那么就通过 undo log 将这个事务恢复到执行之前。 如上，如果事务B异常未提交事务就宕机，恢复时，先根据redo日志将数据恢复为A=3&amp;B=4，然后根据undo记录的A=1&amp;B=2将数据恢复如初。 # 参考： * https://keithlan.github.io/2017/06/12/innodb_locks_redo/ * https://juejin.im/post/5c3c5c0451882525487c498d * https://t.hao0.me/mysql/2016/11/05/mysql-innodb-05-tablespaces.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-InnoDB存储引擎]]></title>
    <url>%2FMysql%2FMysql02%2F</url>
    <content type="text"><![CDATA[概述 InnoDB存储引擎最早由Innobase Oy公司开发，被包括在MySQL数据库所有的二进制发行版本中， 从MySQL 5.5版本开始是默认的表存储引擎（之前的版本InnoDB存储引擎仅在Windows下为默认的存储引擎） 第一个完整支持ACID事务的MySQL存储引擎（BDB是第一个支持事务的MySQL存储引擎，现在已经停止开发） 特点：行锁设计、支持 MVCC、支持外键、提供一致性非锁定读、有效利用内存和 CPU 体系架构InnoDB存储引擎是由内存池、后台线程、磁盘存储三大部分组成。 线程InnoDB 使用的是多线程模型, 其后台有多个不同的线程负责处理不同的任务 Master ThreadMaster Thread是最核心的一个后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性。包括脏页刷新、合并插入缓冲、UNDO页的回收等。 IO Thread在 InnoDB 存储引擎中大量使用了异步IO(Async IO)来处理写IO请求, IO Thread的工作主要是负责这些 IO 请求的回调。 InnoDB 版本 线程 1.0之前 4 个 io thread：write，read，insert buffer，log IO Thread. 在Linux下，IO Thread的数量不能进行调整 在Windows下可以通过参数 innodb_file_io_threads 来增大IO Thread 1.0之后 read 和 write IO thread 分别增大到了 4 个 分别使用 innodb_read_io_threads 和 innodb_write_io_threads 设置线程数 Purge Thread事务提交后，其所使用的undo log可能不再需要，因此需要Purge Thread来回收已经分配并使用的UNDO页。 InnoDB 版本 作用 1.1之前 purge 操作在 master thread 内完成 1.1之后 purge 可以独立到单独的线程,减轻 master thread 工作,提高 cpu 利用率和提高性能 MySQL数据库的配置文件[mysqld]中添加如下命令来启用独立的Purge Thread： innodb_purge_threads=1 1.1版本中，即使将 innodb_purge_threads 设为大于1，InnoDB存储引擎启动时也会将其设为1 1.2之后 支持多个Purge Thread, 这样做可以加快UNDO页的回收，也能更进一步利用磁盘的随机读取性能 Page Cleaner ThreadPage Cleaner Thread的作用是取代Master Thread中脏页刷新的操作，减轻原Master Thread的工作及对于用户查询线程的阻塞，进一步提高性能。 内存innoDB内存主要由缓冲池(innodb buffer pool)、重做日志缓冲(redo log buffer)、额外内存池组成(innodb additional men pool size)组成 缓冲池缓冲池是主存储器中的一个区域，用于在访问时缓存表和索引数据。缓冲池允许直接从内存处理常用数据，从而加快处理速度。在专用服务器上，通常会将最多80％的物理内存分配给缓冲池。读取流程： 更新流程： 因此缓冲池的大小影响数据库的整体性能。 由于32位操作系统的限制，在该系统下最多将该值设置为3G。用户可以打开操作系统的PAE选项来获得32位操作系统下最大64GB内存的支持。为了让数据库使用更多的内存,建议数据库系统都采用 64 位操作系统。 参数 版本 作用 innodb_buffer_pool_instances 从InnoDB 1.0.x开始 配置多个缓冲池实例，默认为1 缓冲池中缓存的数据页类型 索引页(index page)：缓存数据表索引 数据页(data page)：缓存数据页，占缓冲池的绝大部分 undo页(undo Log Page)：undo页是保存事务，为回滚做准备的。 插入缓冲（insert buffer）：插入数据时要先插入到缓存池中。 自适应哈希索引（adaptive hash index）： 除了B+ Tree索引外，在缓冲池还会维护一个哈希索引，以便在缓冲池中快速找到数据页。 InnoDB存储的锁信息（lock info） 数据字典信息（data dictionary） 在MySQL中，数据字典信息内容就包括表结构、数据库名或表名、字段的数据类型、视图、索引、表字段信息、存储过程、触发器等内容。 InnoDB有自己的表缓存，可以称为表定义缓存或者数据字典。当InnoDB打开一张表，就增加一个对应的对象到数据字典。 缓冲池管理方式 Free list当数据库刚启动时，LRU列表是空的，这时页都存放在Free list中。当需要从缓冲池中分页时，从Free list中查找是否有可用的空闲页，若有则将该页从Free列表中删除，放入到LRU列表中,维持页数守恒。 LRU list LRU算法：最频繁使用页在LRU列表的前端，最少使用的页在尾端。首先释放LRU列表中的尾端的页。缓冲池中页的大小默认为16KB。 InnoDB优化的LRU算法(midpoint insertion strategy)：将新读取到的页不放在首部，而是中间部位 midpoint 位置。目标是确保频繁访问”热”页面保留在缓冲池中。 参数 作用 innodb_old_blocks_pct 控制LRU列表中 old list 的百分比。 默认值为 37，对应于原始固定比率3/8。 值范围是 5（缓冲池中的新页面很快就会老化）到 95。 innodb_old_blocks_time 指定第一次访问页面之后的时间窗口（ms） 在此期间可以访问该页面而不移动到LRU列表的前端 默认值为 1000 ms 默认情况下，算法操作如下： 在默认配置下， midpoint位置在LRU list 的5/8处。 midpoint是new sublist的尾部与old sublist的头部相交的边界。 当 InnoDB 将页面读入缓冲池时，将页插入midpoint位置(old sublist的头部)。 访问old sublist中的页 &amp;&amp; 该页在old sublist中的停留时间超过innodb_old_blocks_time设置的时间，使其变young,将其移动到缓冲池的头部(new sublist的头部)。 当页从LRU列表的old部分加入到new部分时，称此时发生的操作为page made young，而因为innodb_old_blocks_time的设置而导致页没有从old部分移动到new部分的操作称为page not made young 在数据库操作中，被访问的页将移到new sublist的表头，这样一来，在new sublist中的未被访问的节点将逐渐往表尾移动，当移动过中点，将变为old list的节点。当表满时，old list末尾的页将会被移除。 为什么不采用朴素的LRU？因为某些SQL操作会访问很多页，甚至全部页，但仅仅在该次查询操作，并不是活跃的热点数据。可能会使缓冲池中的页被刷新出，从而影响缓冲池的效率。 Flush list在LRU类表的页被修改后，称为脏页（Dirty Page），即缓存和硬盘的页数据不一致。数据库会通过CHECKPOINT机制将脏页刷新回磁盘，Flush list中的页即为脏页列表。 重做日志缓冲 什么是redo log？当数据库对数据做修改的时候，需要把数据页从磁盘读到buffer pool中，然后在buffer pool中进行修改，那么这个时候buffer pool中的数据页就与磁盘上的数据页内容不一致，称buffer pool的数据页为dirty page 脏数据。如果发生非正常的DB服务重启，那么这些数据并没有同步到磁盘文件中（注意，同步到磁盘文件是个随机IO），会发生数据丢失。如果这个时候，能够有一个文件，当缓冲池中的data page变更结束后，把相应修改记录记录到这个文件（注意，记录日志是顺序IO），那么当DB服务发生crash的情况，恢复DB的时候，也可以根据这个文件的记录内容，重新应用到磁盘文件，数据保持一致。这个文件就是redo log ，用于记录 数据修改后的记录，顺序记录。什么是undo log？undo日志用于存放数据修改被修改前的值。假设修改表中 id=1 的行数据，把Name=’B’ 修改为Name = ‘B2’ ，那么undo日志就会用来存放Name=’B’的记录，如果这个修改出现异常，可以使用undo日志来实现回滚操作，保证事务的一致性。 重做日志缓冲不需要设置很大，通常情况下8M能满足大部分的应用场景。重做日志支持以下三种情况触发刷新： Master Thread每一秒将重做日志缓冲刷新到重做日志文件 每次事务提交时将重做日志缓冲刷新到重做日志文件 当重做日志缓冲池剩余空间小于1/2时，重做日志缓冲刷新到重做日志文件 额外的内存池 在InnoDB存储引擎中，对内存的管理是通过一种称为内存堆的方式进行的。在对一些数据结构本身的内存进行分配时，需要从额外的内存池中进行申请，当该区域的内存不够时，会从缓冲池中进行申请。 Checkpoint技术什么是Checkpoint？是一个数据库事件(event)，这个事件激活以后会触发数据库写进程(DBWR)将脏数据块写到磁盘中。 为什么需要Checkpoint技术？innoDB在事务提交时，先写重做日志，再修改内存数据这样，就产生了脏页。既然有重做日志保证数据持久性，查询时也可以从缓冲池页中取数据，那为什么还要刷新脏页到磁盘呢？如果重做日志可以无限增大，同时缓冲池足够大，能够缓存所有数据，那么是不需要将缓冲池中的脏页刷新到磁盘。但是，会有以下几个问题：1) 服务器内存有限，缓冲池不够用，无法缓存全部数据2) 重做日志无限增大成本要求太高3) 宕机时如果重做全部日志恢复时间过长 Checkpoint 解决了什么问题？1) 缩短短数据库的恢复时间2) 缓冲池不够时，将脏页刷新到磁盘3) 重做日志不可用时，刷新脏页 对于InnoDB存储引擎而言，其是通过LSN（Log Sequence Number）来标记版本的。每个页有LSN，重做日志中也有LSN，Checkpoint也有LSN。 innodb 内部有两种 checkpoint： sharp checkpoint：数据库关闭的时候将所有的脏页刷回到磁盘，默认方式，参数 innodb_fast_shudown=1 fuzzy checkpoint：只刷新部分脏页 master thread checkpoint：master thread 异步的以每秒或者每 10 秒的速度从缓冲池的脏页列表中刷新一定比列的也回磁盘 周期性，异步，读取flush list，找到脏页，写入磁盘 flush_lru_list checkpoint：InnoDB要保证LRU列表中需要有差不多100个空闲页可供使用。如果没有这么多，就会将 lru list 尾部的页移除。如果这些页有脏页，就需要进行 checkpoint。 innodb 1.1.x版本之前，检查在用户查询线程中,会阻塞用户查询操作。 innodb 1.2.x版本之后，检查放到了单独的 page cleaner 线程中,可通过 innodb_lru_scan_depth 控制lru列表中可用页的数量，默认是1024。 async/sync flush checkpoint：重做日志文件不可用时，强制将一些页刷新到磁盘。达到重做日志文件的大小阈值。 checkpoint age = redo_log_lsn - cp_lsn 低水位=75% * total_redo_log_file_size 高水位=90% * total_redo_log_file_size checkpoint age &lt; 低水位 不需要刷新 低水位 &lt;= checkpoint age &lt;= 高水位 会强制进行 checkpoint ，触发async flush， 根据flush_list的顺序，刷新足够多的脏页，直到 checkpoint age &lt; 低水位 checkpoint age &gt; 高水位 会强制进行 checkpoint ，触发sync flush 根据flush_list的顺序，刷新脏页, 直到 checkpoint age &lt; 低水位 dirty page too much checkpoint：当缓冲池中脏页的数量占据一定百分比时，强制进行Checkpoint，用来保证缓冲池中有足够的页，通过 innodb_max_dirty_pages_pct 参数控制。 -------- 第二部分 -------- Master thread 工作方式InnoDB 1.0.x 版本之前的 Master threadMaster thread 内部有多个循环 loop 组成： 主循环 loop 后台循环 backgroup loop 刷新循环 flush loop 暂停循环 suspend loop 伪代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364void master_thread()&#123; goto loop; //主循环 loop ： for(int i = 0; i &lt; 10; ++i)&#123; thread_sleep(1); //1. 日志缓冲刷新到磁盘，即使事务还没有提交 do log buffer flush to disk; //2. 根据前一秒IO操作小于5，合并插入缓冲 if(last_one_second_ios &lt; 5) do merge at most 5 insert buffer; //3. 脏页的比例超过了阈值，刷新 100 个脏页到磁盘 if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) do buffer pool flush 100 dirty page; //4. 没有用户活动（数据库空闲时）或者数据库关闭（shutdown），切换到 backgroup loop if(no user activity) goto backgroud loop; &#125; //1. 前10秒IO操作小于200，刷新 100 个脏页到磁盘 if(last_ten_second_ios &lt; 200) do buffer pool flush 100 dirty page; //2. 合并至多 5 个插入缓冲 do merge at most 5 insert buffer; //3. 将重做日志刷新到磁盘 do log buffer flush to disk; //4. 删除无用的 undo 页（每次最多尝试回收 20 个 undo 页） do full purge; //5. 脏页比例超过 70% 刷新100 个脏页到磁盘，否则刷新 10 个脏页 if ( buf_get_modified_ratio_pct ＞ 70 % ) do buffer pool flush 100 dirty page else buffer pool flush 10 dirty page goto loop //后台循环 background loop : //1. 删除无用的 undo 页 do full purge //2. 合并 20 个插入缓冲 do merge 20 insert buffer //3.如果有任务，跳转到主循环，否则跳转到刷新循环 if not idle goto loop else goto flush loop //刷新循环 flush loop : //不断刷新100个脏页，直到脏页比例没有超过阈值 do buffer pool flush 100 dirty page if ( buf_get_modified_ratio_pct ＞ innodb_max_dirty_pages_pct ) goto flush loop //没有任务，跳转到暂停循环 goto suspend loop //暂停循环 suspend loop : //将主线程挂起，等待事件发生 suspend_thread() waiting event goto loop;&#125; InnoDB 1.2.x 版本之前的 Master thread 提高刷新脏页数量和合并插入数量，改善磁盘 IO 处理能力,刷新数量不再硬编码，而是使用百分比控制。 在合并插入缓冲的时候，合并插入缓冲的数量为 innodb_io_capacity 的 5% 在从缓冲区刷新脏页的时候，刷新脏页的数量为 innodb_io_capacity 增加了自适应刷新脏页功能。 1.0.x之前版本：脏页在缓冲池占比小于innodb_max_dirty_pages_pct，不刷新脏页，大于则刷新100个脏页 1.0.x版本开始：引入innodb_adaptive_flushing参数，通过函数buf_flush_get_desired_flush_rate判断产生重做日志的速度来决定最适合的刷新脏页数量。 full purge回收的Undo页的数量也不再硬编码，使用参数innodb_purge_batch_size控制。 参数 InnoDB 版本 作用 innodb_io_capacity 1.0.x开始 表示磁盘IO的吞吐量,默认值是200 innodb_max_dirty_pages_pct 1.0.x之前 脏页在缓冲池中所占比率，默认值是90 1.0.x开始 默认值是75加快刷新脏页的频率，保证了磁盘IO的负载。 innodb_adaptive_flushing 1.0.x开始 是否自适应刷新脏页，默认为 ON innodb_purge_batch_size 1.0.x开始 清除 undo 页时,表示一次删除多少页,默认是 20 Master Thread的伪代码变为了下面的形式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465void master_thread()&#123; goto loop; //主循环 loop ： for(int i = 0; i &lt; 10; ++i)&#123; thread_sleep(1); //1. 日志缓冲刷新到磁盘，即使事务还没有提交 do log buffer flush to disk; //2. 根据前一秒IO操作小于5%innodb_io_capacity，合并插入缓冲 if(last_one_second_ios &lt; 5%innodb_io_capacity) do merge 5%innodb_io_capacity insert buffer; //3. 脏页的比例超过了阈值，刷新 100%innodb_io_capacity 个脏页到磁盘 if(buf_get_modified_ratio_pct &gt; innodb_max_dirty_pages_pct) do buffer pool flush 100%innodb_io_capacity dirty page; //4. 没有用户活动（数据库空闲时）或者数据库关闭（shutdown），切换到 backgroup loop if(no user activity) goto backgroud loop; &#125; //1. 前10秒IO操作小于innodb_io_capacity，刷新 innodb_io_capacity 个脏页到磁盘 if(last_ten_second_ios &lt; innodb_io_capacity) do buffer pool flush 100%innodb_io_capacity dirty page; //2. 合并至多 5%innodb_io_capacity 个插入缓冲 do merge at most 5%innodb_io_capacity insert buffer; //3. 将重做日志刷新到磁盘 do log buffer flush to disk; //4. 删除无用的 undo 页（每次最多尝试回收 5%innodb_io_capacity 个 undo 页） do full purge; //5. 脏页比例超过 70% 刷新 100%innodb_io_capacity 个脏页到磁盘， // 否则刷新 10%innodb_io_capacity 个脏页 if ( buf_get_modified_ratio_pct ＞ 70 % ) do buffer pool flush 100%innodb_io_capacity dirty page else buffer pool flush 10%innodb_io_capacity dirty page goto loop //后台循环 background loop : //1. 删除无用的 undo 页 do full purge //2. 合并 100%innodb_io_capacity 个插入缓冲 do merge 100%innodb_io_capacity insert buffer //3.如果有任务，跳转到主循环，否则跳转到刷新循环 if not idle goto loop else goto flush loop //刷新循环 flush loop : //不断刷新 100%innodb_io_capacity 个脏页，直到脏页比例没有超过阈值 do buffer pool flush 100%innodb_io_capacity dirty page if ( buf_get_modified_ratio_pct ＞ innodb_max_dirty_pages_pct ) goto flush loop //没有任务，跳转到暂停循环 goto suspend loop //暂停循环 suspend loop : //将主线程挂起，等待事件发生 suspend_thread() waiting event goto loop;&#125; InnoDB 1.2.x 版本的 Master threadInnoDB 1.2.x 版本中再次对 Master Thread 进行了优化，伪代码如下： 123456if InnoDB is idle//之前版本中每10秒的操作srv_master_do_idle_tasks();else//之前版本中每秒的操作srv_master_do_active_tasks(); 对于刷新脏页的操作，从Master Thread线程分离到一个单独的Page Cleaner Thread，从而减轻了Master Thread的工作，同时进一步提高了系统的并发性。 InnoDB 关键特性关键特性包括： 插入缓冲 insert buffer 两次写 double write 自适应哈希索引 adaptive hash index 异步 io async io 刷新邻接页 flush neighbor page 插入缓冲聚集索引（一级索引）表在存储的时候按照主键排序进行存储，不需要磁盘的随机读取，插入效率高。非叶子节点存放的是键值，叶子节点存放的是行数据，称之为数据页。 辅助索引（二级索引）除了聚集索引之外的索引都可以称之为辅助索引，叶子节点中存放的是主键的键值。叶子节点的插入不再有序，这时就需要离散访问非聚集索引页，插入性能变低。一张表可以存在多个辅助索引，但是只能有一个聚集索引，通过辅助索引来查找对应的航记录的话，需要进行两步，第一步通过辅助索引来确定对应的主键，第二步通过相应的主键值在聚集索引中查询到对应的行记录，也就是进行两次B+树搜索。 索引数据页的更新（针对二级索引）表数据更新的同时也会更新对应的表的索引数据，所以：对表进行insert delete update时，很可能会产生大量的物理读(物理读索引数据页) 1. insert bufferInsert Buffer的使用流程： 插入缓冲的启用需要满足以下两个条件：1）索引是辅助索引（secondary index）2）索引不是唯一的：整个索引数据被切分为2部分，无法保证唯一性。 insert buffer结构insert buffer的数据结构是B+树，全局只有一颗B+树。B+树的非叶子节点是Search key，构造结构为(space,marker,offset)。 space：待插入记录所在表的表空间id。每个表都有唯一的表空间id，通过表空间id可以查出是哪张表。 marker：兼容之前的版本。 offset：在表空间中页的偏移量。 当一个辅助索引要插入到(space,offset)中时，如果该页不在缓冲池中，则按上述规则构造一个search key，将该记录插入到insert buffer中。但是如果该页一直在insert buffer中，不断有记录插入到同一个索引页中，那么该索引页的空间就会逐渐缩小，要出现B+树节点的分裂情况，这时就不能进行insert buffer了。所以，我们需要一个机制来管理每个页面的剩余空闲空间，这就是Insert buffer bitmap。每隔page_size个页面，就是一个Insert buffer bitmap page。例如：若page_size = 16384(16k)，那么page_no为0，16384，32768，…的page，就是Insert buffer bitmap page，Bitmap page的功能，就是管理其后连续的page_size – 1个page的空间使用率。每个辅助索引页在Insert buffer bitmap中占用4bit。 merge insert buffer 发生条件 辅助索引页被读取到buffer pool中：正常的select查询操作，索引页被调入内存，该索引页对应在insert buffer中的索引更改记录就会发生merge操作。 Insert buffer bitmap page追踪到该索引页无可用空间时。 Master Thread。 insert buffer 刷新到磁盘条件 有一个后台线程，会认为数据库空闲时； 数据库缓冲池不够用时； 数据库正常关闭时； redo log写满时：几乎不会出现redo log写满，此时整个数据库处于无法写入的不可用状态 插入缓冲主要带来如下两个坏处1）可能导致数据库宕机后实例恢复时间变长。如果应用程序执行大量的插入和更新操作，且涉及非唯一的聚集索引，一旦出现宕机，这时就有大量内存中的插入缓冲区数据没有合并至索引页中，导致实例恢复时间会很长。2）在写密集的情况下，插入缓冲会占用过多的缓冲池内存，默认情况下最大可以占用1/2，这在实际应用中会带来一定的问题。 2. change bufferInnoDB从1.0.x版本开始引入了Change Buffer，可以将其视为Insert Buffer的升级。从这个版本开始，InnoDB可以对DML操作——Insert、Delete、Update(delete+insert)都进行缓冲，它们分别是：Insert Buffer, Delete Buffer,Purge Buffer。对一个记录进行 update 操作有两个过程 将记录标记为删除：delete buffer 将记录真正删除：pruge buffer 参数 InnoDB 版本 作用 innodb_change_buffering 1.0.x开始 用来开启各种Buffer选项，默认值是all inserts deletes purges changes：开启 inserts 和 deletes all：都开启 none：都不开启 innodb_change_buffer_max_size 1.2.x开始 用来控制change buffer最大使用内存数量默认值为25,表示最多使用1/4的缓存池空间该参数最大有效值是50 两次写提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。 脏页刷新到磁盘风险IO的最小单位： 数据库IO的最小单位是16K（MySQL默认，oracle是8K） 文件系统IO的最小单位是4K（也有1K的） 磁盘IO的最小单位是512字节 因此，存在IO写入导致page损坏的风险： 提高innodb的可靠性，用来解决部分写失败(partial page write页断裂)。 Double write解决了什么问题一个数据页的大小是16K，假设在把内存中的脏页写到数据库的时候，写了8K突然宕机了，也就是说前8K数据是新的，后8K是旧的，那么磁盘数据库这个数据页就是不完整的，是一个坏掉的数据页，这种情况被称为部分写失效 那么可不可以通过 redo log 来进行恢复呢？redo记录的是对页的修改，只能恢复校验完整（还没写）的页，不能修复坏掉的数据页，所以这个数据就丢失了，可能会造成数据不一致，所以需要double write。 为什么 redo log 不需要 doublewrite 的支持？因为 redo log 写入的单位就是 512 字节，也就是磁盘 IO 的最小单位，所以无所谓数据损坏。 两次写工作流程doublewrite由两部分组成，一部分为内存中的doublewrite buffer，其大小为2MB，另一部分是磁盘上共享表空间(ibdata x)中连续的128个页，即2个区(extent)，大小也是2M。 当一系列机制触发数据缓冲池中的脏页刷新时，并不直接写入磁盘数据文件中，而是先拷贝至内存中的doublewrite buffer中； 接着从两次写缓冲区分两次写入磁盘共享表空间中(连续存储，顺序写，性能很高)，每次写1MB； 待第二步完成后，再将doublewrite buffer中的脏页数据写入实际的各个表空间文件(离散写)；(脏页数据固化后，即进行标记对应doublewrite数据可覆盖) 现在我们来分析一下为什么 double write 可以生效。当宕机发生时，有那么几种情况： 磁盘还未写，此时可以通过 redo log 恢复； 磁盘正在进行从内存到共享表空间的写，此时数据文件中的页还没开始被写入，因此也同样可以通过 redo log 恢复； 磁盘正在写数据文件，此时共享表空间已经写完，可以从共享表空间拷贝页的副本到数据文件实现恢复。 自适应哈希索引哈希：一次就可以定位数据 B+树：取决于树的高度，生产环境一般是 3-4 层，所以需要查询 3-4 次 自适应哈希索引 AHI（adaptive hash index）建立条件：观察到一个访问模式访问频繁，就会建立哈希索引 通过该模式访问了 100 次（模式：where x = ?） 页通过该模式访问了 N 次，其中 N = 页的记录总数⁄16 InnoDB 存储引擎官方文档显示，启用 AHI 后,读取和写入速度可以提高 2 倍，辅助索引的连接操作性能可以提高 5 倍。 异步IO为了提高磁盘的操作性能，当前的数据库系统都采用异步IO的方式处理磁盘操作。用户可以在发出一个IO请求胡立即再发出另一个IO请求，当全部IO请求发送完毕后，等待所有IO操作完成，这就是AIO。AIO的另一个优势是可以进行IO Merge操作，也就是将多个IO合并为1个IO, 这样可以提高IOPS(Input/Output Per Second)的性能。 例如：用户访问页的（space, page_no)为(8,6) (8,7) (8,8)，每个页的大小为16KB，同步IO需要3次IO操作。可以优化为从(8,6)开始读取48KB。 刷新临接页当刷新一个脏页时，InnoDB会检查该页所在extent的所有页，如果是脏页，一起刷新。 参数 版本 作用 innodb_flush_neighbors 1.2.x开始 控制是否启用该特性 参考： http://oohcode.com/2015/10/14/InnoDB-Key-Features/ https://chyroc.cn/posts/innodb-storage-engine-reading-1/ https://www.cnblogs.com/zhoujinyi/archive/2013/04/11/2988923.html http://huzb.me/2019/01/14/%E6%8F%92%E5%85%A5%E7%BC%93%E5%86%B2%E3%80%81%E4%B8%A4%E6%AC%A1%E5%86%99%E5%92%8C%E8%87%AA%E9%80%82%E5%BA%94%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/ https://blog.csdn.net/tanliqing2010/article/details/81509539 https://www.cnblogs.com/geaozhang/p/7341333.html https://draveness.me/mysql-innodb http://richfisher.me/blog/2017/12/18/innodb-notes/ https://www.docs4dev.com/docs/zh/mysql/5.7/reference/innodb-architecture.html#innodb%E6%9E%B6%E6%9E%84 MySQL技术内幕：InnoDB存储引擎(第2版)]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Mysql技术内幕》学习笔记-MySql体系结构和存储引擎]]></title>
    <url>%2FMysql%2FMysql01%2F</url>
    <content type="text"><![CDATA[数据库 数据库是文件的集合，是依照某种数据模型组织起来并存放于二级存储器中的数据集合。在MySQL数据库中，数据库文件可以是frm、MYD、MYI、ibd结尾的文件。 数据库实例 数据库实例是程序，是位于用户与操作系统之间的一层数据管理软件，用户对数据库数据的任何操作，包括数据库定义、数据查询、数据维护、数据库运行控制等都是在数据库实例下进行的，应用程序只有通过数据库实例才能和数据库打交道。 MySql体系结构 从图中可以发现，MySQL由：连接池组件、管理服务和工具组件、SQL接口组件、查询分析器组件、优化器组件、缓冲（Cache）组件、插件式存储引擎和物理文件组成。MySQL数据库区别于其他数据库的最重要的一个特点就是其插件式的表存储引擎。 MySql存储引擎MySql数据库常用存储引擎：InnoDB、MyISAM、NDB、Memory(HEAP)、Archive、BDB(BerkeleyDB)、Federated、Maria等。 特性 InnoDB MyISAM NDB Memory Archive BDB 存储限制 64TB No Yes Yes No No 事务 Yes Yes 锁粒度 Row Table Row Table Row Page MVCC Yes Yes Yes B树索引 Yes Yes Yes Yes Yes 哈希索引 Yes Yes Yes 全文索引 5.6支持英文 Yes 集群索引 Yes 数据缓存 Yes Yes Yes 索引缓存 Yes Yes Yes Yes 数据压缩 Yes Yes 加密传输 Yes Yes Yes Yes Yes Yes 批量插入 相对低 高 高 高 非常高 高 内存消耗 高 低 高 中 低 低 存储空间消耗 高 低 低 N/A 非常低 低 外键支持 Yes 复制支持 Yes Yes Yes Yes Yes Yes 查询缓存 Yes Yes Yes Yes Yes Yes 备份恢复 Yes Yes Yes Yes Yes Yes 数据字典更新 Yes Yes Yes Yes Yes Yes 备份/时间点恢复 Yes Yes Yes Yes Yes Yes 集群支持 Yes]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[泰国旅游攻略]]></title>
    <url>%2F%E6%97%85%E6%B8%B8%2FThailand-Raiders%2F</url>
    <content type="text"><![CDATA[准备工作一、签证 所需资料 护照原件（无皮损，且有180天以上的有效期） 往返泰国的机票 携带20,000以上泰铢的现金或者等值货币（约3700-4000元人民币，汇率会有所波动，建议携带足够现金）过关时抽查，如没有携带足够的现金，有可能会被拒签或遣返。 1张2寸白底彩色照片（如没有带照片。可现场拍照，额外拍照费用100泰铢） 费用 1000泰铢手续费（目前落地签免费） 如没带照片，需现场拍照，加收100泰铢拍照费 只收泰铢现多，旁有兑换点，但汇率比较低，建议办理落地签，提前在国内换好所需泰铢费用。(中国银行可预约兑换泰铢，汇率约为1：5，实时汇率请咨询银行） 落地签办理流程 在机场按照指示牌找到“Visa on Arrival” （落地签）柜台 填写落地签申请表（柜台上就发放有表格，以及填写示例，最好提前打印下来填好） 递交资料（包括申请表、照片、护照、往返机票等），工作人员审核后给排队号 等叫号，上交资料和手续费，工作人员在你的护照上盖上落地签的章，病提供泰文收据等 走到“For Visa-on-Arrival Only”的柜台，入境官会再检查你的护照等资料，然后把出入境卡的入境部分撕掉，将出境卡盖章之后订在护照上，你就可以跨过边检柜台，正式进入泰国境内了 落地签的有效期是15天，即你可以在泰国逗留不超过15天的时间 9.10北京出发北京南站-&gt;天津站： C2077 20：50-21：20 二等座 54.5￥ 天津天津站-&gt;天津滨海国际机场T1航站楼 地铁二号线 45min 约3￥ 约10点到达机场 天津滨海国际机场T1航站楼 -&gt;廊曼国际机场T1航站楼SL963 23:50-03:35 +1 经济舱 9.11泰国曼谷曼谷酒店位置：455/4 Maha Chai Road, Bangkok, Krung Thep Maha Nakhon 10200, Thailand 廊曼国际机场T1航站楼 -&gt;曼谷酒店 455/4 Maha Chai Road淘宝下单接送机，或者携程下单（建议携程，航班延误可以2小时免费等）。到酒店睡觉啦啦啦啦 9.11 10:00浏览景点安排 大皇宫 3h 500泰铢 111.5￥ 长袖，长裙不能露膝盖卧佛寺 0.5h 200泰铢 44￥坐船 3泰铢郑王庙 0.5h 30泰铢 7￥考山路夜市吃吃吃 2h 9.12 泰国曼谷退房（留小费） 酒店-&gt;汽车站begin1、步行到船站 船票单次50🐷2、坐船3、下船后步行到BST车站4、坐上BST，约8站到达Ekkamai ，2号出站口出站下楼梯再向后方走100m就到了end 汽车站买票：窗口正上方写着Pattaya就可以买票了约108🐷一位上下浮动 厕所3🐷一次曼谷到芭提雅媒体7：00-20：00，每半小时一班车 车程2h 9.12 芭提雅酒店位置：88/999,Moo.10 Building B Room No.999, Pattayasaisong Rd, Muang Pattaya, Amphoe Bang Lamung, Chang Wat Chon Buri 20150 双条车路线图上车不要说话，说话会被误认为包车，10🐷/人 游玩路线芭提雅车站在North Pattaya Road路 Bus Terminal距酒店2.6公里，可以考虑步行 东芭乐园 9：00-18：00 800泰铢 178￥ 双条车 四方水上市场 9：00-20：00 蒂芬妮人妖秀 每天18：00、19：30、21：00共三场 普通800🐷，vip1000🐷、金vip1200🐷 178￥ 双条车每人10🐷 或步行 芭提雅海滩走一波 吃小吃 9.13芭提雅格兰岛一日游：网上跟团即可海上滑翔、海底漫步、摩托艇、香蕉船、浮潜。 9.14芭提雅购物中心逛一逛 到Bus Terminal汽车站买票回曼谷：窗口正上方写着Ekkamai，最晚班次晚上11点 9.14曼谷曼谷酒店位置：Ratchaprarop Road Ratchaprarop Road indraCondo/177/209 soi saengkran/ratchprarop road, Khet Ratchathewi, Krung Thep Maha Nakhon 10400, Thailand, Khet Ratchathewi, Krung Thep Maha Nakhon 10400, Thailand 9.15天津预约接机（提前两天预定） 泰国必买BIG-C零食711 必买药品 必买化妆品欧莱雅唇膏、小黑瓶ONLY 小白瓶N7冰激凌面膜 必逛华歌尔内衣Wacoal（曼谷尚泰商圈和暹罗挨着central world 三/四楼）LeeLeVI’S（暹罗广场，国内五折）everyandboy泰国本土美妆店日本松本清（central world 三楼） 泰国机场退税取货]]></content>
      <categories>
        <category>旅游</category>
      </categories>
      <tags>
        <tag>旅游攻略</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设置git忽略.idea文件]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%2FGIt-Ignore%2F</url>
    <content type="text"><![CDATA[1.将.idea目录加入ignore： 1$ echo '.idea' &gt;&gt; .gitignore 2.从git中删除idea： 1$ git rm -r --cached .idea 3.将.gitignore文件加入git： 1$ git add .gitignore 4.提交.gitignore文件，将.idea从代码仓库中忽略： 1$ git commit -m '忽略.idea文件夹' 5、Push到Git服务器： 1$ git push]]></content>
      <categories>
        <category>项目搭建</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++右值引用]]></title>
    <url>%2FC%2FC%2B%2B-02%2F</url>
    <content type="text"><![CDATA[左值与右值的区分在C++中，所有的值均被分为左值与右值之一。左值是指表达式结束后依然存在的持久化对象，右值是指表达式结束时就不再存在的临时对象。所有的具名变量或者对象都是左值，而右值不具名。有一个很简单的方法区别左值与右值，观察是否可以对表达式进行取地址，如果能，则为左值，否则为右值。 12345int func() &#123;return func;&#125;int a = 1 + 2;int b = func();int c = a + b;int d = c; 如上图所示，第2行的 a显然是左值，而(1 + 2)产生的临时变量值则为右值。同理第3行的b为左值，而func()的返回值同样是一个临时变量，为右值。第4行的c为左值，而(a+b)作为临时变量则为右值，而第5行中，d与c都为左值。套用上述的区别方法，a,b,c,d变量均可以进行取地址操作，而(1+2), func()返回值, (a+b)我们都无法取其地址。如书中记载，在C++11中，右值分为纯右值与将亡值。纯右值指的就是我们上述所描述的各类临时变量。而将亡值则是c++11新增的和右值引用相关的表达式，这样的表达式通常时将要移动的对象、T&amp;&amp;函数返回值、std::move()函数的返回值等，将亡值与纯右值均为右值，使用没有任何区别。 左值引用与右值引用在C++11前引用已经非常常见了，就是给变量取了一个别名，如下所示。 123int b = 1;int &amp;a = b;int &amp;c = 2; //编译错误! 需注意，在定义左值引用的同时，我们不能将右值绑定至左值引用上，如上面第3行代码所示。而C++的右值引用同左值引用相似，则使用 &amp;&amp; 来表示。 12345int func() &#123;return 1;&#125;int &amp;&amp; a = 1;int &amp;&amp; b = (1 + 2) * 3;int &amp;&amp; c = func();int &amp;&amp; d = a; //编译错误! 同左值引用相同，在定义右值引用的同时，我们不能将左值绑定在右值引用上，如上面第5行所示。也就是说，右值引用只能绑定右值，左值引用只能绑定左值。若希望将一个左值绑定到右值引用，则可以使用移动语义std::move()将左值转换为右值，例如: 12int a = 1;int &amp;&amp;b = std::move(a); //编译通过 但是注意，使用移动移动语义转移对象a后，a不可再次被使用!除了普通引用，还有一类我们经常使用到的引用，常量左值引用。大家可以显而易见的发现，常量左值引用是个“万能”的引用类型。它可以接受非常量左值、常量左值、右值对其进行初始化。但是在它的存活时期中，它只能是可读的。我们常在函数参数中使用到它，如下所示。 123456789101112void func(const string &amp;str) &#123; cout &lt;&lt; str &lt;&lt; endl;&#125;int main(int argc, char *argv[]) &#123; string s1("1234"); const string s2("1234"); func(s1); //非常量左值 func(s2); //常量左值 func(string("1234")); //右值 return 0;&#125; 在如上介绍中很容易可以得出一下结论(转载图片)：]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++强制类型转换]]></title>
    <url>%2FC%2FC%2B%2B-01%2F</url>
    <content type="text"><![CDATA[C语言类型转换c语言类型转换有如下两种(旧式转型) 12(T)expression //将 expression 转型为TT(expression) //将 expression 转型为T C++新式类型转换C++类型转换有如下四种 1234const_cast&lt;T&gt;(expression)dynamic_cast&lt;T&gt;(expression)reinterpret_cast&lt;T&gt;(expression)static_cast&lt;T&gt;(expression) const_cast一般用于移除对象的const与volatile。如下图所示，b可以修改a的值。但是注意，编译器会进行优化，将数字常量1替代a常量。所以cout &lt;&lt; a &lt;&lt; endl;输出为1。 123456789#include &lt;iostream&gt;using namespace std;int main(int argc, char *argv[]) &#123; const int a = 1; int* b = const_cast&lt;int*&gt; (&amp;a); *b = 2; cout &lt;&lt; a &lt;&lt; " " &lt;&lt; *(&amp;a) &lt;&lt; " " &lt;&lt; *b &lt;&lt; endl;&#125;// 输出：1 2 2 dynamic_castdynamic_cast 主要作用是将指向派生类对象的基类指针或引用，安全的转换为指向派生类对象的派生类指针或引用，并使用转换后的指针调用派生类独有的函数(非虚函数)。如果转换指针转换失败，则将返回空指针；如果转换引用失败，则将会抛出一个名为std::bad_cast的异常。在如下3种情况中转换可以成功 expression的类型与待转换类型相同。则转换必定成功。 expression的类型为待转换类型的公有派生类。(指针向上转换) expression的类型为待转换类型的公有基类时，必须满足以下两个要求，才会转换成功，否则转换失败。(指针向下转换) 当expression为指向派生类的指针或引用派生类对象的基类引用。 基类中必须包含虚函数，也就是必须具备多态性。 假设有如下两个类 123456789101112131415161718192021222324252627#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;class Base &#123;public: Base() &#123;&#125; Base(string s) : str(s) &#123;&#125; virtual void Print() &#123;cout &lt;&lt; str &lt;&lt; endl;&#125;private: string str;&#125;;class Derived : public Base &#123;public: Derived() &#123;&#125; Derived(string s, int i) : Base(s), ival(i) &#123;&#125; void Print() &#123; Base::Print(); cout &lt;&lt; ival &lt;&lt; endl; &#125; void PrintIval() &#123; cout &lt;&lt; ival &lt;&lt; endl; &#125;private: int ival;&#125;; example: 1234567891011int main(int argc, char *argv[])&#123; //基类指针指向派生类对象，基类中包含虚函数，符合向下转换规则。 Base* b = new Derived("test", 1); //使用基类指针无法调用派生类独有的函数，编译无法通过 b-&gt;PrintIval(); //类型转换至派生类指针就可以调用到派生类独有的函数 Derived* d = dynamic_cast&lt;Derived*&gt; (b); d-&gt;PrintIval(); return 0;&#125; reinterpret_castreinterpret_cast 主要的作用为允许任意长度相同的对象之间进行转换，而转换的安全性，则全部由程序员所保证，它只关注对象之间长度是否相同，长度不相同则无法通过编译。注意，reinterpret_cast无法去掉源对象的const、volatile属性。 1234567891011int main(int argc, char *argv[])&#123; char a = 1; //将a的指针转化为长整型数 long b = reinterpret_cast&lt;long&gt;(&amp;a); //将空指针转化为长整型数 long d = reinterpret_cast&lt;long&gt;(NULL); //编译报错 int 4字节，而指针8字节 int e = reinterpret_cast&lt;int&gt;(&amp;a); return 0;&#125; 需注意在32位机器上，指针为4字节，而在64位机器上，指针为8字节。 static_caststatic_cast 类似C语言强制类型转换，它可以完成如下一些转换 编译器隐式执行的类型转换，如int与float、double与char、enum与int之间的转换等。(精度大-&gt;精度小使用位截断处理) 将任意类型表达式转换为void类型，或从void*指针中找回其中的值。 基类与派生类指针或引用类型之间的转换，注意，由派生类转换至基类时(向上转换)是安全的，由基类至派生类转换时(向下转换)是非安全的。example(使用上述代码定义的类):1234567891011121314151617181920int main(int argc, char *argv[])&#123; char a = 'a'; //将 char -&gt; int int i = static_cast&lt;int&gt;(a); Base* b = new Base("123"); //将 Base 转换为 void* void* v = static_cast&lt;void*&gt;(b); //将 *void 转换为 Base b = static_cast&lt;Base*&gt;(v); Derived* d = new Derived("123", 123); //向上转换 将Derived* 转换为 Base* Base* bb = static_cast&lt;Base*&gt;(d); //向下转换 将Base* 转换为 Derived* Derived* dd = static_cast&lt;Derived*&gt;(b); //未定义的行为!非常危险!父类对象并不包含该函数，编译不会报错! dd-&gt;PrintIval();&#125; 一下内容取自effective C++ 如果可以，尽量避免转型，特别是在注重效率的代码中避免 dynamic_cast。如果有个设计需要转型的动作，试着发展无需转型的替代设计。如果转型是必要的，试着将它隐藏至某个函数背后，客户随后可以调用该函数，而不需将转型放进他们的代码内。宁可使用C++-style(新式转型)，不要使用旧式转型。前者很容易辨识出来，而且有其不同的职责。]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka-生产者]]></title>
    <url>%2FKafka%2FKafka-Producer%2F</url>
    <content type="text"><![CDATA[Kafka生产者kafka数据生产流程如图： 创建一个 ProducerRecord 对象,包括目标主题和要发送的内容 将对象序列化成字节数组 数据被传给分区： 是否指定了partition -&gt; 直接到指定分区 是否指定了key -&gt; 分区器使用该 key 进行 hash 操作，然后对 topic 对应的分区数量进行取模操作并返回一个分区。 没有指定key -&gt; 则通过先产生随机数，之后在该数上自增的方式产生一个数，并转为正数之后进行取余操作。 添加到批次，并发送 服务器收到消息后返回响应 成功 -&gt; 返回 RecordMetaData对象 失败 -&gt; 返回错误信息]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka-MacOs安装]]></title>
    <url>%2FKafka%2FKafka-Install%2F</url>
    <content type="text"><![CDATA[MacOS Docker 安装安装和镜像加速参考docker安装教程 Docker 下载Zookeeper 和 kafka 镜像123~ » docker pull zookeeper:latest~ » docker pull wurstmeister/kafka:latest~ » docker pull sheepkiller/kafka-manager 启动容器1、创建网络：由于要涉及到zookeeper和kafka之间的通信，所以我们运用docker内部容器通信机制先新建一个网络。 123~ » docker network create appd481270a05236007178e6ed0ce4b775c9d2aebb6c13bc050bb852bc46ca0b874 运行 docker network ls查看新建的网络 1234567~ » docker network ls NETWORK ID NAME DRIVER SCOPEd481270a0523 app bridge local0ab6b1467267 bridge bridge localcd08298f526b host host local86a734066770 none null local 运行docker network inspect app查看网络详细信息 12345678910111213141516171819202122232425262728293031~ » docker network inspect app [ &#123; "Name": "app", "Id": "d481270a05236007178e6ed0ce4b775c9d2aebb6c13bc050bb852bc46ca0b874", "Created": "2019-07-19T06:57:10.768655482Z", "Scope": "local", "Driver": "bridge", "EnableIPv6": false, "IPAM": &#123; "Driver": "default", "Options": &#123;&#125;, "Config": [ &#123; "Subnet": "172.18.0.0/16", "Gateway": "172.18.0.1" &#125; ] &#125;, "Internal": false, "Attachable": false, "Ingress": false, "ConfigFrom": &#123; "Network": "" &#125;, "ConfigOnly": false, "Containers": &#123;&#125;, "Options": &#123;&#125;, "Labels": &#123;&#125; &#125;] 可以看到其连接的containers为空，说明还没有容器连接进来2、创建Zookeeper容器 1~ » docker run --net=app --name zookeeper -p 2181 -t zookeeper 遇到了如下问题 12docker: Error response from daemon: Conflict. The container name "/zookeeper" is already in use by container "26ffbd391e8c6e5e90b8f593e354f80768f179741e1de35640efacc6303fdad0". You have to remove (or rename) that container to be able to reuse that name.See 'docker run --help'. docker ps -l 查看发现已经创建的zookeeper 可以使用docker rm 删除 12345~ » docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES26ffbd391e8c zookeeper "/docker-entrypoint.…" 8 minutes ago Created zookeeper~ » docker rm 26ffbd391e8c 重新执行创建命令 run，创建新容器，并为容器配置一些参数。 -t，在容器内部创建一个tty或者伪终端。 -i，允许主机终端按照容器内部的标准与其交互。 -d，后台运行容器并打印容器名称。 –name，容器名称。 -p，端口映射，参数格式为：主机物理端口:容器内部端口。 最后跟上的就是我们已经下载的镜像 3、创建Kafka容器 1234567~ » docker run --net=app --name kafka -p 9092 \--env HOST_IP=127.0.0.1 \--env KAFKA_ADVERTISED_HOST_NAME=localhost \--env KAFKA_ADVERTISED_PORT=9092 \--env KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \--link zookeeper \wurstmeister/kafka:latest -e，配置容器环境变量。 –link，链接到另一个容器，参数格式为：目标容器名称:在本容器内的别名。 这里的环境变量设置，其实是就是对即将创建的Kafka配置文件server.properties进行初始化。 4、创建kafka-manager 12345~ » docker run --net=app \--name kafka-manager \-p 9000:9000 \-e ZK_HOSTS=zookeeper:2181 \sheepkiller/kafka-manager 访问ip:9000即可 5、测试Kafka进入kafka容器 1~ » docker exec -it kafka /bin/bash 发送消息 12345bash-4.4# kafka-console-producer.sh --broker-list localhost:9092 --topic test&gt;hello&gt;AAAA&gt;BBBB&gt;hey 读取消息(需要打开另一个终端) 1234567bash-4.4# kafka-console-consumer.sh \&gt; --bootstrap-server localhost:9092 \&gt; --topic test --from-beginninghelloAAAABBBBhey 测试成功！(＾－＾)V 参考https://cloud.tencent.com/developer/news/371290]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot + MyBatis 多模块项目搭建]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA%2FProject-Module%2F</url>
    <content type="text"><![CDATA[准备开发工具及系统环境 IDE：IntelliJ IDEA 2019.1 系统环境：mac OSX 项目目录结构 biz层：业务逻辑层 dao层：数据持久层，使用MB插件生成相关代码及xml common层：提供工程层面的基础工具类。 web层：请求处理层 搭建步骤 搭建父工程1、 IDEA 工具栏选择菜单 File -&gt; New -&gt; Project…2、选择Spring Initializr，Initializr默认选择Default，点击Next3、填写项目资料,点击Next4、直接点击Next5、填写name，点击Finish6、项目结构如下7、删除多余目录，只留如下结构 创建子模块8、选择项目根目录,右键-&gt;New -&gt; Module9、选择Maven，点击Next10、填写ArifactId，点击Next11、点击Finish12、同理添加其他子模块，最终项目目录结构如下图 模块间依赖关系各个子模块的依赖关系： biz层：依赖dao层，common层 dao层：不依赖 common层：不依赖 web层：依赖biz层，common层。 13、父pom文件中声明所有子模块依赖 123456789101112131415161718192021222324&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;biz&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;dao&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;web&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 14、biz层pom文件中添加dao层，common层依赖 12345678910&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;dao&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 15、web层pom文件中添加biz层，common层依赖 12345678910&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;biz&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 运行项目16、在web层pom文件中添加spring-boot-starter-web 12345&lt;!-- spring-boot --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 17、在web层创建com.example.test.demo.web包并添加入口类AppServiceApplication.java，目录结构如下入口类代码如下： 123456789101112package com.example.test.demo.web;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class AppServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(AppServiceApplication.class, args); &#125;&#125; 18、在com.example.test.demo.web包下创建controller目录添加test方法测试接口是否可以正常访问 123456789101112131415package com.example.test.demo.web.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping("demo")public class DemoController &#123; @RequestMapping("test") public String test() &#123; return "Hello World!"; &#125;&#125; 19、运行AppServiceApplication中的main方法启动项目，默认端口为8080，访问http://localhost:8080/demo/test得到如下效果 20、在biz层创建com.example.test.demo.biz包并创建DemoService接口类代码如下： 12345package com.example.test.demo.biz;public interface DemoService &#123; String test();&#125; 21、在com.example.test.demo.biz包下创建impl目录并添加DemoServiceImpl类，代码如下： 123456789101112131415package com.example.test.demo.biz.impl;import com.example.test.demo.biz.DemoService;import org.springframework.stereotype.Service;@Servicepublic class DemoServiceImpl implements DemoService &#123; @Override public String test() &#123; return "biz test"; &#125;&#125; 22、DemoController类通过@Autowired注解注入DemoService，修改DemoController的test方法，代码如下： 1234567891011121314151617181920package com.example.test.demo.web.controller;import com.example.test.demo.biz.DemoService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping("demo")public class DemoController &#123; @Autowired private DemoService demoService; @RequestMapping("test") public String test() &#123; return demoService.test(); &#125;&#125; 23、在入口类AppServiceApplication上添加@ComponentScan注解 123456789101112131415package com.example.test.demo.web;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.ComponentScan;@SpringBootApplication@ComponentScan(basePackages = &#123; "com.example.test.demo.*"&#125;)public class AppServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(AppServiceApplication.class, args); &#125;&#125; 24、更改完之后运行main方法，访问http://localhost:8080/demo/test得到如下效果 25、其他层同理验证。 集成Mybatis26、父pom文件中声明mybatis-spring-boot-starter、mysql-connector-java等依赖。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;biz&lt;/module&gt; &lt;module&gt;dao&lt;/module&gt; &lt;module&gt;common&lt;/module&gt; &lt;module&gt;web&lt;/module&gt; &lt;/modules&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;mysql-connector.version&gt;8.0.11&lt;/mysql-connector.version&gt; &lt;mybatis.version&gt;1.3.2&lt;/mybatis.version&gt; &lt;mybatis.generator.version&gt;1.3.2&lt;/mybatis.generator.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;!--mybatis--&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.generator.version&#125;&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql-connector.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.generator.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;biz&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;dao&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.example.test&lt;/groupId&gt; &lt;artifactId&gt;web&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mybatis--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql-connector.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 27、在dao层中的pom文件中添加以下依赖 1234567891011121314151617&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;configurationFile&gt;$&#123;basedir&#125;/src/main/resources/mybatis-generator.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 28、在web/src/main/resources下添加application.properties com.mysql.jdbc.Driver 是 mysql-connector-java 5中的 com.mysql.cj.jdbc.Driver 是 mysql-connector-java 6中的 1234567spring.datasource.driverClassName = com.mysql.cj.jdbc.Driverspring.datasource.url = jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF8&amp;connectTimeout=1000&amp;socketTimeout=3000spring.datasource.username = rootspring.datasource.password = qwertyuimybatis.type-aliases-package = com.example.test.demo.dao.pomybatis.mapper-locations = classpath:mapper/*.xml 29、在web/src/main/resources下添加mybatis-generator.xml配置内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;generatorConfiguration&gt; &lt;!-- 引入SpringBoot配置文件 --&gt; &lt;properties resource="application.properties"/&gt; &lt;context id="Mysql" targetRuntime="MyBatis3" defaultModelType="flat"&gt; &lt;!-- 生成的pojo，将implements Serializable--&gt; &lt;plugin type="org.mybatis.generator.plugins.SerializablePlugin"/&gt; &lt;commentGenerator&gt; &lt;!--*是否去除自动生成的注释包含时间戳 true：是 ： false:否--&gt; &lt;property name="suppressDate" value="true" /&gt; &lt;!-- 是否去除自动生成的注释 true：是 ： false:否 --&gt; &lt;property name="suppressAllComments" value="true" /&gt; &lt;/commentGenerator&gt; &lt;jdbcConnection driverClass="$&#123;spring.datasource.driverClassName&#125;" connectionURL="$&#123;spring.datasource.url&#125;" userId="$&#123;spring.datasource.username&#125;" password="$&#123;spring.datasource.password&#125;"&gt; &lt;/jdbcConnection&gt; &lt;!-- 生成model模型，对应的包路径，以及文件存放路径(targetProject)，targetProject可以指定具体的路径,如./src/main/java， 也可以使用“MAVEN”来自动生成，这样生成的代码会在target/generatord-source目录下 --&gt; &lt;javaModelGenerator targetPackage="com.example.test.demo.dao.po" targetProject="../dao/src/main/java" &gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;!-- 从数据库返回的值被清理前后的空格 --&gt; &lt;property name="trimStrings" value="true"/&gt; &lt;/javaModelGenerator&gt; &lt;!--对应的mapper.xml文件 --&gt; &lt;sqlMapGenerator targetPackage="mapper" targetProject="../dao/src/main/resources"&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 对应的Mapper接口类文件 --&gt; &lt;javaClientGenerator type="XMLMAPPER" targetPackage="com.example.test.demo.dao.mapper" targetProject="../dao/src/main/java"&gt; &lt;property name="enableSubPackages" value="true"/&gt; &lt;/javaClientGenerator&gt; &lt;!-- 表名及对应po类名称--&gt; &lt;table tableName="user_info" domainObjectName="UserInfoPO" enableCountByExample="true" enableUpdateByExample="true" enableDeleteByExample="true" enableSelectByExample="true" selectByExampleQueryId="false"&gt; &lt;property name="useActualColumnNames" value="false"/&gt; &lt;generatedKey column="id" identity="true" sqlStatement="MySql"/&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 30、根据表自动生成对应的Mapper以及po类,步骤如下。得到目录如下 30、biz层下DemoServiceImpl通过@Autowired注解注入UserMapper，修改DemoService的test方法如下： 12345678910111213141516171819202122package com.example.test.demo.biz.impl;import com.example.test.demo.biz.DemoService;import com.example.test.demo.dao.mapper.UserInfoPOMapper;import com.example.test.demo.dao.po.UserInfoPO;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class DemoServiceImpl implements DemoService &#123; @Autowired private UserInfoPOMapper userInfoPOMapper; @Override public String test() &#123; UserInfoPO po = userInfoPOMapper.selectByPrimaryKey(1L); return "UserInfo name is:" + po.getName(); &#125;&#125; 31、在入口类AppServiceApplication上中添加注解 1@MapperScan("com.example.test.demo.dao.mapper") 32、运行main方法启动项目 遇到的坑mybatis 自动生成时遇到 Client does not support authentication protocol requested by server; consider upgrading MySQL client 解决方法登录mysql： 12ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'qwertyui';SELECT plugin FROM mysql.user WHERE User = 'root';]]></content>
      <categories>
        <category>项目搭建</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
        <tag>教程</tag>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka背景及架构介绍]]></title>
    <url>%2FKafka%2FKafka-Background%2F</url>
    <content type="text"><![CDATA[Kafka创建背景kafka最初是LinkedIn的一个内部基础设施系统。最初开发的起因是，LinkedIn虽然有了数据库和其他系统可以用来存储数据，但是缺乏一个可以帮助处理持续数据流的组件。它的设计目的是提供一个高性能的消息系统，可以处理多种类型数据，并能够实时提供纯洁且结构化的用户活动数据和系统度量指标。 Kafka简介Kafka是由LinkedIn开发，使用Scala编写的一个分布式的消息系统，具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。一般在架构设计中起到解耦、削峰、异步处理的作用。 kafka对外使用topic的概念，生产者往topic里写消息，消费者从读消息。为了做到水平扩展，一个topic实际是由多个partition组成的，遇到瓶颈时，可以通过增加partition的数量来进行横向扩容。单个parition内是保证消息有序。 每新写一条消息，kafka就是在对应的文件append写，所以性能非常高。 Kafka架构一、名词解释 Broker：消息中间件处理节点（服务器），一个节点就是一个broker，一个Kafka集群由一个或多个broker组成 Topic：Kafka对消息进行归类，发送到集群的每一条消息都要指定一个topic Partition：物理上的概念，每个topic包含一个或多个partition，一个partition对应一个文件夹，这个文件夹下存储partition的数据和索引文件，每个partition内部是有序的 Producer：生产者，负责发布消息到broker Consumer：消费者，从broker读取消息 ConsumerGroup：每个consumer属于一个特定的consumer group，可为每个consumer指定group name，若不指定，则属于默认的group，一条消息可以发送到不同的consumer group，但一个consumer group中只能有一个consumer能消费这条消息 replica：partition 的副本，保障 partition 的高可用。 leader：replica 中的一个角色， producer 和 consumer 只跟 leader 交互。 follower：replica 中的一个角色，从 leader 中复制数据。 controller：每个集群都有一个broker同时充当了集群控制器角色（自动从集群的活跃成员中选举出来），负责管理工作包括分配分区给broker，监控broker等等 zookeeper：Kafka 通过 zookeeper 来存储集群的 meta 信息。 二、架构图总体数据流Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉去指定Topic的消息，然后进行业务处理。 生产者①首先要构造一个 ProducerRecord 对象，该对象可以声明主题Topic、分区Partition、键 Key以及值 Value，主题和值是必须要声明的，分区和键可以不用指定。 ②调用send() 方法进行消息发送。 ③因为消息要到网络上进行传输，所以必须进行序列化，序列化器的作用就是把消息的 key 和 value对象序列化成字节数组。 ④接下来数据传到分区器，如果之间的 ProducerRecord 对象指定了分区，那么分区器将不再做任何事，直接把指定的分区返回；如果没有，那么分区器会根据 Key 来选择一个分区，选择好分区之后，生产者就知道该往哪个主题和分区发送记录了。 ⑤接着这条记录会被添加到一个记录批次里面，这个批次里所有的消息会被发送到相同的主题和分区。会有一个独立的线程来把这些记录批次发送到相应的 Broker 上。 ③Broker成功接收到消息，表示发送成功，返回消息的元数据（包括主题和分区信息以及记录在分区里的偏移量）。发送失败，可以选择重试或者直接抛出异常。 key的作用：可以为消息的附加消息，也可以用来决定消息该被写到哪个主题分区，拥有相同key的消息将会被写到同一分区 topic从上图可以看出，Topic中数据是顺序不可变序列，采用log追加方式写入，因而kafka中无因随机写入导致性能低下的问题。 Topic的数据可存储在多个partition中，即可存放在不同的服务器上。这可使Topic大小不限于一台server容量。同时，消息存在多个partition上，可以实现Topic上消息的并发访问。 partition 每个 Partition 中的消息都是有序的，生产的消息被不断追加到 Partition log 上，其中的每一个消息都被赋予了一个唯一的 offset 值。 因此数据不会因消费而丢失，所以只要consumer指定offset，一个消息可被不同的consumer多次消费。 kafka中只能保证partition中记录是有序的，而不保证topic中不同partition的顺序。 分区的原因： 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了; 可以提高并发，因为可以以 Partition 为单位读写了。 Replication同一个 partition 可能会有多个 replication(对应 server.properties 配置中的 default.replication.factor=N)。 没有 replication 的情况下，一旦 broker 宕机，其上所有 patition 的数据都不可被消费，同时 producer 也不能再将数据存于其上的 patition。 引入 replication 之 后，同一个 partition 可能会有多个 replication，而这时需要在这些 replication 之间选出一个 leader，producer 和 consumer 只与这个 leader 交互，其它 replication 作为 follower 从 leader 中复制数据。 消费者订阅topic是以一个消费组来订阅的，一个消费组里面可以有多个消费者。 同一个消费组中的两个消费者，不会同时消费一个partition。换句话来说，就是一个partition，只能被消费组里的一个消费者消费，但是可以同时被多个消费组消费。 因此，如果消费组内的消费者如果比partition多的话，那么就会有个别消费者一直空闲。 Kafka Zookeeper 节点 Kafka常见的应用场景1.消息队列比起大多数的消息系统来说，Kafka有更好的吞吐量，内置的分区，冗余及容错性，这让Kafka成为了一个很好的大规模消息处理应用的解决方案。 消息系统一般吞吐量相对较低，但是需要更小的端到端延时，并尝尝依赖于Kafka提供的强大的持久性保障。在这个领域，Kafka足以媲美传统消息系统， 如ActiveMQ或RabbitMQ。 2.行为跟踪行为跟踪是kafka基于发布订阅模式的扩展应用，当我们跟踪用户浏览页面、搜索及其他行为时，以发布-订阅的模式实时记录到对应的topic里那么这些结果被订阅者拿到后，就可以做进一步的实时处理，或实时监控，或放到hadoop/离线数据仓库里处理。 3.元信息监控与行为跟踪相似，作为操作记录的监控模块来使用，即汇集记录一些操作信息，可以理解为运维性质的数据监控吧。 4.日志收集日志收集方面，其实开源产品有很多，包括Scribe、Apache Flume。很多人使用Kafka代替日志聚合（log aggregation）。日志聚合一般来说是从服务器上收集日志文件，然后放到一个集中的位置（文件服务器或分布式文件系统）进行处理。然而Kafka忽略掉文件的细节，将其更清晰地抽象成一个个日志或事件的消息流。这就让Kafka处理过程延迟更低，更容易支持多数据源和分布式数据处理。比其他系统Kafka具有更高的扩展性，高效的性能和因为复制导致的更高的耐用性保证，以及更低的端到端延迟。 5.流处理保存收集上游的流数据，以提供到下游的Storm或其他流式计算框架进行处理。很多用户会将那些从原始topic来的数据进行阶段性处理，汇总，扩充或者以其他的方式转换到新的topic下再继续后面的处理。例如一个文章推荐的处理流程，可能是先从数据源中抓取文章的内容，然后将其丢入一个叫做“文章”的topic中；后续操作可能是需要对这个内容进行清理，比如回复正常数据或者删除重复数据，最后再将内容匹配的结果返还给用户。这就在一个独立的topic之外，产生了一系列的实时数据处理的流程。Strom和Samza是非常著名的实现这种类型数据转换的框架。 6.持久性日志（commit log）Kafka可以为一种外部的持久性日志的分布式系统提供服务。这种日志可以在节点之外进行持久性日志的记录，节点间备份数据，并为故障节点数据回复提供一种重新同步的机制。Kafka中提供了日志压缩功能，日志压缩之后整体的日志状态仍然保留，并且通过日志回溯可以实现持久性日志的功能。在这种用法中，Kafka类似于Apache BookKeeper项目。 7.事件源将状态转移作为按时间顺序排列的记录序列，这种序列可以按时间回溯整个事件的状态变更，kafka本身的持久性，代表着他可以存储大量的日志，并且这些可以根据这些日志进行汇总和回溯等等。 实际应用中，适用最多最广泛的自然是MQ的功能。 Kafka用作MQ时与常用MQ的对比RabbitMQ——Rabbit Message Queue的简写，但不能仅仅理解其为消息队列，消息代理更合适。 RabbitMQ是一个由Erlang 语言开发的AMQP（高级消息队列协议）的开源实现。 RabbitMQ作为一个消息代理，主要和消息打交道，负责接收并转发消息。 ZeroMQ——是一个基于消息队列的多线程网络库，其对套接字类型、连接处理、帧、甚至路由的底层细节进行抽象，提供跨越多种传输协议的套接字。是网络通信中新的一层，介于应用层和传输层之间（按照TCP/IP划分），其是一个可伸缩层，可并行运行，分散在分布式系统间。 RocketMQ——阿里开源的一款高性能、高吞吐量的分布式消息中间件。 ActiveMQ——是一种开源的，实现了JMS1.1规范的，面向消息(MOM)的中间件，为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。 特性 Kafka RabbitMQ ZeroMQ RocketMQ ActiveMQ 开发语言 Scala Erlang C Java Java 支持协议 自行设计的基于TCP层的协议 AMQP TCP、UDP 自行设计 OpenWire、STOMP、REST、MQTT、XMPP、AMQP、WS 消息存储 内存、磁盘、数据库。支持大量堆积。 内存、磁盘。支持少量堆积。 消息发送端的内存或者磁盘中。不支持持久化。 磁盘。支持大量堆积。 内存、磁盘、数据库。支持少量堆积。 消息事务 支持 支持 不支持 支持 支持 负载均衡 支持 支持但支持的不好 去中心化，不支持负载均衡。本身只是一个多线程网络库。 支持 支持，可以基于zookeeper实现 集群方式 天然的‘‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave。 支持简单集群，’复制’模式，对高级集群模式支持不好。 去中心化，不支持集群。 ‘Master-Slave’ 模式，开源版本需手动切换Slave变成Master 支持简单集群模式，比如’主-备’，对高级集群模式支持不好。 可用性 非常高（分布式） 高（主从） 高 非常高（分布式） 高（主从） 消息重复 支持at least once、at most once 支持at least once、at most once 只有重传机制，但是没有持久化，消息丢了重传也没有用。既不是at least once、也不是at most once、更不是exactly only once 支持at least once 支持at least once 吞吐量TPS 极大 比较大 极大 大（发送端不是批量发送） 比较大 时效性 ms以内 us级 ms级 ms级 订阅形式和消息分发 基于topic以及按照topic进行正则匹配的发布订阅模式。 提供了4种方式：direct, topic ,Headers和fanout。 点对点(p2p) 基于topic/messageTag以及按照消息类型、属性进行正则匹配的发布订阅模式 点对点(p2p)、广播（发布-订阅） 顺序消息 支持 不支持 不支持 支持 不支持 消息确认 支持 支持 支持 支持 支持 消息回溯 支持指定分区offset位置的回溯 不支持 不支持 支持指定时间点的回溯 不支持 消费失败重试 不支持，但可以通过指定分区offset位置实现。 不支持，但是可以利用消息确认机制实现。 不支持 支持 不支持 并发度 高 极高 高 高 高 资料文档 中。有kafka作者自己写的书，网上资料也有一些。 多。有一些不错的书，网上资料多。 少。没有专门写zeromq的书，网上的资料多是一些代码的实现和简单介绍。 少。没有专门写rocketmq的书，网上的资料良莠不齐，官方文档很简洁，但是对技术细节没有过多的描述。 多。没有专门写activemq的书，网上资料多。 常用MQ的优缺点Kafka优点 性能卓越，单机写入TPS约在百万条/秒，最大的优点，就是吞吐量高。 时效性：ms级 可用性：非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消费者采用Pull方式获取消息, 消息有序, 通过控制能够保证所有消息被消费且仅被消费一次; 有优秀的第三方Kafka Web管理界面Kafka-Manager； 在日志领域比较成熟，被多家公司和多个开源项目使用； 功能支持：功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用 缺点 Kafka单机超过64个队列/分区，Load会发生明显的飙高现象，队列越多，load越高，发送消息响应时间变长 使用短轮询方式，实时性取决于轮询间隔时间； 消费失败不支持重试； 支持消息顺序，但是一台代理宕机后，就会产生消息乱序； 社区更新较慢； RabbitMQ优点 由于erlang语言的特性，mq 性能较好，高并发； 吞吐量到万级，MQ功能比较完备 健壮、稳定、易用、跨平台、支持多种语言、文档齐全； 开源提供的管理界面非常棒，用起来很好用 社区活跃度高； 缺点 erlang开发，很难去看懂源码，基本职能依赖于开源社区的快速维护和修复bug，不利于做二次开发和维护。 RabbitMQ确实吞吐量会低一些，这是因为他做的实现机制比较重。 需要学习比较复杂的接口和协议，学习和维护成本较高。 ZeroMQ优点 吞吐量：百万级 扩展性强，其他MQ都已经是成形的产品，已经是一款应用程序了。而ZeroMQ说白了就是一组库函数。 缺点 原生不支持持久化，仅支持相当有限的本地缓存，如需要消息持久化需要自己进行扩展。 在高并发环境下不会出问题，但是有可能会导致本地的缓存区被塞满而导致消息丢失的情况。 RocketMQ优点 单机吞吐量：十万级 可用性：非常高，分布式架构 消息可靠性：经过参数优化配置，消息可以做到0丢失 功能支持：MQ功能较为完善，还是分布式的，扩展性好 支持10亿级别的消息堆积，不会因为堆积导致性能下降 源码是java，我们可以自己阅读源码，定制自己公司的MQ，可以掌控 缺点 支持的客户端语言不多，目前是java及c++，其中c++不成熟； 社区活跃度一般 MQ核心代码未遵循JMS规范，有些系统要迁移需要修改大量代码 RocketMQ优点 单机吞吐量：万级 topic数量都吞吐量的影响： 时效性：ms级 可用性：高，基于主从架构实现高可用性 消息可靠性：有较低的概率丢失数据 功能支持：MQ领域的功能极其完备 遵循JMS规范安装部署方便 缺点 在并发较多时，消费端只能接收一部分，会出现丢失消息情况，需重启消费端才能接收到那部分剩下的消息。 官方社区现在对ActiveMQ 5.x维护越来越少，较少在大规模吞吐的场景中使用。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-list双向链表【学习笔记】]]></title>
    <url>%2Fredis%2FRedis-adlist%2F</url>
    <content type="text"><![CDATA[list定义1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* * 双端链表节点 */typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;&#125; listNode;/* * 双端链表迭代器 */typedef struct listIter &#123; // 当前迭代到的节点 listNode *next; // 迭代的方向 int direction;&#125; listIter;/* * 双端链表结构 */typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key); // 链表所包含的节点数量 unsigned long len;&#125; list; list常用函数listCreate-创建新链表12345678910111213141516171819202122/** * 创建一个新的链表 * @return 创建成功返回链表，失败返回 NULL * T = O(1) */list *listCreate(void)&#123; struct list *list; // 分配内存 if ((list = zmalloc(sizeof(*list))) == NULL) return NULL; // 初始化属性 list-&gt;head = list-&gt;tail = NULL; list-&gt;len = 0; list-&gt;dup = NULL; list-&gt;free = NULL; list-&gt;match = NULL; return list;&#125; listRelease-释放整个链表1234567891011121314151617181920212223242526272829/** * 释放整个链表，以及链表中所有节点 * @param list * T = O(N) */void listRelease(list *list)&#123; unsigned long len; listNode *current, *next; // 指向头指针 current = list-&gt;head; // 遍历整个链表 len = list-&gt;len; while(len--) &#123; next = current-&gt;next; // 如果有设置值释放函数，那么调用它 if (list-&gt;free) list-&gt;free(current-&gt;value); // 释放节点结构 zfree(current); current = next; &#125; // 释放链表结构 zfree(list);&#125; listAddNodeHead-添加新节点到链表头1234567891011121314151617181920212223242526272829303132333435/** * 将一个包含有给定值指针 value 的新节点添加到链表的表头 * @param list * @param value * @return 如果为新节点分配内存出错，那么不执行任何动作，仅返回 NULL，如果执行成功，返回传入的链表指针 * T = O(1) */list *listAddNodeHead(list *list, void *value)&#123; listNode *node; // 为节点分配内存 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值指针 node-&gt;value = value; // 添加节点到空链表 if (list-&gt;len == 0) &#123; list-&gt;head = list-&gt;tail = node; node-&gt;prev = node-&gt;next = NULL; // 添加节点到非空链表 &#125; else &#123; node-&gt;prev = NULL; node-&gt;next = list-&gt;head; list-&gt;head-&gt;prev = node; list-&gt;head = node; &#125; // 更新链表节点数 list-&gt;len++; return list;&#125; listAddNodeTail-添加新节点到链表尾1234567891011121314151617181920212223242526272829303132333435/** * 将一个包含有给定值指针 value 的新节点添加到链表的表尾 * @param list * @param value 新节点 * @return 如果为新节点分配内存出错，那么不执行任何动作，仅返回 NULL，如果执行成功，返回传入的链表指针 * T = O(1) */list *listAddNodeTail(list *list, void *value)&#123; listNode *node; // 为新节点分配内存 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值指针 node-&gt;value = value; // 目标链表为空 if (list-&gt;len == 0) &#123; list-&gt;head = list-&gt;tail = node; node-&gt;prev = node-&gt;next = NULL; // 目标链表非空 &#125; else &#123; node-&gt;prev = list-&gt;tail; node-&gt;next = NULL; list-&gt;tail-&gt;next = node; list-&gt;tail = node; &#125; // 更新链表节点数 list-&gt;len++; return list;&#125; listInsertNode-将新节点添加到老节点之前或之后1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 创建一个包含值 value 的新节点，并将它插入到 old_node 的之前或之后 * 如果 after 为 0 ，将新节点插入到 old_node 之前。 * 如果 after 为 1 ，将新节点插入到 old_node 之后。 * @param list 链表 * @param old_node 老节点 * @param value 值 * @param after * @return 如果为新节点分配内存出错，那么不执行任何动作，仅返回 NULL，如果执行成功，返回传入的链表指针 * T = O(1) */list *listInsertNode(list *list, listNode *old_node, void *value, int after) &#123; listNode *node; // 创建新节点 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值 node-&gt;value = value; // 将新节点添加到给定节点之后 if (after) &#123; node-&gt;prev = old_node; node-&gt;next = old_node-&gt;next; // 给定节点是原表尾节点 if (list-&gt;tail == old_node) &#123; list-&gt;tail = node; &#125; // 将新节点添加到给定节点之前 &#125; else &#123; node-&gt;next = old_node; node-&gt;prev = old_node-&gt;prev; // 给定节点是原表头节点 if (list-&gt;head == old_node) &#123; list-&gt;head = node; &#125; &#125; // 更新新节点的前置指针 if (node-&gt;prev != NULL) &#123; node-&gt;prev-&gt;next = node; &#125; // 更新新节点的后置指针 if (node-&gt;next != NULL) &#123; node-&gt;next-&gt;prev = node; &#125; // 更新链表节点数 list-&gt;len++; return list;&#125; listDelNode-删除指定节点123456789101112131415161718192021222324252627282930/** * 从链表 list 中删除给定节点 node * 对节点私有值(private value of the node)的释放工作由调用者进行。 * @param list * @param node * T = O(1) */void listDelNode(list *list, listNode *node)&#123; // 调整前置节点的指针 if (node-&gt;prev) node-&gt;prev-&gt;next = node-&gt;next; else list-&gt;head = node-&gt;next; // 调整后置节点的指针 if (node-&gt;next) node-&gt;next-&gt;prev = node-&gt;prev; else list-&gt;tail = node-&gt;prev; // 释放值 if (list-&gt;free) list-&gt;free(node-&gt;value); // 释放节点 zfree(node); // 链表数减一 list-&gt;len--;&#125; listGetIterator-生成链表的迭代器123456789101112131415161718192021222324252627/** * 为给定链表创建一个迭代器， * 之后每次对这个迭代器调用 listNext 都返回被迭代到的链表节点 * @param list 链表 * @param direction 迭代方向 * AL_START_HEAD ：从表头向表尾迭代 * AL_START_TAIL ：从表尾想表头迭代 * @return 迭代器 * T = O(1) */listIter *listGetIterator(list *list, int direction)&#123; // 为迭代器分配内存 listIter *iter; if ((iter = zmalloc(sizeof(*iter))) == NULL) return NULL; // 根据迭代方向，设置迭代器的起始节点 if (direction == AL_START_HEAD) iter-&gt;next = list-&gt;head; else iter-&gt;next = list-&gt;tail; // 记录迭代方向 iter-&gt;direction = direction; return iter;&#125; listNext-返回迭代器当前所指向的节点1234567891011121314151617181920212223/** * 返回迭代器当前所指向的节点。 * 删除当前节点是允许的，但不能修改链表里的其他节点。 * @param iter 迭代器 * @return 函数要么返回一个节点，要么返回 NULL * T = O(1) */listNode *listNext(listIter *iter)&#123; listNode *current = iter-&gt;next; if (current != NULL) &#123; // 根据方向选择下一个节点 if (iter-&gt;direction == AL_START_HEAD) // 保存下一个节点，防止当前节点被删除而造成指针丢失 iter-&gt;next = current-&gt;next; else // 保存下一个节点，防止当前节点被删除而造成指针丢失 iter-&gt;next = current-&gt;prev; &#125; return current;&#125; listDup-复制整个链表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 复制整个链表。 * 无论复制是成功还是失败，输入节点都不会修改。 * 如果链表有设置值复制函数 dup ，那么对值的复制将使用复制函数进行， * 否则，新节点将和旧节点共享同一个指针。 * @param orig * @return 复制成功返回输入链表的副本，如果因为内存不足而造成复制失败，返回 NULL 。 * T = O(N) */list *listDup(list *orig)&#123; list *copy; listIter *iter; listNode *node; // 创建新链表 if ((copy = listCreate()) == NULL) return NULL; // 设置节点值处理函数 copy-&gt;dup = orig-&gt;dup; copy-&gt;free = orig-&gt;free; copy-&gt;match = orig-&gt;match; // 迭代整个输入链表 iter = listGetIterator(orig, AL_START_HEAD); while((node = listNext(iter)) != NULL) &#123; void *value; // 复制节点值到新节点 if (copy-&gt;dup) &#123; value = copy-&gt;dup(node-&gt;value); if (value == NULL) &#123; listRelease(copy); listReleaseIterator(iter); return NULL; &#125; &#125; else value = node-&gt;value; // 将节点添加到链表 if (listAddNodeTail(copy, value) == NULL) &#123; listRelease(copy); listReleaseIterator(iter); return NULL; &#125; &#125; // 释放迭代器 listReleaseIterator(iter); // 返回副本 return copy;&#125; listSearchKey-查找值为key的节点123456789101112131415161718192021222324252627282930313233343536373839/** * 查找链表 list 中值和 key 匹配的节点。 * 对比操作由链表的 match 函数负责进行，如果没有设置 match 函数， * 那么直接通过对比值的指针来决定是否匹配。 * @param list 链表 * @param key 值 * @return 如果匹配成功，那么第一个匹配的节点会被返回。 * 如果没有匹配任何节点，那么返回 NULL 。 */listNode *listSearchKey(list *list, void *key)&#123; listIter *iter; listNode *node; // 迭代整个链表 iter = listGetIterator(list, AL_START_HEAD); while((node = listNext(iter)) != NULL) &#123; // 对比 if (list-&gt;match) &#123; if (list-&gt;match(node-&gt;value, key)) &#123; listReleaseIterator(iter); // 找到 return node; &#125; &#125; else &#123; if (key == node-&gt;value) &#123; listReleaseIterator(iter); // 找到 return node; &#125; &#125; &#125; listReleaseIterator(iter); // 未找到 return NULL;&#125; listIndex-返回链表在指定索引上的值1234567891011121314151617181920212223/** * 返回链表在给定索引上的值。 * @param list * @param index 索引以 0 为起始，也可以是负数， -1 表示链表最后一个节点，诸如此类。 * @return 如果索引超出范围（out of range），返回 NULL 。 * T = O(N) */listNode *listIndex(list *list, long index) &#123; listNode *n; // 如果索引为负数，从表尾开始查找 if (index &lt; 0) &#123; index = (-index)-1; n = list-&gt;tail; while(index-- &amp;&amp; n) n = n-&gt;prev; // 如果索引为正数，从表头开始查找 &#125; else &#123; n = list-&gt;head; while(index-- &amp;&amp; n) n = n-&gt;next; &#125; return n;&#125; listRewind-设置正向迭代器12345678910/** * 将迭代器的方向设置为 AL_START_HEAD ，并将迭代指针重新指向表头节点。 * @param list 链表 * @param li 迭代器 * T = O(1) */void listRewind(list *list, listIter *li) &#123; li-&gt;next = list-&gt;head; li-&gt;direction = AL_START_HEAD;&#125; listRewindTail-设置反向迭代器12345678910/** * 将迭代器的方向设置为 AL_START_TAIL, 并将迭代指针重新指向表尾节点。 * @param list 链表 * @param li 迭代器 * T = O(1) */void listRewindTail(list *list, listIter *li) &#123; li-&gt;next = list-&gt;tail; li-&gt;direction = AL_START_TAIL;&#125; listRotate-将链表尾移动到表头1234567891011121314151617181920/ * 取出链表的表尾节点，并将它移动到表头，成为新的表头节点。 * @param list * T = O(1) */void listRotate(list *list) &#123; listNode *tail = list-&gt;tail; if (listLength(list) &lt;= 1) return; // 取出表尾节点 list-&gt;tail = tail-&gt;prev; list-&gt;tail-&gt;next = NULL; // 插入到表头 list-&gt;head-&gt;prev = tail; tail-&gt;prev = NULL; tail-&gt;next = list-&gt;head; list-&gt;head = tail;&#125;]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis-sds动态字符串【学习笔记】]]></title>
    <url>%2Fredis%2FRedis-sds%2F</url>
    <content type="text"><![CDATA[SDS定义12345678910111213141516171819/* * 类型别名，用于指向 sdshdr 的 buf 属性 */typedef char *sds;/* * 保存字符串对象的结构 */struct sdshdr &#123; // buf 中已占用空间的长度 int len; // buf 中剩余可用空间的长度 int free; // 数据空间 char buf[];&#125;; SDS常用函数sdslen-sds长度123456789/* * 返回 sds 实际保存的字符串的长度 * * T = O(1) */static inline size_t sdslen(const sds s) &#123; struct sdshdr *sh = (void*)(s-(sizeof(struct sdshdr))); return sh-&gt;len;&#125; s 实际上存的是buf首个char数据的地址，也就是向前移动8个字节，就能到sdshdr的len的首地址char buf[]这个数组没有大小，是所谓的柔性数组，是不占据内存大小的，所以sizeof(struct sdshdr)为8。具体结构如下图 sdsavail-sds可用free空间长度123456789/* * 返回 sds 可用空间的长度 * * T = O(1) */static inline size_t sdsavail(const sds s) &#123; struct sdshdr *sh = (void*)(s-(sizeof(struct sdshdr))); return sh-&gt;free;&#125; sdsnewlen-根据字符串长度创建sds1234567891011121314151617181920212223242526272829303132333435363738/** * 根据给定的初始化字符串 init 和字符串长度 initlen,创建一个新的 sds * @param init 初始化字符串指针 * @param initlen 初始化字符串的长度 * @return 创建成功返回 sdshdr 相对应的 sds,创建失败返回 NULL * T = O(N) */sds sdsnewlen(const void *init, size_t initlen) &#123; struct sdshdr *sh; // 根据是否有初始化内容，选择适当的内存分配方式 // T = O(N) if (init) &#123; // zmalloc 不初始化所分配的内存 sh = zmalloc(sizeof(struct sdshdr)+initlen+1); &#125; else &#123; // zcalloc 将分配的内存全部初始化为 0 sh = zcalloc(sizeof(struct sdshdr)+initlen+1); &#125; // 内存分配失败，返回 if (sh == NULL) return NULL; // 设置初始化长度 sh-&gt;len = initlen; // 新 sds 不预留任何空间 sh-&gt;free = 0; // 如果有指定初始化内容，将它们复制到 sdshdr 的 buf 中 // T = O(N) if (initlen &amp;&amp; init) memcpy(sh-&gt;buf, init, initlen); // 以 \0 结尾 sh-&gt;buf[initlen] = '\0'; // 返回 buf 部分，而不是整个 sdshdr return (char*)sh-&gt;buf;&#125; sdsnew-创建sds12345678910/** * 根据给定字符串 init ，创建一个包含同样字符串的 sds * @param init 如果输入为 NULL ，那么创建一个空白 sds * @return 创建成功返回 sdshdr 相对应的 sds，创建失败返回 NULL * T = O(N) */sds sdsnew(const char *init) &#123; size_t initlen = (init == NULL) ? 0 : strlen(init); return sdsnewlen(init, initlen);&#125; sdsempty-创建空sds12345678/** * 创建并返回一个只保存了空字符串 "" 的 sds * @return 创建成功返回 sdshdr 相对应的 sds,创建失败返回 NULL * T = O(1) */sds sdsempty(void) &#123; return sdsnewlen("",0);&#125; sdsdup-复制sds创建副本123456789/** * 复制给定 sds 创建副本 * @param s sds * @return 创建成功返回输入 sds 的副本 * T = O(N) */sds sdsdup(const sds s) &#123; return sdsnewlen(s, sdslen(s));&#125; sdsfree-释放sds123456789/** * 释放给定的 sds * @param s * T = O(N) */void sdsfree(sds s) &#123; if (s == NULL) return; zfree(s-sizeof(struct sdshdr));&#125; sdsgrowzero-扩充sds未使用空间补0123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * 将 sds 扩充至指定长度，未使用的空间以 0 字节填充。 * @param s * @param len 指定长度 * @return 扩充成功返回新 sds ，失败返回 NULL * T = O(N) */sds sdsgrowzero(sds s, size_t len) &#123; struct sdshdr *sh = (void*)(s-(sizeof(struct sdshdr))); size_t totlen, curlen = sh-&gt;len; // 如果 len 比字符串的现有长度小， // 那么直接返回，不做动作 if (len &lt;= curlen) return s; // 扩展 sds // T = O(N) s = sdsMakeRoomFor(s,len-curlen); // 如果内存不足，直接返回 if (s == NULL) return NULL; // 将新分配的空间用 0 填充，防止出现垃圾内容 // T = O(N) sh = (void*)(s-(sizeof(struct sdshdr))); memset(s+curlen,0,(len-curlen+1)); // 更新属性 totlen = sh-&gt;len+sh-&gt;free; sh-&gt;len = len; sh-&gt;free = totlen-sh-&gt;len; // 返回新的 sds return s;&#125;/** * 对 sds 中 buf 的长度进行扩展，确保在函数执行之后， * buf 至少会有 addlen + 1 长度的空余空间（额外的 1 字节是为 \0 准备的） * @param s * @param addlen * @return 扩展成功返回扩展后的 sds，扩展失败返回 NULL * T = O(N) */sds sdsMakeRoomFor(sds s, size_t addlen) &#123; struct sdshdr *sh, *newsh; // 获取 s 目前的空余空间长度 size_t free = sdsavail(s); size_t len, newlen; // s 目前的空余空间已经足够，无须再进行扩展，直接返回 if (free &gt;= addlen) return s; // 获取 s 目前已占用空间的长度 len = sdslen(s); sh = (void*) (s-(sizeof(struct sdshdr))); // s 最少需要的长度 newlen = (len+addlen); // 根据新长度，为 s 分配新空间所需的大小 if (newlen &lt; SDS_MAX_PREALLOC) // 如果新长度小于 SDS_MAX_PREALLOC // 那么为它分配两倍于所需长度的空间 newlen *= 2; else // 否则，分配长度为目前长度加上 SDS_MAX_PREALLOC newlen += SDS_MAX_PREALLOC; // T = O(N) newsh = zrealloc(sh, sizeof(struct sdshdr)+newlen+1); // 内存不足，分配失败，返回 if (newsh == NULL) return NULL; // 更新 sds 的空余长度 newsh-&gt;free = newlen - len; // 返回 sds return newsh-&gt;buf;&#125;/* * 最大预分配长度 */#define SDS_MAX_PREALLOC (1024*1024) sdscatlen-根据字符串长度将字符串追加到sds末尾12345678910111213141516171819202122232425262728293031323334353637/** * 将长度为 len 的字符串 t 追加到 sds 的字符串末尾 * @param s * @param t 字符串t * @param len t的长度 * @return 追加成功返回新 sds ，失败返回 NULL * T = O(N) */sds sdscatlen(sds s, const void *t, size_t len) &#123; struct sdshdr *sh; // 原有字符串长度 size_t curlen = sdslen(s); // 扩展 sds 空间 // T = O(N) s = sdsMakeRoomFor(s,len); // 内存不足？直接返回 if (s == NULL) return NULL; // 复制 t 中的内容到字符串后部 // T = O(N) sh = (void*) (s-(sizeof(struct sdshdr))); memcpy(s+curlen, t, len); // 更新属性 sh-&gt;len = curlen+len; sh-&gt;free = sh-&gt;free-len; // 添加新结尾符号 s[curlen+len] = '\0'; // 返回新 sds return s;&#125; sdscat-将字符串追加到sds末尾12345678/** * 将给定字符串 t 追加到 sds 的末尾 * @return 追加成功返回新 sds ，失败返回 NULL * T = O(N) */sds sdscat(sds s, const char *t) &#123; return sdscatlen(s, t, strlen(t));&#125; sdscatsds-将sds追加到另一个sds末尾12345678/** * 将另一个 sds 追加到一个 sds 的末尾 * @return 追加成功返回新 sds ，失败返回 NULL * T = O(N) */sds sdscatsds(sds s, const sds t) &#123; return sdscatlen(s, t, sdslen(t));&#125; sdscpylen-将字符串前len复制到sds123456789101112131415161718192021222324252627282930313233343536/** * 将字符串 t 的前 len 个字符复制到 sds s 当中,覆盖原有的字符 * 如果 sds 的长度少于 len 个字符，那么扩展 sds * @return 复制成功返回新的 sds ，否则返回 NULL * T = O(N) */sds sdscpylen(sds s, const char *t, size_t len) &#123; struct sdshdr *sh = (void*) (s-(sizeof(struct sdshdr))); // sds 现有 buf 的长度 size_t totlen = sh-&gt;free+sh-&gt;len; // 如果 s 的 buf 长度不满足 len ，那么扩展它 if (totlen &lt; len) &#123; // T = O(N) s = sdsMakeRoomFor(s,len-sh-&gt;len); if (s == NULL) return NULL; sh = (void*) (s-(sizeof(struct sdshdr))); totlen = sh-&gt;free+sh-&gt;len; &#125; // 复制内容 // T = O(N) memcpy(s, t, len); // 添加终结符号 s[len] = '\0'; // 更新属性 sh-&gt;len = len; sh-&gt;free = totlen-len; // 返回新的 sds return s;&#125; sdscpy-将字符串复制到 sds 当中12345678/** * 将字符串复制到 sds 当中,覆盖原有的字符 * @return 复制成功返回新的 sds ，否则返回 NULL * T = O(N) */sds sdscpy(sds s, const char *t) &#123; return sdscpylen(s, t, strlen(t));&#125;]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《自己动手写JAVA虚拟机》学习笔记三【解析class文件】]]></title>
    <url>%2FJVM%2FJVM3%2F</url>
    <content type="text"><![CDATA[java虚拟机规范中使用一种类似C语言结构体来描述Class文件的基本结构，具体如下： 123456789101112131415161718ClassFile &#123; u4 magic;//魔数 u2 minor_version;//主版本号 u2 major_version;//次版本号 u2 constant_pool_count;//常量池长度 cp_info constant_pool[constant_pool_count-1];//常量池信息 u2 access_flags;//该类的访问修饰符 u2 this_class;//类索引 u2 super_class;//父类索引 u2 interfaces_count;//接口个数 u2 interfaces[interfaces_count];//接口详细信息 u2 fields_count;//属性个数 field_info fields[fields_count];//属性详细信息 u2 methods_count;//方法个数 method_info methods[methods_count];//方法详情 u2 attributes_count;//类文件属性个数 attribute_info attributes[attributes_count];//类文件属性详细信息&#125; 准备工作把ch02的目录结构复制一份改名ch03，在ch03的目录中创建一个classfile子目录。 12345678|-jvmgo |-ch01 |-ch01 |-ch03 |-classfile |-classpath |-cmd.go |-main.go 为了学习编译后的class文件，新建一个classFileTest.java然后编译 12345678910111213public class ClassFileTest &#123; public static final boolean FLAG = true; public static final byte BYTE = 123; public static final char X = 'X'; public static final short SHORT = 12345; public static final int INT = 123456789; public static final long LONG = 12345678901L; public static final float PI = 3.14f; public static final double E = 2.71828; public static void main(String[] args) throws RuntimeException &#123; System.out.println("Hello, World!"); &#125;&#125; 用作者提供的classpy的图形化工具，可以查看反编译后的class文件。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>JVM</tag>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《自己动手写JAVA虚拟机》学习笔记二【搜索class文件】]]></title>
    <url>%2FJVM%2FJVM2%2F</url>
    <content type="text"><![CDATA[12345public class HelloWorld &#123; public static void main(String[] args)&#123; System.out.println("Hello, world!"); &#125;&#125; 运行上面的java程序时，我们知道首先要启动java虚拟机，然后加载主类，最后调用主类的main方法。但是在加载HelloWorld类之前，首先要加载它的超类java.lang.Object，在调用main()函数之前，虚拟机要准备好参数数组，所以需要加载java.lang.String和java.lang.String[]类。把字符串打印到控制台还需要加载java.lang.System类，等等。。那么java虚拟机如何寻找这些类的呢？ 类路径类路径可以分为以下三种： 启动类路径(bootstrap classpath)：启动类路径默认对应jre/lib目录，Java标准库位于该路径。 扩展类路径(extention classpath)：扩展类路径默认对应jre/lib/ext目录，使用Java扩展机制的类位于该路径。 用户类路径(user classpath)：我们自己实现的类，以及第三方类库则位于用户类路径。用户类路径的默认值是当前路径，也就是”.”，可以给java命令传递-classpath选项来指定。 准备工作把ch01的目录结构复制一份改名ch02，在ch02的目录中创建一个classpath子目录。 123456|-jvmgo |-ch01 |-ch01 |-classpath |-cmd.go |-main.go 修改cmd结构体，添加XjreOption字段 12345678type Cmd struct &#123; helpFlag bool versionFlag bool cpOption string XjreOption string class string args []string&#125; parseCmd()函数也对应添加Xjre 123456789//命令解析func parseCmd() *Cmd &#123; ...//其他代码不变 flag.StringVar(&amp;cmd.cpOption,"cp","","classpath") flag.StringVar(&amp;cmd.XjreOption,"Xjre","","path to jre") //解析命令行参数到定义的flag flag.Parse() ...//其他代码不变&#125; 实现类路径采用组合模式来实现类路径，把类路径当成一个大的整体，由启动类路径、扩展类路径和用户类路径三个小路径构成，三个小路径又分别由更小的路径构成。 首先定义一个Entry接口 1234567891011//获取系统分隔符，windows是;类UNIX系统是:号const pathListSeparator = string(os.PathListSeparator)type Entry interface &#123; //寻找和加载class文件 参数：class文件相对路径，路径之间用/，文件名有.class后缀 //例如读取java.lang.Object入参是java/lang/Object.class readClass(classname string) ([]byte, Entry, error) //toString String() string&#125; Entry接口一共有四种实现，CompositeEntry，WildcardEntry，ZipEntry，DirEntry DirEntryDirEntry相对简单些，表示目录形式的类路径 12345678910111213141516171819202122232425262728293031323334package classpathimport ( "path/filepath" "io/ioutil")type DirEntry struct &#123; //存放目录的绝对路径 absDir string&#125;//相当于构造函数func newDirEntry(path string) *DirEntry &#123; //将参数转换成绝对路径 absDir, err := filepath.Abs(path) if err != nil &#123; panic(err) &#125; return &amp;DirEntry&#123;absDir&#125;&#125;//读取class文件func (self *DirEntry) readClass (className string) ([]byte, Entry, error) &#123; //把目录和class名拼成完成路径 fileName := filepath.Join(self.absDir,className) //读取class文件内容 data, err := ioutil.ReadFile(fileName) return data,self,err&#125;//直接返回目录func (self *DirEntry) String() string&#123; return self.absDir&#125; ZipEntryZipEntry表示ZIP或者JAR文件形式的类路径 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package classpathimport ( "path/filepath" "archive/zip" "io/ioutil" "errors")type ZipEntry struct &#123; //存放目录的绝对路径 absPath string&#125;//相当于构造函数func newZipEntry(path string) *ZipEntry &#123; //将参数转换成绝对路径 absPath, err := filepath.Abs(path) if err != nil &#123; panic(err) &#125; return &amp;ZipEntry&#123;absPath&#125;&#125;//读取class文件func (self *ZipEntry) readClass(classname string) ([]byte, Entry, error) &#123; //打开zip文件 r, err := zip.OpenReader(self.absPath) if err != nil &#123; return nil,nil,err &#125; defer r.Close() //遍历zip包里的文件 for _, f := range r.File &#123; //找到class文件 if f.Name == classname &#123; //打开class文件 rc , err := f.Open() if err != nil &#123; return nil,nil,err &#125; defer rc.Close() //读取class文件内容 data, err := ioutil.ReadAll(rc) if err != nil &#123; return nil,nil,err &#125; return data,self,err &#125; &#125; //未找到class文件 return nil,nil,errors.New("class not found :" +classname)&#125;//直接返回目录func (self *ZipEntry) String() string &#123; return self.absPath&#125; CompositeEntryCompositeEntry表示有分隔符的类路径，CompositeEntry由更小的Entry组成，可以表示成[]Entry，go语言中则使用便利的slice 12345678910111213141516171819202122232425262728293031323334353637383940package classpathimport ( "strings" "errors")type CompositeEntry []Entry//将每个小路径转换成具体的Entryfunc newCompositeEntry(pathList string) CompositeEntry &#123; var compositeEntry []Entry //将路径按照分隔符进行分割 for _, path := range strings.Split(pathList,pathListSeparator)&#123; entry := newEntry(path) compositeEntry = append(compositeEntry,entry) &#125; return compositeEntry&#125;func (self CompositeEntry) readClass(classname string) ([]byte, Entry, error) &#123; //遍历entry数据 for _, entry := range self&#123; //读取class文件，依次调用每一个子路径的readClass方法 data, from, err := entry.readClass(classname) if err == nil&#123; return data,from,err &#125; &#125; return nil,nil,errors.New("class not found :" +classname)&#125;//调用每个子路径的String方法，用分隔符拼接起来func (self CompositeEntry) String() string &#123; strs := make([]string,len(self)) for i, entry := range self&#123; strs[i] = entry.String() &#125; return strings.Join(strs,pathListSeparator)&#125; WildcardEntryWildcardEntry表示以*结尾的类路径，实际上也是CompositeEntry，因此就不再新定义类型类 1234567891011121314151617181920212223242526272829303132package classpathimport ( "strings" "os" "path/filepath")func newWildcardEntry(path string) CompositeEntry &#123; //去掉尾部的* baseDir := path[:len(path)-1] var compositeEntry []Entry walkFn := func(path string, info os.FileInfo, err error) error&#123; if err != nil&#123; return err &#125; //如果不是目录，返回跳过标识 if info.IsDir() &amp;&amp; path != baseDir &#123; return filepath.SkipDir &#125; //选出jar文件 if strings.HasSuffix(path,".jar") || strings.HasSuffix(path,".JAR")&#123; jarEntry := newZipEntry(path) compositeEntry = append(compositeEntry,jarEntry) &#125; return nil &#125; //遍历baseDir路径，创建zipEntry filepath.Walk(baseDir,walkFn) //fmt.Printf("compositeEntry : %s\n",compositeEntry) return compositeEntry&#125; Entry四种类路径都实现完之后，再来完善下Entry接口，添加Entry实例的构造方法。 12345678910111213141516func newEntry(path string) Entry &#123; //如果路径中含有分隔符 if strings.Contains(path,pathListSeparator)&#123; return newCompositeEntry(path) &#125; //如果路径末尾是* if strings.HasSuffix(path,"*")&#123; return newWildcardEntry(path) &#125; //如果路径以jar或者zip结尾 if strings.HasSuffix(path,".jar") || strings.HasSuffix(path,".JAR")|| strings.HasSuffix(path,".zip") || strings.HasSuffix(path,".ZIP")&#123; return newZipEntry(path) &#125; return newDirEntry(path)&#125; 实现Classpath1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package classpathimport ( "path/filepath" "os" "fmt")type Classpath struct &#123; bootClasspath Entry extClasspath Entry userClasspath Entry&#125;//使用-Xjre选项解析启动类路径和扩展类路径，使用-classpath/-cp选项解析用户类路径func Parse(jreOption,cpOption string) *Classpath &#123; cp := &amp;Classpath&#123;&#125; //解析启动类路径和扩展类路径 cp.parseBootAndExtClasspath(jreOption) //解析用户类路径 cp.parseUserClasspath(cpOption) return cp&#125;func getJreDir(jreOption string) string &#123; //优先使用用户输入的-Xjre作为目录 if jreOption != "" &amp;&amp; exists(jreOption)&#123; return jreOption &#125; //在当前目录下寻找jre目录 if exists("./jre") &#123; return "./jre" &#125; //尝试使用JAVA_HOME环境变量 if jh := os.Getenv("JAVA_HOME"); jh != ""&#123; return filepath.Join(jh,"jre") &#125; panic("Can not find jre folder")&#125;//判断目录是否存在func exists(path string) bool &#123; if _, err := os.Stat(path); err != nil&#123; if os.IsNotExist(err)&#123; return false &#125; &#125; return true&#125;func (self *Classpath) parseBootAndExtClasspath(jreOption string) &#123; // 获取jre目录 jreDir := getJreDir(jreOption) //jre/lib/* jreLibPath := filepath.Join(jreDir,"lib","*") self.bootClasspath = newWildcardEntry(jreLibPath) //jre/lib/ext/* jreExtPath := filepath.Join(jreDir,"lib","ext","*") self.extClasspath = newWildcardEntry(jreExtPath)&#125;//解析用户类路径func (self *Classpath) parseUserClasspath(cpOption string) &#123; // 如果用户没有提供-classpath/-cp选项，则使用当前目录作为用户类路径 if cpOption == ""&#123; cpOption = "." &#125; self.userClasspath = newEntry(cpOption)&#125;//寻找class方法func (self *Classpath) ReadClass(classname string) ([]byte, Entry, error) &#123; //访问ReadClass方法只需传递类名，不用包含".class"后缀 classname = classname + ".class" // 从bootClasspath寻找class文件 if data, entry, err := self.bootClasspath.readClass(classname); err == nil&#123; return data, entry, err &#125; // 从extClasspath寻找class文件 if data, entry, err := self.extClasspath.readClass(classname); err == nil&#123; return data, entry, err &#125; // 从userClasspath寻找class文件 return self.userClasspath.readClass(classname)&#125;func (self *Classpath) String() string &#123; return self.userClasspath.String()&#125; 测试代码完善main.go中的startJVM 123456789101112131415//模拟启动jvmfunc startJVM(cmd *Cmd) &#123; // 获取Classpath cp := classpath.Parse(cmd.XjreOption,cmd.cpOption) fmt.Printf("classpath:%s class:%s args:%v\n",cp,cmd.class,cmd.args) // 将.替换成/(java.lang.String -&gt; java/lang/String) className := strings.Replace(cmd.class,".","/",-1) // 读取class classData, _, err := cp.ReadClass(className) if err != nil &#123; fmt.Printf("Could not find or load main class %s\n",cmd.class) return &#125; fmt.Printf("class data : %v\n",classData)&#125; 编译main.go，并测试-version 12345$ go install jvmgo/ch02 $ ch02 java.lang.String# 没有传递-Xjre，会去读取$JAVA_HOME，成功打印出String.class的内容$ ch02 -Xjre /opt java.lang.Object # 传递错误-Xjre会打印出Could not find or load main class java.lang.Object]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>JVM</tag>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《自己动手写JAVA虚拟机》学习笔记一【命令行工具】]]></title>
    <url>%2FJVM%2FJVM1%2F</url>
    <content type="text"><![CDATA[最近正在看张秀宏著的《自己动手写Java虚拟机》，这本书适合初学者更深入的理解java虚拟机的含义，也可以简单学习go语言的基本使用。 准备工作安装JDK从Oracle官网下载最新的JDK，双击运行即可。我使用的是1.8.0_161 安装GO从GO语言官网下载最新版本的GO安装文件，双击运行即可,我使用的是1.11.2。测试Go环境是否安装成功 12～$ go versiongo version go1.11.2 darwin/amd64 设置环境变量 1234#添加Go的运行环境路径export PATH=$PATH:/usr/local/go/bin#添加Go工程的工作空间,可自行修改export GOPATH=/home/XXX/XXX/jvmgo/go 执行以下命令，如果GOPATH与你设置的相同环境变量设置成功, 1～$ go env 实现JAVA命令java命令常用选项及其用途 选项 用途 -version 输出版本信息，然后退出 -?/-help 输出帮助信息，然后退出 -cp/-classpath 指定用户类路径 -Dproperty=value 设置Java系统属性 -Xms 设置初始堆空间大小 -Xmx 设置最大堆空间大小 -Xss 设置线程栈空间大小 编写命令行工具首先创建项目结构 12|-jvmgo |-ch01 在ch01目录下创建cmd.go文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package mainimport "flag"import "fmt"import "os"//用法: java [-options] class [args...] (执行类)//或 java [-options] -jar jarfile [args...] (执行 jar 文件)type Cmd struct &#123; helpFlag bool versionFlag bool cpOption string class string args []string&#125;//把命令的用法打印到控制台func printUsage() &#123; fmt.Printf("Usage：%s [-options] class [args...]\n",os.Args[0])&#125;//命令解析func parseCmd() *Cmd &#123; //声明cmd为指向空的Cmd对象的指针 cmd := &amp;Cmd&#123;&#125; //定义flag参数 //Usage是一个函数，默认输出所有定义了的命令行参数和帮助信息 flag.Usage = printUsage flag.BoolVar(&amp;cmd.helpFlag,"help",false,"print help message") flag.BoolVar(&amp;cmd.helpFlag,"?",false,"print help message") flag.BoolVar(&amp;cmd.versionFlag,"version",false,"print version and exit") flag.StringVar(&amp;cmd.cpOption,"classpath","","classpath") flag.StringVar(&amp;cmd.cpOption,"cp","","classpath") //在所有的flag定义完成之后，可以通过调用flag.Parse()进行解析。 flag.Parse() //flag.Args()可以捕获未被解析的参数 args := flag.Args() if len(args) &gt; 0&#123; cmd.class = args[0] cmd.args = args[1:] &#125; return cmd&#125; 测试代码在ch01目录下创建main.go文件 12345678910111213141516171819package mainimport "fmt"func main() &#123; cmd := parseCmd() if cmd.versionFlag &#123; fmt.Println("version 0.0.1") &#125;else if cmd.helpFlag || cmd.class == ""&#123; printUsage() &#125;else &#123; startJVM(cmd) &#125;&#125;//模拟启动jvmfunc startJVM(cmd *Cmd) &#123; //还未开始写，暂时打印 fmt.Printf("classpath:%s class:%s args:%v\n",cmd.cpOption,cmd.class,cmd.args)&#125; 编译main.go，并测试-version 123$ go install jvmgo/ch01 $ ch01 -versionversion 0.0.1]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>JAVA</tag>
        <tag>JVM</tag>
        <tag>GO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下利用Github Pages快速搭建免费博客]]></title>
    <url>%2Fblog%2FGithub-Pages-Blog%2F</url>
    <content type="text"><![CDATA[本博客利用Hexo + Next + GitHubPages 搭建博客。 准备工作安装node.js1$ brew install node 安装 git1$ brew install git 安装yarn1$ npm install yarn 如果安装失败用 1$ brew install yarn 安装hexo进入你的项目目录，举个例子我的目录是blog 1$ cd blog 然后用 yarn 安装 hexo 1$ yarn add hexo -S 如果安装失败用 12$ npm install hexo-cli -g$ npm install hexo-server --save 详细可见官方文档 搭建本地博客项目初始化进入你的项目目录,初始化hexo 12$ cd blog$ hexo init 安装依赖1$ yarn install 启动本地服务1$ hexo server 在浏览器输入 localhost:4000 即可 关联 GitHub创建项目 项目名称一定一定是 [用户名].github.io 设置 在设置中找到GitHub Pages ，分支选择master，点击choose a theme 选择喜欢的主题，点击select theme即可 到这里就可以访问你的主页啦 https://[用户名].github.io/,接下来复制你的项目链接 到你的本地项目根目录找到 _config.yml 文件，粘贴到以下位置 站点配置站点配置包括修改博客名称，描述，作者等等，建议直接查看官方教程 关联安装 hexo-deployer-git 插件 1$ yarn add hexo-deployer-git -S 如果失败的话尝试下面的方法 1$ npm install --save hexo-deployer-git 在你的项目文件夹下运行 hexo d -g （生成本地文件并将本地文件推送到 GitHub ，和 git push 功能相同）,如果失败的话前面加sudo 1$ hexo d -g 这时访问你的github主页https://[用户名].github.io/就可以啦 NexT主题配置NexT 是一个高质量并且优雅的Hexo 主题。详细可见官方文档 安装NexT主题在你的项目文件夹下，执行以下命令，安装NexT主题 1$ git clone https://github.com/iissnan/hexo-theme-next.git themes/next 更改项目根目录下_config.yml 文件，如下 重新生成本地文件并推送到github 1$ hexo d -g 访问你的github主页https://[用户名].github.io/就可以看到啦 主题配置themes/next文件夹下的_config.yml是主题的配置文件next内置了4种主题方案，选择你喜欢的方案解开注释即可 123456789# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Musescheme: Mist#scheme: Pisces#scheme: Gemini Menu Settings是控制图中菜单的位置，根据需要解开注释即可 123456789101112# ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------menu: home: / || home# about: /about/ || user tags: /tags/ || tags categories: /categories/ || th# archives: /archives/ || archive# schedule: /schedule/ || calendar# sitemap: /sitemap.xml || sitemap# commonweal: /404.html || heartbeat 还有很多很多的配置请参考官方教程,主题个性配置教程,主题美化 修改之后，运行以下命令就可以再你的主页看到啦 12$ hexo clean$ hexo d -g 也可以在本地启动服务器，在浏览器输入 localhost:4000 观看效果 1$ hexo server 创建文章添加【标签】页面新建标签页面 1$ hexo new page tags 修改项目根目录下 source/tags 的 index.md 文件如下： 1234title: tagstype: "tags"comments: false--- 修改themes/next文件夹下的_config.yml主题配置文件，取消 tags: /tags/ || tags 这行注释新建测试文章 1$ hexo new 'test' 在测试文章的头部添加tags信息，如下： 12345title: 测试文章tags: - Testing - Another Tag--- 启动本地服务，就可以看到标签菜单，点击可进入标签页，看到 如图所示 证明标签页面添加成功。 添加【分类】页面新建标签页面 1$ hexo new page categories 修改项目根目录下 source/tags 的 index.md 文件如下： 1234title: categoriestype: "categories"comments: false--- 修改themes/next文件夹下的_config.yml主题配置文件，取消 categories: /categories/ || th 这行注释在测试文章的头部添加categories信息，如下： 123456title: 测试文章tags: - Testing - Another Tag---categories: Testing 启动本地服务，就可以看到标签菜单，点击可进入标签页，看到 如图所示 证明分类页面添加成功。 给博客添加图片在项目目录下执行 1$ npm install hexo-asset-image --save 在用下面命令生成md文章时，会在_post目录下看到一个与文章同名的文件夹 1$ hexo new '文章名' 将想要上传的图片先放到文件夹下，然后在博客中使用markdown的格式引入图片： 1![文字](xxxx/图片名.jpg) 文章名和文件夹名字相同，所以不需要绝对路径，只要xxxx是文件夹的名字就可以了。 添加Valine评论系统首先需要去注册一个Leancloud账号,验证邮箱然后随便创建一个应用，按如图所示找到appid，appkey 按下图修改themes/next文件夹下的_config.yml主题配置文件，重新启动服务器就可以啦 12345678910111213# Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: true appid: your appid # your leancloud application appid appkey: your appkey # your leancloud application appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 描述 # comment box placeholder avatar: monsterid # gravatar style guest_info: nick,mail # custom comment header pageSize: 10 # pagination size 如果要删除评论请到Leancloud里删除哦 hexo添加多作者在项目目录下执行 1$ npm install hexo-generator-author --save 在文章的头部添加author信息，如下： 123456title: 测试文章tags: - Testing - Another Tag---author: Alice 修改/layout/_macro/下的post.swig文件 +为新添加的行 123456789 &lt;div class=&quot;post-meta&quot;&gt;+ &lt;span itemprop=&quot;about&quot; itemscope itemtype=&quot;https://schema.org/Thing&quot;&gt;+ &lt;a href=&quot;/authors/&#123;&#123; post.author &#125;&#125;&quot; itemprop=&quot;url&quot; rel=&quot;index&quot;&gt;+ &lt;span itemprop=&quot;name&quot;&gt;&#123;&#123; post.author &#125;&#125;&lt;/span&gt;+ &lt;/a&gt;+ &lt;/span&gt; &lt;span class=&quot;post-time&quot;&gt; &lt;span class=&quot;post-meta-item-icon&quot;&gt; &lt;i class=&quot;fa fa-calendar-o&quot;&gt;&lt;/i&gt; 在/layout下新创建author.swig文件 12345678910111213141516171819202122232425262728293031323334&#123;% extends &apos;_layout.swig&apos; %&#125;&#123;% import &apos;_macro/post-collapse.swig&apos; as post_template %&#125;&#123;% import &apos;_macro/sidebar.swig&apos; as sidebar_template %&#125;&#123;% block title %&#125; &#123;&#123; __(&apos;title.author&apos;) &#125;&#125;: &#123;&#123; page.author &#125;&#125; | &#123;&#123; config.title &#125;&#125; &#123;% endblock %&#125;&#123;% block content %&#125; &lt;div class=&quot;post-block category&quot;&gt; &lt;div id=&quot;posts&quot; class=&quot;posts-collapse&quot;&gt; &lt;div class=&quot;collection-title&quot;&gt; &lt;&#123;% if theme.seo %&#125;h2&#123;% else %&#125;h1&#123;% endif %&#125;&gt;&#123;# #&#125;&#123;&#123; page.author &#125;&#125;&#123;# #&#125;&lt;small&gt;&#123;&#123; __(&apos;title.author&apos;) &#125;&#125;&lt;/small&gt; &lt;/&#123;% if theme.seo %&#125;h2&#123;% else %&#125;h1&#123;% endif %&#125;&gt; &lt;/div&gt; &#123;% for post in page.posts %&#125; &#123;&#123; post_template.render(post) &#125;&#125; &#123;% endfor %&#125; &lt;/div&gt; &lt;/div&gt; &#123;% include &apos;_partials/pagination.swig&apos; %&#125;&#123;% endblock %&#125;&#123;% block sidebar %&#125; &#123;&#123; sidebar_template.render(false) &#125;&#125;&#123;% endblock %&#125; 修改/layout下page.swig文件 1234567891011121314151617181920212223242526 &#123;&#123; __(&apos;title.category&apos;) + page_title_suffix &#125;&#125; &#123;% elif page.type === &quot;tags&quot; %&#125; &#123;&#123; __(&apos;title.tag&apos;) + page_title_suffix &#125;&#125;+ &#123;% elif page.type === &quot;authors&quot; %&#125;+ &#123;&#123; __(&apos;title.author&apos;) + page_title_suffix &#125;&#125; &#123;% else %&#125; &#123;&#123; page.title + page_title_suffix &#125;&#125; &#123;% endif %&#125;。。。。。。。 &#123;&#123; list_categories() &#125;&#125; &lt;/div&gt; &lt;/div&gt;+ &#123;% elif page.type === &apos;authors&apos; %&#125;+ &lt;div class=&quot;author-all-page&quot;&gt;+ &lt;div class=&quot;author-all-title&quot;&gt;+ &#123;&#123; _p(&apos;counter.authors&apos;, site.authors.length) &#125;&#125;+ &lt;/div&gt;+ &lt;div class=&quot;author-all&quot;&gt;+ &#123;&#123; list_authors() &#125;&#125;+ &lt;/div&gt;+ &lt;/div&gt; &#123;% else %&#125; &#123;&#123; page.content &#125;&#125; &#123;% endif %&#125; 修改{项目名称}/themes/next下zh-Hans.yml文件 123456789101112131415title: archive: 归档 category: 分类 tag: 标签 schedule: 日程表 author : 作者 。。。counter: authors: zero: 暂无分类 one: 目前共计 1 个分类 other: "目前共计 %d 个作者" 在{项目名称}/themes/next/source/css/_common/components/pages/添加authors.styl，复制categories.styl内容将categorie改成author 在同级文件pages.styl中添加@import “authors”; 修改之后，运行以下命令就可以再你的主页看到啦 12$ hexo clean$ hexo d -g ###博文压缩 12$ npm install gulp -g$ npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp --save 在项目根目录下创建gulpfile.js并填入以下内容： 123456789101112131415161718192021222324252627282930313233var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html','minify-css','minify-js']); 生成博文是执行 hexo g &amp;&amp; gulp 就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>github</tag>
        <tag>hexo</tag>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
